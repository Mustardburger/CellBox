{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 21:11:36.412340: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-05 21:11:36.604704: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-05 21:11:36.612348: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-07-05 21:11:36.612367: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-05 21:11:40.671498: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-07-05 21:11:40.672768: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-07-05 21:11:40.672798: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "================================================================================\n",
      "   _____     _ _ ____              \n",
      "  / ____|   | | |  _ \\             \n",
      " | |     ___| | | |_) | _____  __  \n",
      " | |    / _ \\ | |  _ < / _ \\ \\/ /  \n",
      " | |___|  __/ | | |_) | (_) >  <   \n",
      "  \\_____\\___|_|_|____/ \\___/_/\\_\\  \n",
      "Running CellBox scripts developed in Sander lab\n",
      "Maintained by Bo Yuan, Judy Shen, and Augustin Luna; contributions by Daniel Ritter\n",
      "\n",
      "        version 0.3.2\n",
      "        -- Feb 10, 2023 --\n",
      "        * Modify CellBox to support TF2     \n",
      "        \n",
      "Tutorials and documentations are available at https://github.com/sanderlab/CellBox\n",
      "If you want to discuss the usage or to report a bug, please use the 'Issues' function at GitHub.\n",
      "If you find CellBox useful for your research, please consider citing the corresponding publication.\n",
      "For more information, please email us at boyuan@g.harvard.edu and c_shen@g.harvard.edu, augustin_luna@hms.harvard.edu\n",
      " --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import cellbox\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import shutil\n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from tensorflow.compat.v1.errors import OutOfRangeError\n",
    "from cellbox.utils import TimeLogger\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_id': 'Example_RP', 'model_prefix': 'seed', 'ckpt_name': 'model11.ckpt', 'export_verbose': 3, 'experiment_type': 'random partition', 'sparse_data': False, 'batchsize': 4, 'trainset_ratio': 0.7, 'validset_ratio': 0.8, 'n_batches_eval': None, 'add_noise_level': 0, 'dT': 0.1, 'ode_solver': 'heun', 'envelope_form': 'tanh', 'envelope': 0, 'pert_form': 'by u', 'ode_degree': 1, 'ode_last_steps': 2, 'n_iter_buffer': 50, 'n_iter_patience': 100, 'weight_loss': 'None', 'l1lambda': 0.0001, 'l2lambda': 0.0001, 'model': 'CellBox', 'pert_file': '/users/ngun7t/Documents/cellbox-jun-6/data/pert.csv', 'expr_file': '/users/ngun7t/Documents/cellbox-jun-6/data/expr.csv', 'node_index_file': '/users/ngun7t/Documents/cellbox-jun-6/data/node_Index.csv', 'n_protein_nodes': 82, 'n_activity_nodes': 87, 'n_x': 99, 'envelop_form': 'tanh', 'envelop': 0, 'n_epoch': 100, 'n_iter': 100, 'stages': [{'nT': 200, 'sub_stages': [{'lr_val': 0.001, 'l1lambda': 0.0001}]}], 'ckpt_path_full': './model11.ckpt', 'drug_index': 5, 'seed': 1000}\n",
      "Working directory is ready at results/Example_RP_d2f1204ae85ed795160c7d3209f04b32.\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "def set_seed(in_seed):\n",
    "    int_seed = int(in_seed)\n",
    "    tf.compat.v1.set_random_seed(int_seed)\n",
    "    np.random.seed(int_seed)\n",
    "\n",
    "\n",
    "def prepare_workdir(in_cfg):\n",
    "    # Read Data\n",
    "    in_cfg.root_dir = os.getcwd()\n",
    "    in_cfg.node_index = pd.read_csv(in_cfg.node_index_file, header=None, names=None) \\\n",
    "        if hasattr(in_cfg, 'node_index_file') else pd.DataFrame(np.arange(in_cfg.n_x))\n",
    "\n",
    "    # Create Output Folder\n",
    "    experiment_path = 'results/{}_{}'.format(in_cfg.experiment_id, md5)\n",
    "    try:\n",
    "        os.makedirs(experiment_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    out_cfg = vars(in_cfg)\n",
    "    out_cfg = {key: out_cfg[key] for key in out_cfg if type(out_cfg[key]) is not pd.DataFrame}\n",
    "    os.chdir(experiment_path)\n",
    "    json.dump(out_cfg, open('config.json', 'w'), indent=4)\n",
    "\n",
    "    if \"leave one out\" in in_cfg.experiment_type:\n",
    "        try:\n",
    "            in_cfg.model_prefix = '{}_{}'.format(in_cfg.model_prefix, in_cfg.drug_index)\n",
    "        except Exception('Drug index not specified') as e:\n",
    "            raise e\n",
    "\n",
    "    in_cfg.working_index = in_cfg.model_prefix + \"_\" + str(working_index).zfill(3)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(in_cfg.working_index)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.makedirs(in_cfg.working_index)\n",
    "    os.chdir(in_cfg.working_index)\n",
    "\n",
    "    with open(\"record_eval.csv\", 'w') as f:\n",
    "        f.write(\"epoch,iter,train_loss,valid_loss,train_mse,valid_mse,test_mse,time_elapsed\\n\")\n",
    "\n",
    "    print('Working directory is ready at {}.'.format(experiment_path))\n",
    "    return 0\n",
    "\n",
    "experiment_config_path = \"/users/ngun7t/Documents/cellbox-jun-6/configs_dev/Example.random_partition.CellBox.json\"\n",
    "working_index = 0\n",
    "stage = {\n",
    "    \"nT\": 100,\n",
    "    \"sub_stages\":[\n",
    "        {\"lr_val\": 0.1,\"l1lambda\": 0.01, \"n_iter_patience\":1000},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.01},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.0001},\n",
    "        {\"lr_val\": 0.001,\"l1lambda\": 0.00001}\n",
    "    ]}\n",
    "\n",
    "cfg = cellbox.config.Config(experiment_config_path)\n",
    "cfg.ckpt_path_full = os.path.join('./', cfg.ckpt_name)\n",
    "md5 = cellbox.utils.md5(cfg)\n",
    "cfg.drug_index = 5         # Change this for testing purposes\n",
    "cfg.seed = working_index + cfg.seed if hasattr(cfg, \"seed\") else working_index + 1000\n",
    "set_seed(cfg.seed)\n",
    "print(vars(cfg))\n",
    "\n",
    "prepare_workdir(cfg)\n",
    "logger = cellbox.utils.TimeLogger(time_logger_step=1, hierachy=3)\n",
    "args = cfg\n",
    "for i, stage in enumerate(cfg.stages):\n",
    "    set_seed(cfg.seed)\n",
    "    cfg = cellbox.dataset.factory(cfg)\n",
    "    args.sub_stages = stage['sub_stages']\n",
    "    args.n_T = stage['nT']\n",
    "    model = cellbox.model.factory(args)\n",
    "    if i == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Screenshot(dict):\n",
    "    \"\"\"summarize the model\"\"\"\n",
    "    def __init__(self, args, n_iter_buffer):\n",
    "        # initialize loss_min\n",
    "        super().__init__()\n",
    "        self.loss_min = 1000\n",
    "        # initialize tuning_metric\n",
    "        self.saved_losses = [self.loss_min]\n",
    "        self.n_iter_buffer = n_iter_buffer\n",
    "        # initialize verbose\n",
    "        self.summary = {}\n",
    "        self.summary = {}\n",
    "        self.substage_i = []\n",
    "        self.export_verbose = args.export_verbose\n",
    "\n",
    "    def avg_n_iters_loss(self, new_loss):\n",
    "        \"\"\"average the last few losses\"\"\"\n",
    "        self.saved_losses = self.saved_losses + [new_loss]\n",
    "        self.saved_losses = self.saved_losses[-self.n_iter_buffer:]\n",
    "        return sum(self.saved_losses) / len(self.saved_losses)\n",
    "\n",
    "    def screenshot(self, sess, model, substage_i, node_index, loss_min, args):\n",
    "        \"\"\"evaluate models\"\"\"\n",
    "        self.substage_i = substage_i\n",
    "        self.loss_min = loss_min\n",
    "        # Save the variables to disk.\n",
    "        if self.export_verbose > 0:\n",
    "            params = sess.run(model.params)\n",
    "            for item in params:\n",
    "                try:\n",
    "                    params[item] = pd.DataFrame(params[item], index=node_index[0])\n",
    "                except Exception:\n",
    "                    params[item] = pd.DataFrame(params[item])\n",
    "            self.update(params)\n",
    "\n",
    "        if self.export_verbose > 1 or self.export_verbose == -1:  # no params but y_hat\n",
    "            sess.run(model.iter_eval.initializer, feed_dict=model.args.feed_dicts['test_set'])\n",
    "            y_hat = eval_model(sess, model.iter_eval, model.eval_yhat, args.feed_dicts['test_set'], return_avg=False)\n",
    "            y_hat = pd.DataFrame(y_hat, columns=node_index[0])\n",
    "            self.update({'y_hat': y_hat})\n",
    "\n",
    "        if self.export_verbose > 2:\n",
    "            try:\n",
    "                # TODO: not yet support data iterators\n",
    "                summary_train = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_train']})\n",
    "                summary_test = sess.run(model.convergence_metric, feed_dict={model.in_pert: args.dataset['pert_test']})\n",
    "                summary_valid = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_valid']})\n",
    "                summary_train = pd.DataFrame(summary_train, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                summary_test = pd.DataFrame(summary_test, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                   '_sd', node_index.values + '_dxdt'])\n",
    "                summary_valid = pd.DataFrame(summary_valid, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                self.update(\n",
    "                    {'summary_train': summary_train, 'summary_test': summary_test, 'summary_valid': summary_valid}\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"save model parameters\"\"\"\n",
    "        for file in glob.glob(str(self.substage_i) + \"_best.*.csv\"):\n",
    "            os.remove(file)\n",
    "        for key in self:\n",
    "            self[key].to_csv(\"{}_best.{}.loss.{}.csv\".format(self.substage_i, key, self.loss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(saver, sess, path):\n",
    "    \"\"\"save model\"\"\"\n",
    "    # Save the variables to disk.\n",
    "    tmp = saver.save(sess, path)\n",
    "    print(\"Model saved in path: %s\" % tmp)\n",
    "\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "def eval_model(sess, eval_iter, obj_fn, eval_dict, return_avg=True, n_batches_eval=None):\n",
    "    \"\"\"simulate the model for prediction\"\"\"\n",
    "    sess.run(eval_iter.initializer, feed_dict=eval_dict)\n",
    "    counter = 0\n",
    "    eval_results = []\n",
    "    while True:\n",
    "        try:\n",
    "            eval_results.append(sess.run(obj_fn, feed_dict=eval_dict))\n",
    "        except OutOfRangeError:\n",
    "            break\n",
    "        counter += 1\n",
    "        if n_batches_eval is not None and counter > n_batches_eval:\n",
    "            break\n",
    "\n",
    "    print(f\"eval_model eval_results: {eval_results[0].shape} with len {len(eval_results)}\")\n",
    "    if return_avg:\n",
    "        return np.mean(np.array(eval_results), axis=0)\n",
    "    return np.vstack(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_substage(model, sess, lr_val, l1_lambda, l2_lambda, n_epoch, n_iter, n_iter_buffer, n_iter_patience, args):\n",
    "    \"\"\"\n",
    "    Training function that does one stage of training. The stage training can be repeated and modified to give better\n",
    "    training result.\n",
    "\n",
    "    Args:\n",
    "        model (CellBox): an CellBox instance\n",
    "        sess (tf.Session): current session, need reinitialization for every nT\n",
    "        lr_val (float): learning rate (read in from config file)\n",
    "        l1_lambda (float): l1 regularization weight\n",
    "        l2_lambda (float): l2 regularization weight\n",
    "        n_epoch (int): maximum number of epochs\n",
    "        n_iter (int): maximum number of iterations\n",
    "        n_iter_buffer (int): training loss moving average window\n",
    "        n_iter_patience (int): training loss tolerance\n",
    "        args: Args or configs\n",
    "    \"\"\"\n",
    "\n",
    "    stages = glob.glob(\"*best*.csv\")\n",
    "    try:\n",
    "        substage_i = 1 + max([int(stage[0]) for stage in stages])\n",
    "    except Exception:\n",
    "        substage_i = 1\n",
    "\n",
    "    best_params = Screenshot(args, n_iter_buffer)\n",
    "\n",
    "    n_unchanged = 0\n",
    "    idx_iter = 0\n",
    "    for key in args.feed_dicts:\n",
    "        args.feed_dicts[key].update({\n",
    "            model.lr: lr_val,\n",
    "            model.l1_lambda: l1_lambda,\n",
    "            model.l2_lambda: l2_lambda\n",
    "        })\n",
    "    args.logger.log(\"--------- lr: {}\\tl1: {}\\tl2: {}\\t\".format(lr_val, l1_lambda, l2_lambda))\n",
    "    sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    loss_train_across_epochs, loss_train_mse_across_epochs = [], []\n",
    "    loss_val_across_epochs, loss_val_mse_across_epochs = [], []\n",
    "    for idx_epoch in range(n_epoch):\n",
    "\n",
    "        loss_train_l, loss_train_mse_l = [], []\n",
    "        loss_val_l, loss_val_mse_l = [], []\n",
    "        if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "            break\n",
    "\n",
    "        sess.run(model.iter_train.initializer, feed_dict=args.feed_dicts['train_set'])\n",
    "        while True:\n",
    "            if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "                break\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                _, loss_train_i, loss_train_mse_i = sess.run(\n",
    "                    (model.op_optimize, model.train_loss, model.train_mse_loss), feed_dict=args.feed_dicts['train_set'])\n",
    "                print(f\"Loss train i: {loss_train_i}\")\n",
    "                loss_train_l.append(loss_train_i)\n",
    "                loss_train_mse_l.append(loss_train_mse_i)\n",
    "\n",
    "            except OutOfRangeError:  # for iter_train\n",
    "                break\n",
    "\n",
    "            # record training\n",
    "            loss_valid_i, loss_valid_mse_i = sess.run(\n",
    "                (model.monitor_loss, model.monitor_mse_loss), feed_dict=args.feed_dicts['valid_set'])\n",
    "            loss_val_l.append(loss_valid_i)\n",
    "            loss_val_mse_l.append(loss_valid_mse_i)\n",
    "            #new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            #if args.export_verbose > 0:\n",
    "            #    print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(\n",
    "            #        substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "            #        n_iter, loss_train_i,\n",
    "            #        best_params.loss_min, n_unchanged,\n",
    "            #        n_iter_patience\n",
    "            #        ))\n",
    "            new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            if args.export_verbose > 0:\n",
    "                print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\\tloss (buffer on valid):\"\n",
    "                       \"{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "                                                                              n_iter, loss_train_i, new_loss,\n",
    "                                                                              best_params.loss_min, n_unchanged,\n",
    "                                                                              n_iter_patience))\n",
    "            append_record(\"record_eval.csv\",\n",
    "                          [idx_epoch, idx_iter, loss_train_i, loss_valid_i, loss_train_mse_i,\n",
    "                           loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "            # early stopping\n",
    "            idx_iter += 1\n",
    "            if new_loss < best_params.loss_min:\n",
    "                n_unchanged = 0\n",
    "                best_params.screenshot(sess, model, substage_i, args=args,\n",
    "                                       node_index=args.dataset['node_index'], loss_min=new_loss)\n",
    "            else:\n",
    "                n_unchanged += 1\n",
    "\n",
    "        loss_train_across_epochs.append(loss_train_l)\n",
    "        loss_train_mse_across_epochs.append(loss_train_mse_l)\n",
    "        loss_val_across_epochs.append(loss_val_l)\n",
    "        loss_val_mse_across_epochs.append(loss_val_mse_l)\n",
    "\n",
    "\n",
    "    return best_params, {\n",
    "        \"train\": loss_train_across_epochs,\n",
    "        \"train_mse\": loss_train_mse_across_epochs,\n",
    "        \"val\": loss_val_across_epochs,\n",
    "        \"val_mse\": loss_val_mse_across_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Evaluation on valid set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    #loss_valid_i, loss_valid_mse_i = eval_model(sess, model.iter_eval, (model.eval_loss, model.eval_mse_loss),\n",
    "    #                                            args.feed_dicts['valid_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, loss_valid_i, None, loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "#\n",
    "    ## Evaluation on test set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['test_set'])\n",
    "    #loss_test_mse = eval_model(sess, model.iter_eval, model.eval_mse_loss,\n",
    "    #                           args.feed_dicts['test_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, None, None, None, loss_test_mse, time.perf_counter() - t0])\n",
    "#\n",
    "    #best_params.save()\n",
    "    #args.logger.log(\"------------------ Substage {} finished!-------------------\".format(substage_i))\n",
    "    #save_model(args.saver, sess, './' + args.ckpt_name)\n",
    "#\n",
    "#\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, args):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    args.logger = TimeLogger(time_logger_step=1, hierachy=2)\n",
    "\n",
    "    # Check if all variables in scope\n",
    "    # TODO: put variables under appropriate scopes\n",
    "    for i in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='initialization'):\n",
    "        print(i)\n",
    "\n",
    "    # Initialization\n",
    "    args.saver = tf.compat.v1.train.Saver()\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    config.graph_options.rewrite_options.memory_optimization = off\n",
    "\n",
    "    # Launching session\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    try:\n",
    "        args.saver.restore(sess, './' + args.ckpt_name)\n",
    "        print('Load existing model at {}...'.format(args.ckpt_name))\n",
    "    except Exception:\n",
    "        print('Create new model at {}...'.format(args.ckpt_name))\n",
    "\n",
    "    # Training\n",
    "    for substage in args.sub_stages:\n",
    "        n_iter_buffer = substage['n_iter_buffer'] if 'n_iter_buffer' in substage else args.n_iter_buffer\n",
    "        n_iter = substage['n_iter'] if 'n_iter' in substage else args.n_iter\n",
    "        n_iter_patience = substage['n_iter_patience'] if 'n_iter_patience' in substage else args.n_iter_patience\n",
    "        n_epoch = substage['n_epoch'] if 'n_epoch' in substage else args.n_epoch\n",
    "        l1 = substage['l1lambda'] if 'l1lambda' in substage else args.l1lambda if hasattr(args, 'l1lambda') else 0\n",
    "        l2 = substage['l2lambda'] if 'l2lambda' in substage else args.l2lambda if hasattr(args, 'l2lambda') else 0\n",
    "        screenshot, d = train_substage(model, sess, substage['lr_val'], l1_lambda=l1, l2_lambda=l2, n_epoch=n_epoch,\n",
    "                       n_iter=n_iter, n_iter_buffer=n_iter_buffer, n_iter_patience=n_iter_patience, args=args)\n",
    "\n",
    "    # Terminate session\n",
    "    sess.close()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return screenshot, d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screenshot, d = train_model(model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox-3.6-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
