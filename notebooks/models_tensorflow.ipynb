{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-29 22:14:07.812332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-29 22:14:07.983294: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-29 22:14:07.990612: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/\n",
      "2023-07-29 22:14:07.990642: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-29 22:14:11.860400: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/\n",
      "2023-07-29 22:14:11.860645: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/:/users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/\n",
      "2023-07-29 22:14:11.860674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "================================================================================\n",
      "   _____     _ _ ____              \n",
      "  / ____|   | | |  _ \\             \n",
      " | |     ___| | | |_) | _____  __  \n",
      " | |    / _ \\ | |  _ < / _ \\ \\/ /  \n",
      " | |___|  __/ | | |_) | (_) >  <   \n",
      "  \\_____\\___|_|_|____/ \\___/_/\\_\\  \n",
      "Running CellBox scripts developed in Sander lab\n",
      "Maintained by Bo Yuan, Judy Shen, and Augustin Luna; contributions by Daniel Ritter\n",
      "\n",
      "        version 0.3.2\n",
      "        -- Feb 10, 2023 --\n",
      "        * Modify CellBox to support TF2     \n",
      "        \n",
      "Tutorials and documentations are available at https://github.com/sanderlab/CellBox\n",
      "If you want to discuss the usage or to report a bug, please use the 'Issues' function at GitHub.\n",
      "If you find CellBox useful for your research, please consider citing the corresponding publication.\n",
      "For more information, please email us at boyuan@g.harvard.edu and c_shen@g.harvard.edu, augustin_luna@hms.harvard.edu\n",
      " --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import cellbox\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "import shutil\n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from tensorflow.compat.v1.errors import OutOfRangeError\n",
    "from cellbox.utils import TimeLogger\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_id': 'Example_RP', 'model_prefix': 'seed', 'ckpt_name': 'model11.ckpt', 'export_verbose': 3, 'experiment_type': 'random partition', 'sparse_data': False, 'batchsize': 4, 'trainset_ratio': 0.7, 'validset_ratio': 0.8, 'n_batches_eval': None, 'add_noise_level': 0, 'dT': 0.1, 'ode_solver': 'heun', 'envelope_form': 'tanh', 'envelope': 0, 'pert_form': 'by u', 'ode_degree': 1, 'ode_last_steps': 2, 'n_iter_buffer': 50, 'n_iter_patience': 100, 'weight_loss': 'None', 'l1lambda': 0.0001, 'l2lambda': 0.0001, 'model': 'CellBox', 'pert_file': '/users/ngun7t/Documents/cellbox-jun-6/data/pert.csv', 'expr_file': '/users/ngun7t/Documents/cellbox-jun-6/data/expr.csv', 'node_index_file': '/users/ngun7t/Documents/cellbox-jun-6/data/node_Index.csv', 'n_protein_nodes': 82, 'n_activity_nodes': 87, 'n_x': 99, 'envelop_form': 'tanh', 'envelop': 0, 'n_epoch': 100, 'n_iter': 100, 'stages': [{'nT': 100, 'sub_stages': [{'lr_val': 0.1, 'l1lambda': 0.01, 'n_iter_patience': 1000}, {'lr_val': 0.01, 'l1lambda': 0.01}, {'lr_val': 0.01, 'l1lambda': 0.0001}, {'lr_val': 0.001, 'l1lambda': 1e-05}]}], 'ckpt_path_full': './model11.ckpt', 'drug_index': 5, 'seed': 1000}\n",
      "Working directory is ready at results/Example_RP_370705d3fa02832e2d75733a602382b0.\n",
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "def set_seed(in_seed):\n",
    "    int_seed = int(in_seed)\n",
    "    tf.compat.v1.set_random_seed(int_seed)\n",
    "    np.random.seed(int_seed)\n",
    "\n",
    "\n",
    "def prepare_workdir(in_cfg):\n",
    "    # Read Data\n",
    "    in_cfg.root_dir = os.getcwd()\n",
    "    in_cfg.node_index = pd.read_csv(in_cfg.node_index_file, header=None, names=None) \\\n",
    "        if hasattr(in_cfg, 'node_index_file') else pd.DataFrame(np.arange(in_cfg.n_x))\n",
    "\n",
    "    # Create Output Folder\n",
    "    experiment_path = 'results/{}_{}'.format(in_cfg.experiment_id, md5)\n",
    "    try:\n",
    "        os.makedirs(experiment_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    out_cfg = vars(in_cfg)\n",
    "    out_cfg = {key: out_cfg[key] for key in out_cfg if type(out_cfg[key]) is not pd.DataFrame}\n",
    "    os.chdir(experiment_path)\n",
    "    json.dump(out_cfg, open('config.json', 'w'), indent=4)\n",
    "\n",
    "    if \"leave one out\" in in_cfg.experiment_type:\n",
    "        try:\n",
    "            in_cfg.model_prefix = '{}_{}'.format(in_cfg.model_prefix, in_cfg.drug_index)\n",
    "        except Exception('Drug index not specified') as e:\n",
    "            raise e\n",
    "\n",
    "    in_cfg.working_index = in_cfg.model_prefix + \"_\" + str(working_index).zfill(3)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(in_cfg.working_index)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.makedirs(in_cfg.working_index)\n",
    "    os.chdir(in_cfg.working_index)\n",
    "\n",
    "    with open(\"record_eval.csv\", 'w') as f:\n",
    "        f.write(\"epoch,iter,train_loss,valid_loss,train_mse,valid_mse,test_mse,time_elapsed\\n\")\n",
    "\n",
    "    print('Working directory is ready at {}.'.format(experiment_path))\n",
    "    return 0\n",
    "\n",
    "experiment_config_path = \"/users/ngun7t/Documents/cellbox-jun-6/configs_dev/Example.random_partition.CellBox.json\"\n",
    "working_index = 0\n",
    "stage = {\n",
    "    \"nT\": 100,\n",
    "    \"sub_stages\":[\n",
    "        {\"lr_val\": 0.1,\"l1lambda\": 0.01, \"n_iter_patience\":1000},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.01},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.0001},\n",
    "        {\"lr_val\": 0.001,\"l1lambda\": 0.00001}\n",
    "    ]}\n",
    "\n",
    "cfg = cellbox.config.Config(experiment_config_path)\n",
    "cfg.ckpt_path_full = os.path.join('./', cfg.ckpt_name)\n",
    "md5 = cellbox.utils.md5(cfg)\n",
    "cfg.drug_index = 5         # Change this for testing purposes\n",
    "cfg.seed = working_index + cfg.seed if hasattr(cfg, \"seed\") else working_index + 1000\n",
    "set_seed(cfg.seed)\n",
    "print(vars(cfg))\n",
    "\n",
    "prepare_workdir(cfg)\n",
    "logger = cellbox.utils.TimeLogger(time_logger_step=1, hierachy=3)\n",
    "args = cfg\n",
    "for i, stage in enumerate(cfg.stages):\n",
    "    set_seed(cfg.seed)\n",
    "    cfg = cellbox.dataset.factory(cfg)\n",
    "    args.sub_stages = stage['sub_stages']\n",
    "    args.n_T = stage['nT']\n",
    "    model = cellbox.model.factory(args)\n",
    "    if i == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(saver, sess, path):\n",
    "    \"\"\"save model\"\"\"\n",
    "    # Save the variables to disk.\n",
    "    tmp = saver.save(sess, path)\n",
    "    print(\"Model saved in path: %s\" % tmp)\n",
    "\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "def eval_model(sess, eval_iter, obj_fn, eval_dict, return_avg=True, n_batches_eval=None):\n",
    "    \"\"\"simulate the model for prediction\"\"\"\n",
    "    sess.run(eval_iter.initializer, feed_dict=eval_dict)\n",
    "    counter = 0\n",
    "    eval_results = []\n",
    "    while True:\n",
    "        try:\n",
    "            eval_results.append(sess.run(obj_fn, feed_dict=eval_dict))\n",
    "        except OutOfRangeError:\n",
    "            break\n",
    "        counter += 1\n",
    "        if n_batches_eval is not None and counter > n_batches_eval:\n",
    "            break\n",
    "\n",
    "    #print(f\"eval_model eval_results: {eval_results[0].shape} with len {len(eval_results)}\")\n",
    "    if return_avg:\n",
    "        return np.mean(np.array(eval_results), axis=0)\n",
    "    return np.vstack(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Screenshot(dict):\n",
    "    \"\"\"summarize the model\"\"\"\n",
    "    def __init__(self, args, n_iter_buffer):\n",
    "        # initialize loss_min\n",
    "        super().__init__()\n",
    "        self.loss_min = 1000\n",
    "        # initialize tuning_metric\n",
    "        self.saved_losses = [self.loss_min]\n",
    "        self.n_iter_buffer = n_iter_buffer\n",
    "        # initialize verbose\n",
    "        self.summary = {}\n",
    "        self.summary = {}\n",
    "        self.substage_i = []\n",
    "        self.export_verbose = args.export_verbose\n",
    "\n",
    "    def avg_n_iters_loss(self, new_loss):\n",
    "        \"\"\"average the last few losses\"\"\"\n",
    "        self.saved_losses = self.saved_losses + [new_loss]\n",
    "        self.saved_losses = self.saved_losses[-self.n_iter_buffer:]\n",
    "        return sum(self.saved_losses) / len(self.saved_losses)\n",
    "\n",
    "    def screenshot(self, sess, model, substage_i, node_index, loss_min, args):\n",
    "        \"\"\"evaluate models\"\"\"\n",
    "        self.substage_i = substage_i\n",
    "        self.loss_min = loss_min\n",
    "        # Save the variables to disk.\n",
    "        if self.export_verbose > 0:\n",
    "            params = sess.run(model.params)\n",
    "            for item in params:\n",
    "                try:\n",
    "                    params[item] = pd.DataFrame(params[item], index=node_index[0])\n",
    "                except Exception:\n",
    "                    params[item] = pd.DataFrame(params[item])\n",
    "            self.update(params)\n",
    "\n",
    "        if self.export_verbose > 1 or self.export_verbose == -1:  # no params but y_hat\n",
    "            sess.run(model.iter_eval.initializer, feed_dict=model.args.feed_dicts['test_set'])\n",
    "            y_hat = eval_model(sess, model.iter_eval, model.eval_yhat, args.feed_dicts['test_set'], return_avg=False)\n",
    "            y_hat = pd.DataFrame(y_hat, columns=node_index[0])\n",
    "            self.update({'y_hat': y_hat})\n",
    "\n",
    "        if self.export_verbose > 2:\n",
    "            try:\n",
    "                # TODO: not yet support data iterators\n",
    "                summary_train = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_train']})\n",
    "                summary_test = sess.run(model.convergence_metric, feed_dict={model.in_pert: args.dataset['pert_test']})\n",
    "                summary_valid = sess.run(model.convergence_metric,\n",
    "                                         feed_dict={model.in_pert: args.dataset['pert_valid']})\n",
    "                summary_train = pd.DataFrame(summary_train, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                summary_test = pd.DataFrame(summary_test, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                   '_sd', node_index.values + '_dxdt'])\n",
    "                summary_valid = pd.DataFrame(summary_valid, columns=[node_index.values + '_mean', node_index.values +\n",
    "                                                                     '_sd', node_index.values + '_dxdt'])\n",
    "                self.update(\n",
    "                    {'summary_train': summary_train, 'summary_test': summary_test, 'summary_valid': summary_valid}\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"save model parameters\"\"\"\n",
    "        for file in glob.glob(str(self.substage_i) + \"_best.*.csv\"):\n",
    "            os.remove(file)\n",
    "        for key in self:\n",
    "            self[key].to_csv(\"{}_best.{}.loss.{}.csv\".format(self.substage_i, key, self.loss_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_substage(model, sess, lr_val, l1_lambda, l2_lambda, n_epoch, n_iter, n_iter_buffer, n_iter_patience, args):\n",
    "    \"\"\"\n",
    "    Training function that does one stage of training. The stage training can be repeated and modified to give better\n",
    "    training result.\n",
    "\n",
    "    Args:\n",
    "        model (CellBox): an CellBox instance\n",
    "        sess (tf.Session): current session, need reinitialization for every nT\n",
    "        lr_val (float): learning rate (read in from config file)\n",
    "        l1_lambda (float): l1 regularization weight\n",
    "        l2_lambda (float): l2 regularization weight\n",
    "        n_epoch (int): maximum number of epochs\n",
    "        n_iter (int): maximum number of iterations\n",
    "        n_iter_buffer (int): training loss moving average window\n",
    "        n_iter_patience (int): training loss tolerance\n",
    "        args: Args or configs\n",
    "    \"\"\"\n",
    "\n",
    "    stages = glob.glob(\"*best*.csv\")\n",
    "    try:\n",
    "        substage_i = 1 + max([int(stage[0]) for stage in stages])\n",
    "    except Exception:\n",
    "        substage_i = 1\n",
    "\n",
    "    best_params = Screenshot(args, n_iter_buffer)\n",
    "\n",
    "    n_unchanged = 0\n",
    "    idx_iter = 0\n",
    "    for key in args.feed_dicts:\n",
    "        args.feed_dicts[key].update({\n",
    "            model.lr: lr_val,\n",
    "            model.l1_lambda: l1_lambda,\n",
    "            model.l2_lambda: l2_lambda\n",
    "        })\n",
    "    args.logger.log(\"--------- lr: {}\\tl1: {}\\tl2: {}\\t\".format(lr_val, l1_lambda, l2_lambda))\n",
    "    sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    loss_train_across_epochs, loss_train_mse_across_epochs = [], []\n",
    "    loss_val_across_epochs, loss_val_mse_across_epochs = [], []\n",
    "    for idx_epoch in range(n_epoch):\n",
    "\n",
    "        loss_train_l, loss_train_mse_l = [], []\n",
    "        loss_val_l, loss_val_mse_l = [], []\n",
    "        if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "            break\n",
    "\n",
    "        sess.run(model.iter_train.initializer, feed_dict=args.feed_dicts['train_set'])\n",
    "        while True:\n",
    "            if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "                break\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                _, loss_train_i, loss_train_mse_i = sess.run(\n",
    "                    (model.op_optimize, model.train_loss, model.train_mse_loss), feed_dict=args.feed_dicts['train_set'])\n",
    "                #print(f\"Loss train i: {loss_train_i}\")\n",
    "                loss_train_l.append(loss_train_i)\n",
    "                loss_train_mse_l.append(loss_train_mse_i)\n",
    "\n",
    "            except OutOfRangeError:  # for iter_train\n",
    "                break\n",
    "\n",
    "            # record training\n",
    "            loss_valid_i, loss_valid_mse_i = sess.run(\n",
    "                (model.monitor_loss, model.monitor_mse_loss), feed_dict=args.feed_dicts['valid_set'])\n",
    "            loss_val_l.append(loss_valid_i)\n",
    "            loss_val_mse_l.append(loss_valid_mse_i)\n",
    "            #new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            #if args.export_verbose > 0:\n",
    "            #    print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(\n",
    "            #        substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "            #        n_iter, loss_train_i,\n",
    "            #        best_params.loss_min, n_unchanged,\n",
    "            #        n_iter_patience\n",
    "            #        ))\n",
    "            new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            if args.export_verbose > 0:\n",
    "                print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\\tloss (buffer on valid):\"\n",
    "                       \"{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "                                                                              n_iter, loss_train_i, new_loss,\n",
    "                                                                              best_params.loss_min, n_unchanged,\n",
    "                                                                              n_iter_patience))\n",
    "            append_record(\"record_eval.csv\",\n",
    "                          [idx_epoch, idx_iter, loss_train_i, loss_valid_i, loss_train_mse_i,\n",
    "                           loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "            # early stopping\n",
    "            idx_iter += 1\n",
    "            if new_loss < best_params.loss_min:\n",
    "                n_unchanged = 0\n",
    "                best_params.screenshot(sess, model, substage_i, args=args,\n",
    "                                       node_index=args.dataset['node_index'], loss_min=new_loss)\n",
    "            else:\n",
    "                n_unchanged += 1\n",
    "\n",
    "        loss_train_across_epochs.append(loss_train_l)\n",
    "        loss_train_mse_across_epochs.append(loss_train_mse_l)\n",
    "        loss_val_across_epochs.append(loss_val_l)\n",
    "        loss_val_mse_across_epochs.append(loss_val_mse_l)\n",
    "\n",
    "\n",
    "    return best_params, {\n",
    "        \"train\": loss_train_across_epochs,\n",
    "        \"train_mse\": loss_train_mse_across_epochs,\n",
    "        \"val\": loss_val_across_epochs,\n",
    "        \"val_mse\": loss_val_mse_across_epochs\n",
    "    }\n",
    "\n",
    "\n",
    "    # Evaluation on valid set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    #loss_valid_i, loss_valid_mse_i = eval_model(sess, model.iter_eval, (model.eval_loss, model.eval_mse_loss),\n",
    "    #                                            args.feed_dicts['valid_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, loss_valid_i, None, loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "#\n",
    "    ## Evaluation on test set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['test_set'])\n",
    "    #loss_test_mse = eval_model(sess, model.iter_eval, model.eval_mse_loss,\n",
    "    #                           args.feed_dicts['test_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, None, None, None, loss_test_mse, time.perf_counter() - t0])\n",
    "#\n",
    "    #best_params.save()\n",
    "    #args.logger.log(\"------------------ Substage {} finished!-------------------\".format(substage_i))\n",
    "    #save_model(args.saver, sess, './' + args.ckpt_name)\n",
    "#\n",
    "#\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, args):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    args.logger = TimeLogger(time_logger_step=1, hierachy=2)\n",
    "\n",
    "    # Check if all variables in scope\n",
    "    # TODO: put variables under appropriate scopes\n",
    "    for i in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='initialization'):\n",
    "        print(i)\n",
    "\n",
    "    # Initialization\n",
    "    args.saver = tf.compat.v1.train.Saver()\n",
    "    from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    off = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    config.graph_options.rewrite_options.memory_optimization = off\n",
    "\n",
    "    # Launching session\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    try:\n",
    "        args.saver.restore(sess, './' + args.ckpt_name)\n",
    "        print('Load existing model at {}...'.format(args.ckpt_name))\n",
    "    except Exception:\n",
    "        print('Create new model at {}...'.format(args.ckpt_name))\n",
    "\n",
    "    # Training\n",
    "    for substage in args.sub_stages:\n",
    "        n_iter_buffer = substage['n_iter_buffer'] if 'n_iter_buffer' in substage else args.n_iter_buffer\n",
    "        n_iter = substage['n_iter'] if 'n_iter' in substage else args.n_iter\n",
    "        n_iter_patience = substage['n_iter_patience'] if 'n_iter_patience' in substage else args.n_iter_patience\n",
    "        n_epoch = substage['n_epoch'] if 'n_epoch' in substage else args.n_epoch\n",
    "        l1 = substage['l1lambda'] if 'l1lambda' in substage else args.l1lambda if hasattr(args, 'l1lambda') else 0\n",
    "        l2 = substage['l2lambda'] if 'l2lambda' in substage else args.l2lambda if hasattr(args, 'l2lambda') else 0\n",
    "        screenshot, d = train_substage(model, sess, substage['lr_val'], l1_lambda=l1, l2_lambda=l2, n_epoch=n_epoch,\n",
    "                       n_iter=n_iter, n_iter_buffer=n_iter_buffer, n_iter_patience=n_iter_patience, args=args)\n",
    "\n",
    "    # Terminate session\n",
    "    sess.close()\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    return screenshot, d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'initialization/W:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'initialization/eps:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization/alpha:0' shape=(99, 1) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-11 23:00:54.651684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-11 23:00:54.665376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-07-11 23:00:54.665446: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-07-11 23:00:54.665497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bmiclusterp2.chmcres.cchmc.org): /proc/driver/nvidia/version does not exist\n",
      "2023-07-11 23:00:55.130820: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new model at model11.ckpt...\n",
      "########   --------- lr: 0.001\tl1: 0.0001\tl2: 0.0001\t   --time elapsed: 2.39\n",
      "Loss train i: 2.149747133255005\n",
      "Substage:1\tEpoch:0/100\tIteration: 0/100\tloss (train):2.149747\tloss (buffer on valid):501.137357\tbest:1000.000000\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.4056596755981445\n",
      "Substage:1\tEpoch:0/100\tIteration: 1/100\tloss (train):2.405660\tloss (buffer on valid):334.835955\tbest:501.137357\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.418765068054199\n",
      "Substage:1\tEpoch:0/100\tIteration: 2/100\tloss (train):2.418765\tloss (buffer on valid):251.727617\tbest:334.835955\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1666769981384277\n",
      "Substage:1\tEpoch:0/100\tIteration: 3/100\tloss (train):2.166677\tloss (buffer on valid):201.812260\tbest:251.727617\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.306957483291626\n",
      "Substage:1\tEpoch:0/100\tIteration: 4/100\tloss (train):2.306957\tloss (buffer on valid):168.557848\tbest:201.812260\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2625672817230225\n",
      "Substage:1\tEpoch:0/100\tIteration: 5/100\tloss (train):2.262567\tloss (buffer on valid):144.800688\tbest:168.557848\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2887322902679443\n",
      "Substage:1\tEpoch:0/100\tIteration: 6/100\tloss (train):2.288732\tloss (buffer on valid):126.983417\tbest:144.800688\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.338268280029297\n",
      "Substage:1\tEpoch:0/100\tIteration: 7/100\tloss (train):2.338268\tloss (buffer on valid):113.141790\tbest:126.983417\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.0994832515716553\n",
      "Substage:1\tEpoch:0/100\tIteration: 8/100\tloss (train):2.099483\tloss (buffer on valid):102.054908\tbest:113.141790\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.5815398693084717\n",
      "Substage:1\tEpoch:0/100\tIteration: 9/100\tloss (train):2.581540\tloss (buffer on valid):92.988810\tbest:102.054908\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3224642276763916\n",
      "Substage:1\tEpoch:0/100\tIteration: 10/100\tloss (train):2.322464\tloss (buffer on valid):85.440147\tbest:92.988810\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.386025905609131\n",
      "Substage:1\tEpoch:0/100\tIteration: 11/100\tloss (train):2.386026\tloss (buffer on valid):79.039451\tbest:85.440147\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.0753180980682373\n",
      "Substage:1\tEpoch:0/100\tIteration: 12/100\tloss (train):2.075318\tloss (buffer on valid):73.561823\tbest:79.039451\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1281774044036865\n",
      "Substage:1\tEpoch:1/100\tIteration: 13/100\tloss (train):2.128177\tloss (buffer on valid):68.814085\tbest:73.561823\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.4842681884765625\n",
      "Substage:1\tEpoch:1/100\tIteration: 14/100\tloss (train):2.484268\tloss (buffer on valid):64.656555\tbest:68.814085\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3005166053771973\n",
      "Substage:1\tEpoch:1/100\tIteration: 15/100\tloss (train):2.300517\tloss (buffer on valid):60.986984\tbest:64.656555\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1332950592041016\n",
      "Substage:1\tEpoch:1/100\tIteration: 16/100\tloss (train):2.133295\tloss (buffer on valid):57.720958\tbest:60.986984\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2292280197143555\n",
      "Substage:1\tEpoch:1/100\tIteration: 17/100\tloss (train):2.229228\tloss (buffer on valid):54.805600\tbest:57.720958\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2119815349578857\n",
      "Substage:1\tEpoch:1/100\tIteration: 18/100\tloss (train):2.211982\tloss (buffer on valid):52.187775\tbest:54.805600\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2096993923187256\n",
      "Substage:1\tEpoch:1/100\tIteration: 19/100\tloss (train):2.209699\tloss (buffer on valid):49.810220\tbest:52.187775\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.317368745803833\n",
      "Substage:1\tEpoch:1/100\tIteration: 20/100\tloss (train):2.317369\tloss (buffer on valid):47.652544\tbest:49.810220\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1480207443237305\n",
      "Substage:1\tEpoch:1/100\tIteration: 21/100\tloss (train):2.148021\tloss (buffer on valid):45.672967\tbest:47.652544\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.476710796356201\n",
      "Substage:1\tEpoch:1/100\tIteration: 22/100\tloss (train):2.476711\tloss (buffer on valid):43.867475\tbest:45.672967\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3211183547973633\n",
      "Substage:1\tEpoch:1/100\tIteration: 23/100\tloss (train):2.321118\tloss (buffer on valid):42.213992\tbest:43.867475\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3685669898986816\n",
      "Substage:1\tEpoch:1/100\tIteration: 24/100\tloss (train):2.368567\tloss (buffer on valid):40.688539\tbest:42.213992\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.118701934814453\n",
      "Substage:1\tEpoch:1/100\tIteration: 25/100\tloss (train):2.118702\tloss (buffer on valid):39.273201\tbest:40.688539\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1727843284606934\n",
      "Substage:1\tEpoch:2/100\tIteration: 26/100\tloss (train):2.172784\tloss (buffer on valid):37.956167\tbest:39.273201\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3659958839416504\n",
      "Substage:1\tEpoch:2/100\tIteration: 27/100\tloss (train):2.365996\tloss (buffer on valid):36.729395\tbest:37.956167\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3807802200317383\n",
      "Substage:1\tEpoch:2/100\tIteration: 28/100\tloss (train):2.380780\tloss (buffer on valid):35.574934\tbest:36.729395\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.085430383682251\n",
      "Substage:1\tEpoch:2/100\tIteration: 29/100\tloss (train):2.085430\tloss (buffer on valid):34.500179\tbest:35.574934\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2295961380004883\n",
      "Substage:1\tEpoch:2/100\tIteration: 30/100\tloss (train):2.229596\tloss (buffer on valid):33.497825\tbest:34.500179\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.20906400680542\n",
      "Substage:1\tEpoch:2/100\tIteration: 31/100\tloss (train):2.209064\tloss (buffer on valid):32.552749\tbest:33.497825\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.253544807434082\n",
      "Substage:1\tEpoch:2/100\tIteration: 32/100\tloss (train):2.253545\tloss (buffer on valid):31.665281\tbest:32.552749\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2684121131896973\n",
      "Substage:1\tEpoch:2/100\tIteration: 33/100\tloss (train):2.268412\tloss (buffer on valid):30.827009\tbest:31.665281\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.175401210784912\n",
      "Substage:1\tEpoch:2/100\tIteration: 34/100\tloss (train):2.175401\tloss (buffer on valid):30.031754\tbest:30.827009\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.4700255393981934\n",
      "Substage:1\tEpoch:2/100\tIteration: 35/100\tloss (train):2.470026\tloss (buffer on valid):29.287523\tbest:30.031754\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.334822177886963\n",
      "Substage:1\tEpoch:2/100\tIteration: 36/100\tloss (train):2.334822\tloss (buffer on valid):28.575900\tbest:29.287523\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.4084649085998535\n",
      "Substage:1\tEpoch:2/100\tIteration: 37/100\tloss (train):2.408465\tloss (buffer on valid):27.900099\tbest:28.575900\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1026065349578857\n",
      "Substage:1\tEpoch:2/100\tIteration: 38/100\tloss (train):2.102607\tloss (buffer on valid):27.260950\tbest:27.900099\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.165800094604492\n",
      "Substage:1\tEpoch:3/100\tIteration: 39/100\tloss (train):2.165800\tloss (buffer on valid):26.650271\tbest:27.260950\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.348655939102173\n",
      "Substage:1\tEpoch:3/100\tIteration: 40/100\tloss (train):2.348656\tloss (buffer on valid):26.070584\tbest:26.650271\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.3520054817199707\n",
      "Substage:1\tEpoch:3/100\tIteration: 41/100\tloss (train):2.352005\tloss (buffer on valid):25.520827\tbest:26.070584\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1341817378997803\n",
      "Substage:1\tEpoch:3/100\tIteration: 42/100\tloss (train):2.134182\tloss (buffer on valid):24.996783\tbest:25.520827\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.242554187774658\n",
      "Substage:1\tEpoch:3/100\tIteration: 43/100\tloss (train):2.242554\tloss (buffer on valid):24.494784\tbest:24.996783\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.224297285079956\n",
      "Substage:1\tEpoch:3/100\tIteration: 44/100\tloss (train):2.224297\tloss (buffer on valid):24.009979\tbest:24.494784\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1689069271087646\n",
      "Substage:1\tEpoch:3/100\tIteration: 45/100\tloss (train):2.168907\tloss (buffer on valid):23.546934\tbest:24.009979\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2961525917053223\n",
      "Substage:1\tEpoch:3/100\tIteration: 46/100\tloss (train):2.296153\tloss (buffer on valid):23.101759\tbest:23.546934\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.158195734024048\n",
      "Substage:1\tEpoch:3/100\tIteration: 47/100\tloss (train):2.158196\tloss (buffer on valid):22.679640\tbest:23.101759\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.553088665008545\n",
      "Substage:1\tEpoch:3/100\tIteration: 48/100\tloss (train):2.553089\tloss (buffer on valid):22.275821\tbest:22.679640\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.322390079498291\n",
      "Substage:1\tEpoch:3/100\tIteration: 49/100\tloss (train):2.322390\tloss (buffer on valid):2.321173\tbest:22.275821\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.272393226623535\n",
      "Substage:1\tEpoch:3/100\tIteration: 50/100\tloss (train):2.272393\tloss (buffer on valid):2.324719\tbest:2.321173\tTolerance: 0/100\n",
      "Loss train i: 2.1631085872650146\n",
      "Substage:1\tEpoch:3/100\tIteration: 51/100\tloss (train):2.163109\tloss (buffer on valid):2.323213\tbest:2.321173\tTolerance: 1/100\n",
      "Loss train i: 2.1623542308807373\n",
      "Substage:1\tEpoch:4/100\tIteration: 52/100\tloss (train):2.162354\tloss (buffer on valid):2.322504\tbest:2.321173\tTolerance: 2/100\n",
      "Loss train i: 2.38539719581604\n",
      "Substage:1\tEpoch:4/100\tIteration: 53/100\tloss (train):2.385397\tloss (buffer on valid):2.324436\tbest:2.321173\tTolerance: 3/100\n",
      "Loss train i: 2.375701904296875\n",
      "Substage:1\tEpoch:4/100\tIteration: 54/100\tloss (train):2.375702\tloss (buffer on valid):2.325108\tbest:2.321173\tTolerance: 4/100\n",
      "Loss train i: 2.1140527725219727\n",
      "Substage:1\tEpoch:4/100\tIteration: 55/100\tloss (train):2.114053\tloss (buffer on valid):2.323796\tbest:2.321173\tTolerance: 5/100\n",
      "Loss train i: 2.2973451614379883\n",
      "Substage:1\tEpoch:4/100\tIteration: 56/100\tloss (train):2.297345\tloss (buffer on valid):2.321372\tbest:2.321173\tTolerance: 6/100\n",
      "Loss train i: 2.203223943710327\n",
      "Substage:1\tEpoch:4/100\tIteration: 57/100\tloss (train):2.203224\tloss (buffer on valid):2.317876\tbest:2.321173\tTolerance: 7/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2402219772338867\n",
      "Substage:1\tEpoch:4/100\tIteration: 58/100\tloss (train):2.240222\tloss (buffer on valid):2.316599\tbest:2.317876\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2461483478546143\n",
      "Substage:1\tEpoch:4/100\tIteration: 59/100\tloss (train):2.246148\tloss (buffer on valid):2.314013\tbest:2.316599\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.1459600925445557\n",
      "Substage:1\tEpoch:4/100\tIteration: 60/100\tloss (train):2.145960\tloss (buffer on valid):2.312332\tbest:2.314013\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.547250986099243\n",
      "Substage:1\tEpoch:4/100\tIteration: 61/100\tloss (train):2.547251\tloss (buffer on valid):2.317437\tbest:2.312332\tTolerance: 0/100\n",
      "Loss train i: 2.2930641174316406\n",
      "Substage:1\tEpoch:4/100\tIteration: 62/100\tloss (train):2.293064\tloss (buffer on valid):2.316472\tbest:2.312332\tTolerance: 1/100\n",
      "Loss train i: 2.2343909740448\n",
      "Substage:1\tEpoch:4/100\tIteration: 63/100\tloss (train):2.234391\tloss (buffer on valid):2.314782\tbest:2.312332\tTolerance: 2/100\n",
      "Loss train i: 2.1489081382751465\n",
      "Substage:1\tEpoch:4/100\tIteration: 64/100\tloss (train):2.148908\tloss (buffer on valid):2.315075\tbest:2.312332\tTolerance: 3/100\n",
      "Loss train i: 2.2181859016418457\n",
      "Substage:1\tEpoch:5/100\tIteration: 65/100\tloss (train):2.218186\tloss (buffer on valid):2.316762\tbest:2.312332\tTolerance: 4/100\n",
      "Loss train i: 2.3407070636749268\n",
      "Substage:1\tEpoch:5/100\tIteration: 66/100\tloss (train):2.340707\tloss (buffer on valid):2.320708\tbest:2.312332\tTolerance: 5/100\n",
      "Loss train i: 2.456599712371826\n",
      "Substage:1\tEpoch:5/100\tIteration: 67/100\tloss (train):2.456600\tloss (buffer on valid):2.320388\tbest:2.312332\tTolerance: 6/100\n",
      "Loss train i: 2.1282830238342285\n",
      "Substage:1\tEpoch:5/100\tIteration: 68/100\tloss (train):2.128283\tloss (buffer on valid):2.319553\tbest:2.312332\tTolerance: 7/100\n",
      "Loss train i: 2.2884202003479004\n",
      "Substage:1\tEpoch:5/100\tIteration: 69/100\tloss (train):2.288420\tloss (buffer on valid):2.322408\tbest:2.312332\tTolerance: 8/100\n",
      "Loss train i: 2.253312587738037\n",
      "Substage:1\tEpoch:5/100\tIteration: 70/100\tloss (train):2.253313\tloss (buffer on valid):2.320991\tbest:2.312332\tTolerance: 9/100\n",
      "Loss train i: 2.1693947315216064\n",
      "Substage:1\tEpoch:5/100\tIteration: 71/100\tloss (train):2.169395\tloss (buffer on valid):2.327575\tbest:2.312332\tTolerance: 10/100\n",
      "Loss train i: 2.263295888900757\n",
      "Substage:1\tEpoch:5/100\tIteration: 72/100\tloss (train):2.263296\tloss (buffer on valid):2.328349\tbest:2.312332\tTolerance: 11/100\n",
      "Loss train i: 2.1684913635253906\n",
      "Substage:1\tEpoch:5/100\tIteration: 73/100\tloss (train):2.168491\tloss (buffer on valid):2.320753\tbest:2.312332\tTolerance: 12/100\n",
      "Loss train i: 2.5218825340270996\n",
      "Substage:1\tEpoch:5/100\tIteration: 74/100\tloss (train):2.521883\tloss (buffer on valid):2.315895\tbest:2.312332\tTolerance: 13/100\n",
      "Loss train i: 2.339402914047241\n",
      "Substage:1\tEpoch:5/100\tIteration: 75/100\tloss (train):2.339403\tloss (buffer on valid):2.310245\tbest:2.312332\tTolerance: 14/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2150397300720215\n",
      "Substage:1\tEpoch:5/100\tIteration: 76/100\tloss (train):2.215040\tloss (buffer on valid):2.304893\tbest:2.310245\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.169243097305298\n",
      "Substage:1\tEpoch:5/100\tIteration: 77/100\tloss (train):2.169243\tloss (buffer on valid):2.299634\tbest:2.304893\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.207780599594116\n",
      "Substage:1\tEpoch:6/100\tIteration: 78/100\tloss (train):2.207781\tloss (buffer on valid):2.303655\tbest:2.299634\tTolerance: 0/100\n",
      "Loss train i: 2.319558620452881\n",
      "Substage:1\tEpoch:6/100\tIteration: 79/100\tloss (train):2.319559\tloss (buffer on valid):2.303412\tbest:2.299634\tTolerance: 1/100\n",
      "Loss train i: 2.485926866531372\n",
      "Substage:1\tEpoch:6/100\tIteration: 80/100\tloss (train):2.485927\tloss (buffer on valid):2.297644\tbest:2.299634\tTolerance: 2/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.126542091369629\n",
      "Substage:1\tEpoch:6/100\tIteration: 81/100\tloss (train):2.126542\tloss (buffer on valid):2.297759\tbest:2.297644\tTolerance: 0/100\n",
      "Loss train i: 2.3073909282684326\n",
      "Substage:1\tEpoch:6/100\tIteration: 82/100\tloss (train):2.307391\tloss (buffer on valid):2.294866\tbest:2.297644\tTolerance: 1/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.248513698577881\n",
      "Substage:1\tEpoch:6/100\tIteration: 83/100\tloss (train):2.248514\tloss (buffer on valid):2.293815\tbest:2.294866\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.2151498794555664\n",
      "Substage:1\tEpoch:6/100\tIteration: 84/100\tloss (train):2.215150\tloss (buffer on valid):2.298522\tbest:2.293815\tTolerance: 0/100\n",
      "Loss train i: 2.267923355102539\n",
      "Substage:1\tEpoch:6/100\tIteration: 85/100\tloss (train):2.267923\tloss (buffer on valid):2.292730\tbest:2.293815\tTolerance: 1/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.153024673461914\n",
      "Substage:1\tEpoch:6/100\tIteration: 86/100\tloss (train):2.153025\tloss (buffer on valid):2.293364\tbest:2.292730\tTolerance: 0/100\n",
      "Loss train i: 2.4626309871673584\n",
      "Substage:1\tEpoch:6/100\tIteration: 87/100\tloss (train):2.462631\tloss (buffer on valid):2.296786\tbest:2.292730\tTolerance: 1/100\n",
      "Loss train i: 2.3054192066192627\n",
      "Substage:1\tEpoch:6/100\tIteration: 88/100\tloss (train):2.305419\tloss (buffer on valid):2.295365\tbest:2.292730\tTolerance: 2/100\n",
      "Loss train i: 2.249366283416748\n",
      "Substage:1\tEpoch:6/100\tIteration: 89/100\tloss (train):2.249366\tloss (buffer on valid):2.297998\tbest:2.292730\tTolerance: 3/100\n",
      "Loss train i: 2.1510162353515625\n",
      "Substage:1\tEpoch:6/100\tIteration: 90/100\tloss (train):2.151016\tloss (buffer on valid):2.294070\tbest:2.292730\tTolerance: 4/100\n",
      "Loss train i: 2.1604790687561035\n",
      "Substage:1\tEpoch:7/100\tIteration: 91/100\tloss (train):2.160479\tloss (buffer on valid):2.291484\tbest:2.292730\tTolerance: 5/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.368687152862549\n",
      "Substage:1\tEpoch:7/100\tIteration: 92/100\tloss (train):2.368687\tloss (buffer on valid):2.287816\tbest:2.291484\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.423617362976074\n",
      "Substage:1\tEpoch:7/100\tIteration: 93/100\tloss (train):2.423617\tloss (buffer on valid):2.281585\tbest:2.287816\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.183709144592285\n",
      "Substage:1\tEpoch:7/100\tIteration: 94/100\tloss (train):2.183709\tloss (buffer on valid):2.281922\tbest:2.281585\tTolerance: 0/100\n",
      "Loss train i: 2.302727460861206\n",
      "Substage:1\tEpoch:7/100\tIteration: 95/100\tloss (train):2.302727\tloss (buffer on valid):2.283884\tbest:2.281585\tTolerance: 1/100\n",
      "Loss train i: 2.1963088512420654\n",
      "Substage:1\tEpoch:7/100\tIteration: 96/100\tloss (train):2.196309\tloss (buffer on valid):2.282765\tbest:2.281585\tTolerance: 2/100\n",
      "Loss train i: 2.165557384490967\n",
      "Substage:1\tEpoch:7/100\tIteration: 97/100\tloss (train):2.165557\tloss (buffer on valid):2.283671\tbest:2.281585\tTolerance: 3/100\n",
      "Loss train i: 2.319312810897827\n",
      "Substage:1\tEpoch:7/100\tIteration: 98/100\tloss (train):2.319313\tloss (buffer on valid):2.276521\tbest:2.281585\tTolerance: 4/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.123527765274048\n",
      "Substage:1\tEpoch:7/100\tIteration: 99/100\tloss (train):2.123528\tloss (buffer on valid):2.274293\tbest:2.276521\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n",
      "Loss train i: 2.5066800117492676\n",
      "Substage:1\tEpoch:7/100\tIteration: 100/100\tloss (train):2.506680\tloss (buffer on valid):2.269368\tbest:2.274293\tTolerance: 0/100\n",
      "eval_model eval_results: (4, 99) with len 7\n"
     ]
    }
   ],
   "source": [
    "screenshot, d = train_model(model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try out a model with preloaded weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellbox.utils import loss, optimize\n",
    "\n",
    "class PertBio:\n",
    "    \"\"\"define abstract perturbation model\"\"\"\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.n_x = args.n_x\n",
    "        self.pert_in, self.expr_out = args.pert_in, args.expr_out\n",
    "        self.iter_train, self.iter_monitor, self.iter_eval = args.iter_train, args.iter_monitor, args.iter_eval\n",
    "        self.train_x, self.train_y = self.iter_train.get_next()\n",
    "        self.monitor_x, self.monitor_y = self.iter_monitor.get_next()\n",
    "        self.eval_x, self.eval_y = self.iter_eval.get_next()\n",
    "        self.l1_lambda, self.l2_lambda = self.args.l1_lambda_placeholder, self.args.l2_lambda_placeholder\n",
    "        self.train_y0, self.monitor_y0, self.eval_y0 = None, None, None\n",
    "        self.lr = self.args.lr\n",
    "\n",
    "    def get_ops(self):\n",
    "        \"\"\"get operators for tensorflow\"\"\"\n",
    "        if self.args.weight_loss == 'expr':\n",
    "            self.train_loss, self.train_mse_loss = loss(self.train_y, self.train_yhat, self.params['W'],\n",
    "                                                        self.l1_lambda, self.l2_lambda, weight=self.train_y)\n",
    "            self.monitor_loss, self.monitor_mse_loss = loss(self.monitor_y, self.monitor_yhat, self.params['W'],\n",
    "                                                            self.l1_lambda, self.l2_lambda, weight=self.monitor_y)\n",
    "            self.eval_loss, self.eval_mse_loss = loss(self.eval_y, self.eval_yhat, self.params['W'],\n",
    "                                                      self.l1_lambda, self.l2_lambda, weight=self.eval_y)\n",
    "        elif self.args.weight_loss == 'None':\n",
    "            self.train_loss, self.train_mse_loss = loss(self.train_y, self.train_yhat, self.params['W'],\n",
    "                                                        self.l1_lambda, self.l2_lambda)\n",
    "            self.monitor_loss, self.monitor_mse_loss = loss(self.monitor_y, self.monitor_yhat, self.params['W'],\n",
    "                                                            self.l1_lambda, self.l2_lambda)\n",
    "            self.eval_loss, self.eval_mse_loss = loss(self.eval_y, self.eval_yhat, self.params['W'],\n",
    "                                                      self.l1_lambda, self.l2_lambda)\n",
    "        \n",
    "        var_list = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n",
    "        #with tf.compat.v1.variable_scope(\"optimization\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "        #    opt = tf.compat.v1.train.GradientDescentOptimizer(lr)\n",
    "        #    opt_op = opt.minimize(self.train_loss, var_list=var_list)\n",
    "        opt_op = tf.compat.v1.train.GradientDescentOptimizer(lr).minimize(self.train_loss)\n",
    "        self.op_optimize = opt_op\n",
    "        \n",
    "        #self.op_optimize = optimize(self.train_loss, self.lr)\n",
    "\n",
    "    def get_variables(self):\n",
    "        \"\"\"get model parameters (overwritten by model configuration)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x, mu):\n",
    "        \"\"\"forward propagation (overwritten by model configuration)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"build model\"\"\"\n",
    "        self.params = {}\n",
    "        self.get_variables()\n",
    "        self.train_yhat = self.forward(self.train_y0, self.train_x)\n",
    "        self.monitor_yhat = self.forward(self.monitor_y0, self.monitor_x)\n",
    "        self.eval_yhat = self.forward(self.eval_y0, self.train_x)\n",
    "        self.get_ops()\n",
    "        return self\n",
    "\n",
    "\n",
    "class CellBox(PertBio):\n",
    "    \"\"\"CellBox model\"\"\"\n",
    "    def build(self):\n",
    "        self.params = {}\n",
    "        self.get_variables()\n",
    "        with open(\"/users/ngun7t/Documents/cellbox-jun-6/sample_input.npy\", \"rb\") as f: self.train_x = tf.constant(np.load(f), name=\"sample_input\", dtype=tf.float32)\n",
    "        with open(\"/users/ngun7t/Documents/cellbox-jun-6/sample_output.npy\", \"rb\") as f: self.train_y = tf.constant(np.load(f), name=\"sample_output\", dtype=tf.float32)\n",
    "        if self.args.pert_form == 'by u':\n",
    "            y0 = tf.constant(np.zeros((self.n_x, 1)), name=\"x_init\", dtype=tf.float32)\n",
    "            self.train_y0 = y0\n",
    "            self.monitor_y0 = y0\n",
    "            self.eval_y0 = y0\n",
    "            self.gradient_zero_from = None\n",
    "        elif self.args.pert_form == 'fix x':  # fix level of node x (here y) by input perturbation u (here x)\n",
    "            self.train_y0 = tf.transpose(self.train_x)\n",
    "            self.monitor_y0 = tf.transpose(self.monitor_x)\n",
    "            self.eval_y0 = tf.transpose(self.eval_x)\n",
    "            self.gradient_zero_from = self.args.n_activity_nodes\n",
    "\n",
    "        # ODE-specific params\n",
    "        self.envelope_fn = cellbox.kernel.get_envelope(self.args)\n",
    "        self.ode_solver = cellbox.kernel.get_ode_solver(self.args)\n",
    "        self._dxdt = cellbox.kernel.get_dxdt(self.args, self.params)\n",
    "        self.convergence_metric_train, self.train_yhat = self.forward(self.train_y0, self.train_x)\n",
    "        self.convergence_metric_monitor, self.monitor_yhat = self.forward(self.monitor_y0, self.monitor_x)\n",
    "        self.convergence_metric_eval, self.eval_yhat = self.forward(self.eval_y0, self.eval_x)\n",
    "        self.get_ops()\n",
    "        return self\n",
    "\n",
    "    def forward(self, y0, mu):\n",
    "        if isinstance(mu, tf.SparseTensor):\n",
    "            mu_t = tf.sparse.to_dense(tf.sparse.transpose(mu))\n",
    "        else:\n",
    "            mu_t = tf.transpose(mu)\n",
    "        ys = self.ode_solver(y0, mu_t, self.args.dT, self.args.n_T, self._dxdt, self.gradient_zero_from)\n",
    "        # [n_T, n_x, batch_size]\n",
    "        ys = ys[-self.args.ode_last_steps:]\n",
    "        # [n_iter_tail, n_x, batch_size]\n",
    "        mean, sd = tf.nn.moments(ys, axes=0)\n",
    "        yhat = tf.transpose(ys[-1])\n",
    "        dxdt = self._dxdt(ys[-1], mu_t)\n",
    "        # [n_x, batch_size] for last ODE step\n",
    "        convergence_metric = tf.concat([mean, sd, dxdt], axis=0)\n",
    "        return convergence_metric, yhat\n",
    "\n",
    "    def get_variables(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters in the Hopfield equation\n",
    "\n",
    "        Mutates:\n",
    "            self.params(dict):{\n",
    "                W (tf.Variable): interaction matrix with constraints enforced, , shape: [n_x, n_x]\n",
    "                alpha (tf.Variable): alpha, shape: [n_x, 1]\n",
    "                eps (tf.Variable): eps, shape: [n_x, 1]\n",
    "            }\n",
    "        \"\"\"\n",
    "        n_x, n_protein_nodes, n_activity_nodes = self.n_x, self.args.n_protein_nodes, self.args.n_activity_nodes\n",
    "        with tf.compat.v1.variable_scope(\"initialization\", reuse=True):\n",
    "            \"\"\"\n",
    "               Enforce constraints  (i: recipient)\n",
    "               no self regulation wii=0\n",
    "               ingoing wij for drug nodes (88th to 99th) = 0 [n_activity_nodes 87: ]\n",
    "                                w [87:99,_] = 0\n",
    "               outgoing wij for phenotypic nodes (83th to 87th) [n_protein_nodes 82 : n_activity_nodes 87]\n",
    "                                w [_, 82:87] = 0\n",
    "               ingoing wij for phenotypic nodes from drug ndoes (direct) [n_protein_nodes 82 : n_activity_nodes 87]\n",
    "                                w [82:87, 87:99] = 0\n",
    "            \"\"\"\n",
    "            #W = tf.Variable(np.random.normal(0.01, size=(n_x, n_x)), name=\"W\", dtype=tf.float32)\n",
    "            with open(\"/users/ngun7t/Documents/cellbox-jun-6/init_weights.npy\", \"rb\") as f: W = tf.Variable(np.load(f), name=\"W\", dtype=tf.float32)\n",
    "            W_mask = (1.0 - np.diag(np.ones([n_x])))\n",
    "            W_mask[n_activity_nodes:, :] = np.zeros([n_x - n_activity_nodes, n_x])\n",
    "            W_mask[:, n_protein_nodes:n_activity_nodes] = np.zeros([n_x, n_activity_nodes - n_protein_nodes])\n",
    "            W_mask[n_protein_nodes:n_activity_nodes, n_activity_nodes:] = np.zeros([n_activity_nodes - n_protein_nodes,\n",
    "                                                                                    n_x - n_activity_nodes])\n",
    "            self.params['W'] = W_mask * W\n",
    "\n",
    "            eps = tf.Variable(np.ones((n_x, 1)), name=\"eps\", dtype=tf.float32)\n",
    "            alpha = tf.Variable(np.ones((n_x, 1)), name=\"alpha\", dtype=tf.float32)\n",
    "            self.params['alpha'] = tf.nn.softplus(alpha)\n",
    "            self.params['eps'] = tf.nn.softplus(eps)\n",
    "\n",
    "            if self.args.envelope == 2:\n",
    "                psi = tf.Variable(np.ones((n_x, 1)), name=\"psi\", dtype=tf.float32)\n",
    "                self.params['psi'] = tf.nn.softplus(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CellBox(args).build()\n",
    "lr = 0.1\n",
    "l1_lambda = 0.1\n",
    "l2_lambda = 0.01\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(model.iter_train.initializer, feed_dict=args.feed_dicts['train_set'])\n",
    "sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "train_input = args.iter_train.get_next()[0].eval(session=sess)\n",
    "# train_input is only a placeholder. The actual input the model uses is defined within the model's function above\n",
    "yhat = sess.run(model.train_yhat, feed_dict={args.pert_in: np.ones((4, 99)), args.expr_out: np.ones((4, 99))})\n",
    "_, loss_train_i, loss_train_mse_i = sess.run(\n",
    "                    (model.op_optimize, model.train_loss, model.train_mse_loss), \n",
    "                    feed_dict={\n",
    "                        args.pert_in: np.ones((4, 99)), \n",
    "                        args.expr_out: np.ones((4, 99)),\n",
    "                        model.lr: lr,\n",
    "                        model.l1_lambda: l1_lambda,\n",
    "                        model.l2_lambda: l2_lambda\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.60294807, -0.5107813 , -0.8356503 , -0.97413486, -0.9999933 ,\n",
       "        0.3804593 , -0.85972494,  0.99844074, -0.99999636, -0.99303246,\n",
       "       -0.9323062 ,  0.66923696,  0.9791808 ,  0.7210359 , -0.43123835,\n",
       "       -0.3943841 , -0.99488854,  0.9021126 ,  0.3787154 , -0.6886934 ,\n",
       "        0.99829763,  0.99999714, -0.9999965 , -0.43100008, -0.9999472 ,\n",
       "       -0.54657626,  0.19546202, -0.9997856 , -0.9858742 , -0.99963504,\n",
       "       -0.99988383, -0.9960832 , -0.19178352, -0.9445106 ,  0.9912114 ,\n",
       "        0.99086523,  0.68524194, -0.4577501 ,  0.5740201 , -0.94932646,\n",
       "       -0.9932275 ,  0.8384868 ,  0.99342585, -0.9418055 , -0.9999968 ,\n",
       "       -0.6376603 , -0.95547795, -0.9950621 , -0.99999017,  0.999996  ,\n",
       "       -0.522029  ,  0.9762989 , -0.99886215, -0.84110045, -0.9515626 ,\n",
       "        0.99999636,  0.9999865 , -0.41374728,  0.96523756,  0.981771  ,\n",
       "        0.99995905,  0.58384544, -0.9956126 , -0.14302462, -0.99903363,\n",
       "       -0.99997985,  0.94984573,  0.77075106,  0.98795605, -0.96814257,\n",
       "        0.7034763 ,  0.28228143, -0.994091  , -0.9997405 , -0.8956865 ,\n",
       "        0.44468445, -0.99998385,  0.99999636, -0.99495417,  0.15031098,\n",
       "       -0.00547823,  0.9999784 , -0.9588035 ,  0.84039634, -0.9995654 ,\n",
       "        0.99890333, -0.9999673 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.5212092 , -0.44142467], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0, :99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724.02606\n",
      "0.74363357\n"
     ]
    }
   ],
   "source": [
    "print(loss_train_i)\n",
    "print(loss_train_mse_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same analysis as above but with multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(model.iter_train.initializer, feed_dict=args.feed_dicts['train_set'])\n",
    "sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "\n",
    "loss_train_i_l, loss_train_mse_i_l = [], []\n",
    "for i in range(iterations):\n",
    "    _, loss_train_i, loss_train_mse_i = sess.run(\n",
    "                    (model.op_optimize, model.train_loss, model.train_mse_loss), \n",
    "                    feed_dict={\n",
    "                        args.pert_in: np.ones((5, 99)), \n",
    "                        args.expr_out: np.ones((5, 99)),\n",
    "                        model.lr: lr,\n",
    "                        model.l1_lambda: l1_lambda,\n",
    "                        model.l2_lambda: l2_lambda\n",
    "                        }\n",
    "                    )\n",
    "    loss_train_i_l.append(loss_train_i)\n",
    "    loss_train_mse_i_l.append(loss_train_mse_i)\n",
    "\n",
    "yhat = sess.run(model.train_yhat, feed_dict={args.pert_in: np.ones((4, 99)), args.expr_out: np.ones((4, 99))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[724.02606]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train_i_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.74363357]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train_mse_i_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.51324284,  0.07308498, -1.1287975 , -0.867312  , -0.7085849 ,\n",
       "       -0.15886928, -1.0589092 ,  0.46898544,  0.69542205, -0.73778945,\n",
       "       -1.0002403 , -0.40088785, -0.6310365 , -0.8822228 ,  0.83570576,\n",
       "       -1.1408044 ,  0.9594115 ,  0.90079725, -0.29693466, -0.8394686 ,\n",
       "       -0.46851206, -1.1755939 ,  0.9726745 ,  1.058157  ,  1.126096  ,\n",
       "        0.5443501 , -1.3753495 ,  0.8360784 , -1.199735  ,  1.3293992 ,\n",
       "       -0.9105251 ,  0.89462864,  0.9678952 , -0.90311325, -0.64759254,\n",
       "        0.7910253 ,  1.6882433 ,  0.24733545, -0.8769814 , -0.47205344,\n",
       "        0.4988147 ,  0.6320422 , -1.4062762 ,  0.78256905,  0.71208656,\n",
       "       -0.66237473,  0.9213729 ,  0.9040096 ,  1.848657  , -1.239121  ,\n",
       "       -0.7988341 , -0.69402355,  0.04898115,  0.996424  ,  1.1582758 ,\n",
       "       -1.0836715 , -0.8040622 , -0.8963666 , -0.8317058 ,  1.0899535 ,\n",
       "       -0.94164   , -0.6167047 , -0.7873902 ,  0.93686694,  0.63897747,\n",
       "        1.0894954 ,  0.75865775,  0.99653125,  1.6331884 ,  0.87641954,\n",
       "       -0.7713877 ,  0.7755606 , -0.48710114, -0.88863283, -0.8323293 ,\n",
       "        0.40818346,  0.20716324, -0.9945047 ,  0.78547513, -0.9356324 ,\n",
       "        0.5739854 , -1.0706538 ,  0.99886245,  0.9988161 ,  0.9608008 ,\n",
       "       -0.82218724,  0.98991513,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -0.49873027, -0.46228725], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sess.run(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          2.1715503  -0.908842    0.9407575  -1.286512    0.0240781\n",
      "  0.8812522  -1.9291488  -0.2583921  -0.6853537  -0.5981381   0.31213337\n",
      "  1.1467353  -0.18321851  0.19775833  0.23412874 -0.14715421 -1.1720301\n",
      "  1.0828431   0.2738312   0.45422068 -0.6942488  -0.52003336 -0.16097067\n",
      " -1.6341966  -1.6773866  -0.5747381   0.41893256 -0.6227084  -1.1106267\n",
      " -0.90806794 -0.515291   -0.28372738  2.1967633   1.1634768   1.0365913\n",
      "  1.1257043  -1.1037233  -1.0496227  -1.0093861   1.2750769  -0.5708314\n",
      " -1.1659907  -0.32305312 -0.29352018  0.3626546   0.16050918 -0.7087239\n",
      "  0.8113428  -0.6318111  -0.06247362  0.63457    -0.5877734  -0.52332705\n",
      "  0.54134136 -0.85701036 -0.25958723  0.5191231   2.1577947   0.66192347\n",
      "  0.5882902   0.66867316 -1.0945454   0.16819328 -0.12900276  0.2702159\n",
      "  0.00240558  0.41907296 -0.17158563 -0.01872332 -1.1698176  -0.42341697\n",
      "  1.4195758   0.24048118 -0.07711294  1.9028044  -1.421672   -0.9370198\n",
      "  1.7535641  -0.02964338 -0.33632183 -1.0655713   0.          0.\n",
      " -0.         -0.         -0.          0.47380203 -1.0970317   1.2494236\n",
      "  0.91397214 -0.35191604 -0.2070215  -0.1161693  -0.756772   -0.42256257\n",
      " -0.24974981  1.7362574   1.0807011 ]\n"
     ]
    }
   ],
   "source": [
    "print(params[\"W\"][0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'initialization/W:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'initialization/eps:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization/alpha:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'optimization/beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization/W/Adam:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization/W/Adam_1:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization/eps/Adam:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization/eps/Adam_1:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization/alpha/Adam:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization/alpha/Adam_1:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_1/W:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_1/eps:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_1/alpha:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization_1/beta1_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'optimization_1/beta2_power:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization_1/W/Adam:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization_1/W/Adam_1:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization_1/eps/Adam:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization_1/eps/Adam_1:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization_1/alpha/Adam:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'optimization/initialization_1/alpha/Adam_1:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_2/W:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_2/eps:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_2/alpha:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_3/W:0' shape=(99, 99) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_3/eps:0' shape=(99, 1) dtype=float32_ref>\n",
      "<tf.Variable 'initialization_3/alpha:0' shape=(99, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for var in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.GLOBAL_VARIABLES):\n",
    "    print(var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox-3.6-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
