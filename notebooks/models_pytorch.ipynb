{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [[3.6140742, 7.227595, 6.70115, 3.6506257, 6.622177, 3.5594428, 5.5601854, 4.5101857, 3.3610966, 9.822557, 6.1879897, 7.4154177, 3.217002], [3.5472631, 7.108092, 6.5936656, 3.5849123, 6.519177, 3.498232, 5.4680862, 4.434257, 3.303298, 9.674823, 6.095251, 7.3001394, 3.158329], [3.4862378, 6.9968557, 6.4919667, 3.5228007, 6.420512, 3.4389234, 5.378127, 4.3604865, 3.2469144, 9.530659, 6.004282, 7.186862, 3.1008427], [3.4265602, 6.888028, 6.392318, 3.462064, 6.3238683, 3.38082, 5.289957, 4.2882223, 3.191661, 9.389341, 5.9149823, 7.0757785, 3.0445414], [3.3680334, 6.781307, 6.2945023, 3.402525, 6.2290397, 3.323862, 5.2035356, 4.2173944, 3.1375017, 9.250699, 5.8273, 6.96684, 2.9893854], [3.310612, 6.676588, 6.198433, 3.3441286, 6.135934, 3.2680173, 5.1187944, 4.1479607, 3.0844135, 9.114625, 5.7411876, 6.8599553, 2.9353518], [3.2542937, 6.57382, 6.104082, 3.286867, 6.0445, 3.2132654, 5.035672, 4.079875, 3.0323608, 8.981022, 5.6565876, 6.7550373, 2.882391], [3.199048, 6.4729385, 6.011403, 3.2307167, 5.954699, 3.1595926, 4.9541283, 4.013123, 2.981333, 8.84982, 5.5734644, 6.652027, 2.8304787], [3.1448717, 6.373915, 5.9203672, 3.1756618, 5.866491, 3.106972, 4.8741126, 3.9476697, 2.9313087, 8.720971, 5.4917893, 6.550873, 2.7795956], [3.0917516, 6.276715, 5.8309484, 3.1216917, 5.7798524, 3.055396, 4.7955976, 3.8834858, 2.8822534, 8.594389, 5.411502, 6.451497, 2.7296948], [3.0396466, 6.181275, 5.743085, 3.0687582, 5.6947193, 3.0048137, 4.718524, 3.820527, 2.8341398, 8.470017, 5.332571, 6.353852, 2.680747], [2.9885287, 6.087545, 5.6567435, 3.01684, 5.611058, 2.955201, 4.6428566, 3.7587588, 2.786937, 8.3477955, 5.254969, 6.257899, 2.6327434], [2.938389, 5.9954934, 5.571897, 2.9659274, 5.5288467, 2.906548, 4.5685687, 3.6981661, 2.7406378, 8.227682, 5.1786623, 6.1635904, 2.5856485], [2.8891943, 5.905073, 5.4885, 2.9159844, 5.4480395, 2.8588204, 4.495615, 3.63871, 2.695209, 8.109624, 5.1036253, 6.070892, 2.5394468], [2.840935, 5.816259, 5.4065347, 2.867003, 5.3686175, 2.8120086, 4.423981, 3.5803823, 2.6506484, 7.9935904, 5.029849, 5.9797835, 2.4941278], [2.7935982, 5.729021, 5.3259783, 2.8189735, 5.290561, 2.7661016, 4.3536563, 3.523166, 2.6069455, 7.8795466, 4.9573045, 5.890234, 2.449684], [2.7471712, 5.6433372, 5.2468085, 2.7718725, 5.2138357, 2.7210689, 4.284589, 3.4670184, 2.5640638, 7.7674284, 4.885952, 5.8022013, 2.4060824], [2.7016246, 5.559161, 5.1689944, 2.7256818, 5.1384206, 2.6769078, 4.216769, 3.4119346, 2.5220046, 7.657213, 4.815779, 5.715659, 2.3633113], [2.656938, 5.476451, 5.092495, 2.6803606, 5.064266, 2.6335788, 4.150148, 3.3578637, 2.4807289, 7.5488434, 4.746747, 5.63056, 2.3213415], [2.6130903, 5.395178, 5.017292, 2.6359031, 4.9913616, 2.591079, 4.084716, 3.3048081, 2.440242, 7.442298, 4.6788564, 5.546901, 2.2801824], [2.5700874, 5.3153296, 4.9433737, 2.5923038, 4.9196935, 2.5493965, 4.02045, 3.2527452, 2.4005208, 7.3375287, 4.612073, 5.4646363, 2.2398016], [2.5279028, 5.236869, 4.870706, 2.5495377, 4.849228, 2.5085087, 3.9573283, 3.2016501, 2.3615522, 7.234499, 4.5463786, 5.38374, 2.2001836], [2.48651, 5.1597543, 4.7992578, 2.5075836, 4.7799397, 2.46839, 3.8953207, 3.151494, 2.3233166, 7.133174, 4.4817486, 5.3041844, 2.1613135], [2.4458952, 5.083966, 4.7290077, 2.4664245, 4.7117987, 2.429023, 3.8344023, 3.1022527, 2.285792, 7.033512, 4.418161, 5.225939, 2.1231713], [2.4060395, 5.0094743, 4.659931, 2.426043, 4.644787, 2.3903954, 3.7745547, 3.053916, 2.248973, 6.935488, 4.355604, 5.148986, 2.0857496], [2.3669322, 4.9362507, 4.592006, 2.3864217, 4.578882, 2.3524907, 3.7157497, 3.006455, 2.212833, 6.839058, 4.2940454, 5.073291, 2.0490265], [2.328553, 4.8642726, 4.525211, 2.3475451, 4.514062, 2.3152986, 3.6579757, 2.9598675, 2.17737, 6.7441974, 4.233478, 4.9988317, 2.0129888], [2.290886, 4.7935047, 4.4595175, 2.309391, 4.4502954, 2.2787895, 3.6011932, 2.914112, 2.142554, 6.6508584, 4.173868, 4.925575, 1.9776149], [2.2539077, 4.72392, 4.3949003, 2.2719364, 4.387554, 2.242947, 3.5453825, 2.8691711, 2.1083703, 6.5590057, 4.1151896, 4.853491, 1.9428785], [2.217596, 4.655485, 4.331332, 2.2351687, 4.325819, 2.2077556, 3.4905267, 2.8250294, 2.074813, 6.4686203, 4.0574393, 4.782575, 1.9087926], [2.181961, 4.5881977, 4.268805, 2.1990852, 4.2650824, 2.1732163, 3.4366188, 2.7816818, 2.041882, 6.379687, 4.0006084, 4.712805, 1.8753368], [2.1469793, 4.522032, 4.2072945, 2.1636662, 4.205321, 2.139315, 3.3836405, 2.7391043, 2.0095577, 6.2921696, 3.944679, 4.644163, 1.8425072], [2.1126478, 4.456974, 4.1467886, 2.12891, 4.146525, 2.1060443, 3.3315825, 2.6972954, 1.9778366, 6.2060485, 3.889636, 4.5766273, 1.8102891], [2.0789533, 4.3929977, 4.0872707, 2.0948, 4.088675, 2.0733895, 3.2804205, 2.6562333, 1.9467036, 6.1213, 3.8354666, 4.5101857, 1.7786816], [2.0458875, 4.330088, 4.0287333, 2.0613332, 4.031761, 2.041346, 3.2301445, 2.615911, 1.9161537, 6.0378995, 3.7821584, 4.444823, 1.7476696], [2.0134346, 4.268219, 3.971155, 2.02849, 3.9757617, 2.0098982, 3.1807368, 2.5763123, 1.8861754, 5.955821, 3.7296913, 4.3805127, 1.7172357], [1.9815823, 4.207363, 3.9145117, 1.9962461, 3.920645, 1.9790163, 3.1321654, 2.5374053, 1.8567386, 5.875026, 3.6780357, 4.317226, 1.687363], [1.9503123, 4.147503, 3.8587892, 1.964597, 3.8664036, 1.9486988, 3.0844216, 2.4991868, 1.8278444, 5.795497, 3.6271842, 4.254947, 1.658042], [1.9196151, 4.0886173, 3.8039565, 1.9335209, 3.813013, 1.9189229, 3.0374782, 2.4616325, 1.7994657, 5.717201, 3.5771139, 4.1936555, 1.6292584], [1.8894761, 4.03069, 3.750006, 1.903014, 3.7604685, 1.8896914, 2.991339, 2.4247463, 1.7716137, 5.640132, 3.5278273, 4.1333537, 1.6010188], [1.8599, 3.973716, 3.6969292, 1.8730699, 3.7087536, 1.8609929, 2.9459848, 2.3885117, 1.7442768, 5.564265, 3.4793103, 4.074017, 1.5733013], [1.8308672, 3.917665, 3.6447074, 1.8436732, 3.6578498, 1.8328128, 2.9013958, 2.352908, 1.7174323, 5.489564, 3.4315395, 4.015617, 1.5460935], [1.8023568, 3.86251, 3.593313, 1.8148046, 3.6077275, 1.8051295, 2.8575475, 2.3179123, 1.6910671, 5.4159985, 3.3844893, 3.9581223, 1.5193775], [1.7743545, 3.808235, 3.5427291, 1.7864478, 3.5583758, 1.777936, 2.8144255, 2.2835135, 1.6651741, 5.343551, 3.3381562, 3.9015245, 1.4931425], [1.7468477, 3.754825, 3.4929402, 1.7585936, 3.509781, 1.7512224, 2.7720194, 2.2497015, 1.6397436, 5.272205, 3.2925339, 3.8458178, 1.4673955], [1.71984, 3.7022705, 3.443942, 1.7312452, 3.4619412, 1.7249904, 2.7303245, 2.2164714, 1.6147715, 5.20194, 3.2476006, 3.790973, 1.4421182], [1.69331, 3.6505454, 3.3957129, 1.70438, 3.4148345, 1.6992244, 2.6893249, 2.183815, 1.5902532, 5.1327443, 3.2033567, 3.7369924, 1.417314], [1.6672605, 3.5996442, 3.3482492, 1.6780031, 3.3684554, 1.6739187, 2.649012, 2.1517186, 1.566174, 5.064595, 3.1597836, 3.683852, 1.3929663], [1.6416775, 3.5495412, 3.3015308, 1.6520965, 3.3227825, 1.6490643, 2.6093736, 2.1201713, 1.5425293, 4.997475, 3.1168716, 3.6315355, 1.3690679], [1.6165494, 3.5002246, 3.2555444, 1.6266497, 3.2778022, 1.6246464, 2.5703886, 2.089157, 1.5193045, 4.9313574, 3.0746055, 3.5800247, 1.3456022], [1.5918607, 3.4516711, 3.2102618, 1.6016465, 3.2334945, 1.6006451, 2.5320358, 2.0586588, 1.4964846, 4.8662186, 3.0329633, 3.5292947, 1.3225536], [1.5676018, 3.403867, 3.165674, 1.577077, 3.1898494, 1.5770582, 2.4943085, 2.028667, 1.4740629, 4.802044, 2.9919405, 3.479341, 1.2999201], [1.5437633, 3.3568013, 3.1217692, 1.5529327, 3.146855, 1.5538775, 2.4571946, 1.999175, 1.4520347, 4.738816, 2.951524, 3.430151, 1.2776961], [1.5203435, 3.3104649, 3.078536, 1.529212, 3.104508, 1.5311017, 2.4206889, 1.9701754, 1.4303954, 4.676526, 2.9117136, 3.3817182, 1.2558763], [1.4973358, 3.2648458, 3.035967, 1.505903, 3.062788, 1.5087159, 2.3847759, 1.9416561, 1.4091346, 4.61515, 2.8724906, 3.334022, 1.2344532], [1.4747279, 3.2199264, 2.9940503, 1.483, 3.0216944, 1.4867201, 2.3494499, 1.913613, 1.3882499, 4.554677, 2.8338504, 3.2870512, 1.2134159], [1.4525115, 3.1756914, 2.9527721, 1.4604855, 2.9812088, 1.4650993, 2.314693, 1.8860335, 1.3677307, 4.495093, 2.7957866, 3.2407947, 1.1927652], [1.430686, 3.1321385, 2.912133, 1.4383683, 2.9413297, 1.4438593, 2.2805037, 1.8589114, 1.3475765, 4.4363847, 2.7582908, 3.1952386, 1.1724955], [1.4092481, 3.089255, 2.8721163, 1.4166377, 2.9020479, 1.422992, 2.246872, 1.8322427, 1.3277801, 4.378537, 2.721353, 3.1503708, 1.1525884], [1.3881814, 3.0470219, 2.8327038, 1.3952757, 2.8633409, 1.402477, 2.2137828, 1.8060129, 1.3083293, 4.3215327, 2.684961, 3.1061916, 1.1330539], [1.3674889, 3.005436, 2.7938979, 1.3742937, 2.8252134, 1.3823237, 2.181232, 1.7802225, 1.2892247, 4.2653556, 2.649101, 3.062681, 1.1138808], [1.3471591, 2.9644809, 2.7556787, 1.3536727, 2.7876449, 1.3625193, 2.1492028, 1.7548513, 1.2704506, 4.20998, 2.6137624, 3.0198236, 1.0950497], [1.3271718, 2.9241383, 2.7180293, 1.3334002, 2.7506201, 1.3430499, 2.1176872, 1.7298946, 1.2520063, 4.1554065, 2.578942, 2.9776149, 1.0765648], [1.3075346, 2.8844123, 2.6809525, 1.3134819, 2.7141447, 1.3239213, 2.0866854, 1.7053479, 1.2338878, 4.101627, 2.5446343, 2.9360447, 1.0584202], [1.2882417, 2.8452923, 2.6444335, 1.2939047, 2.6782038, 1.3051205, 2.0561852, 1.6812072, 1.2160871, 4.048622, 2.510826, 2.8950963, 1.0406023], [1.2692778, 2.8067572, 2.6084602, 1.2746575, 2.6427834, 1.2866331, 2.0261664, 1.6574502, 1.1985896, 3.9963734, 2.4775102, 2.8547616, 1.0231057], [1.2506373, 2.7688005, 2.5730252, 1.2557359, 2.60788, 1.2684608, 1.9966264, 1.6340721, 1.1813922, 3.9448671, 2.4446735, 2.8150249, 1.0059204], [1.232312, 2.731406, 2.5381193, 1.2371305, 2.5734773, 1.2505878, 1.9675512, 1.611066, 1.1644875, 3.8940897, 2.4123085, 2.7758784, 0.9890362], [1.2142946, 2.6945615, 2.5037224, 1.2188346, 2.5395713, 1.2330151, 1.9389392, 1.5884339, 1.1478714, 3.8440325, 2.3804116, 2.7373166, 0.97245806], [1.196583, 2.6582625, 2.4698331, 1.2008393, 2.5061421, 1.215725, 1.9107687, 1.5661511, 1.1315285, 3.7946708, 2.3489594, 2.699314, 0.9561667], [1.1791582, 2.6224856, 2.4364343, 1.1831365, 2.4731865, 1.1987169, 1.8830389, 1.5442196, 1.115461, 3.7460043, 2.3179593, 2.6618764, 0.9401665], [1.162029, 2.5872395, 2.403532, 1.1657318, 2.4407032, 1.1819915, 1.8557508, 1.5226389, 1.0996679, 3.6980267, 2.2874095, 2.624998, 0.92445934], [1.1451912, 2.5525117, 2.3711197, 1.148621, 2.4086843, 1.1655445, 1.8288916, 1.5014031, 1.0841455, 3.650725, 2.2572944, 2.5886655, 0.9090328], [1.1286293, 2.5182862, 2.3391807, 1.1317881, 2.3771172, 1.1493703, 1.8024491, 1.4805042, 1.0688883, 3.604092, 2.2276149, 2.552875, 0.89388895], [1.1123531, 2.4845712, 2.3077219, 1.1152446, 2.3460085, 1.1334728, 1.7764344, 1.4599545, 1.0539014, 3.5581203, 2.1983702, 2.51762, 0.87902117], [1.096356, 2.4513526, 2.2767253, 1.0989779, 2.315345, 1.117836, 1.7508259, 1.4397309, 1.0391692, 3.512794, 2.1695454, 2.4828835, 0.8644192], [1.0806311, 2.4186225, 2.2461784, 1.0829797, 2.2851152, 1.1024547, 1.7256207, 1.4198279, 1.024684, 3.4680943, 2.1411273, 2.448654, 0.85007286], [1.0651654, 2.3863661, 2.2160735, 1.0672457, 2.2553134, 1.0873222, 1.7008075, 1.4002372, 1.0104439, 3.4240158, 2.1131124, 2.4149258, 0.8359849], [1.0499632, 2.3545787, 2.1864116, 1.0517744, 2.2259355, 1.0724419, 1.676385, 1.38095, 0.99644566, 3.3805497, 2.0854995, 2.381694, 0.8221524], [1.0350187, 2.3232563, 2.1571877, 1.0365536, 2.196969, 1.0578053, 1.6523398, 1.3619553, 0.9826828, 3.3376808, 2.0582712, 2.3489413, 0.8085611], [1.0203158, 2.2923799, 2.1283858, 1.0215774, 2.1684074, 1.0434047, 1.6286659, 1.3432544, 0.9691523, 3.2954097, 2.031434, 2.316672, 0.7952249], [1.0058677, 2.2619572, 2.1000116, 1.0068567, 2.1402562, 1.0292444, 1.6053686, 1.3248512, 0.9558555, 3.253726, 2.00498, 2.2848754, 0.7821318], [0.9916601, 2.2319758, 2.0720506, 0.99238205, 2.1125019, 1.0153146, 1.582437, 1.3067384, 0.9427862, 3.2126203, 1.9789022, 2.2535474, 0.7692787], [0.97769547, 2.2024271, 2.0444944, 0.9781484, 2.0851398, 1.0016128, 1.5598657, 1.2889096, 0.9299407, 3.172085, 1.9531971, 2.222679, 0.756657], [0.9639652, 2.1733036, 2.0173402, 0.9641539, 2.0581603, 0.9881319, 1.5376422, 1.2713563, 0.9173104, 3.1321046, 1.9278523, 2.1922593, 0.74426013], [0.95045996, 2.1445942, 1.9905784, 0.9503907, 2.0315607, 0.97486943, 1.51576, 1.2540716, 0.904889, 3.0926647, 1.9028603, 2.1622763, 0.7320778], [0.9371729, 2.1162887, 1.9641942, 0.9368423, 2.0053282, 0.961815, 1.494212, 1.2370496, 0.8926704, 3.0537572, 1.8782178, 2.1327262, 0.7201091], [0.92410064, 2.0883825, 1.9381877, 0.9235113, 1.9794577, 0.948967, 1.472989, 1.2202843, 0.8806533, 3.0153728, 1.8539119, 2.1035976, 0.7083477], [0.9112412, 2.0608675, 1.9125457, 0.9103893, 1.9539398, 0.9363192, 1.4520833, 1.2037697, 0.8688312, 2.9775043, 1.8299389, 2.0748835, 0.6967896], [0.8985962, 2.0337393, 1.887272, 0.8974804, 1.9287776, 0.9238727, 1.4315019, 1.1875114, 0.8572045, 2.9401457, 1.8063011, 2.0465844, 0.6854376], [0.8861619, 2.0069935, 1.8623655, 0.8847837, 1.9039712, 0.9116256, 1.4112402, 1.1715037, 0.8457796, 2.903296, 1.7829958, 2.0186982, 0.6742922], [0.8739381, 1.9806291, 1.8378233, 0.87229466, 1.8795142, 0.8995791, 1.3912967, 1.1557437, 0.83454794, 2.8669477, 1.7600219, 1.9912196, 0.66334784], [0.86191815, 1.9546351, 1.8136339, 0.86000884, 1.8553995, 0.88772094, 1.3716573, 1.140226, 0.8235004, 2.8310902, 1.7373627, 1.9641312, 0.65259755], [0.8500942, 1.929004, 1.789789, 0.84791845, 1.8316197, 0.8760524, 1.3523202, 1.1249468, 0.81263644, 2.7957213, 1.7150202, 1.9374341, 0.6420455], [0.83846587, 1.9037397, 1.7662933, 0.8360252, 1.8081756, 0.86457574, 1.3332866, 1.1099117, 0.80195874, 2.7608352, 1.6929944, 1.9111254, 0.631686], [0.8270284, 1.8788285, 1.743131, 0.82432073, 1.7850568, 0.8532783, 1.3145406, 1.0951012, 0.79145384, 2.7264175, 1.6712743, 1.8851943, 0.62151706], [0.8157806, 1.8542699, 1.720305, 0.8128058, 1.7622619, 0.8421671, 1.2960851, 1.0805178, 0.78112864, 2.692467, 1.6498625, 1.8596361, 0.6115312], [0.8047117, 1.8300538, 1.6978022, 0.801467, 1.7397758, 0.8312267, 1.2779053, 1.0661519, 0.77096903, 2.658965, 1.6287411, 1.834435, 0.6017167], [0.79381543, 1.8061692, 1.6756124, 0.7903049, 1.7175943, 0.8204511, 1.2599939, 1.0519993, 0.7609708, 2.6259072, 1.607909, 1.8095875, 0.59207416], [0.78309447, 1.7826139, 1.6537335, 0.77931756, 1.6957121, 0.809847, 1.2423551, 1.038059, 0.7511362, 2.5932899, 1.587365, 1.7850958, 0.58261067]], 'train_mse': [[1.8348542, 5.450862, 4.9268403, 1.8787048, 4.852622, 1.7922447, 3.7953393, 2.74769, 1.6009489, 8.064751, 4.432519, 5.662278, 1.4661918], [1.7987717, 5.361916, 4.849804, 1.8433641, 4.77994, 1.761302, 3.7334592, 2.7019327, 1.5732731, 7.947093, 4.3698163, 5.5769954, 1.4374747], [1.7676644, 5.280559, 4.777945, 1.8110524, 4.7110343, 1.7317132, 3.6731799, 2.6578014, 1.5464883, 7.832488, 4.308366, 5.493197, 1.4094268], [1.7373857, 5.2010922, 4.707619, 1.7796, 4.643636, 1.7028146, 3.6141744, 2.6146612, 1.5203172, 7.720211, 4.248065, 5.4110713, 1.3820417], [1.7077341, 5.123205, 4.6385965, 1.7488147, 4.5775228, 1.6745337, 3.5563915, 2.5724325, 1.494719, 7.6100903, 4.1888633, 5.3305726, 1.3552877], [1.6786764, 5.04681, 4.5708117, 1.7186621, 4.51262, 1.6468515, 3.4997725, 2.531081, 1.4696735, 7.5020204, 4.130716, 5.2516146, 1.3291411], [1.6502049, 4.971851, 4.504232, 1.689135, 4.448883, 1.6197592, 3.4442725, 2.4905803, 1.4451662, 7.3959227, 4.073583, 5.1741257, 1.3035711], [1.6223133, 4.8982844, 4.4388285, 1.660221, 4.3862796, 1.5932437, 3.3898475, 2.4509082, 1.4211816, 7.291729, 4.0174303, 5.0980477, 1.2785525], [1.5949922, 4.826079, 4.3745737, 1.6319102, 4.324778, 1.5672927, 3.3364608, 2.4120443, 1.3977067, 7.1893883, 3.9622254, 5.0233264, 1.2540662], [1.5682325, 4.755202, 4.311441, 1.6041892, 4.2643514, 1.5418929, 3.2840884, 2.3739693, 1.3747264, 7.088846, 3.9079432, 4.9499183, 1.2300965], [1.5420227, 4.6856227, 4.249404, 1.5770463, 4.204974, 1.5170313, 3.2327006, 2.3366604, 1.3522271, 6.9900556, 3.85456, 4.877788, 1.2066299], [1.5163511, 4.617304, 4.1884384, 1.550471, 4.1466217, 1.4926927, 3.182272, 2.300098, 1.330197, 6.8929715, 3.802058, 4.8068995, 1.183656], [1.4912066, 4.5502143, 4.1285195, 1.5244516, 4.0892706, 1.4688663, 3.1327782, 2.2642655, 1.3086252, 6.7975526, 3.7504158, 4.737223, 1.16116], [1.4665792, 4.4843287, 4.0696244, 1.4989775, 4.032899, 1.4455417, 3.0841951, 2.229148, 1.2875011, 6.703767, 3.6996164, 4.6687293, 1.1391301], [1.4424586, 4.4196196, 4.011732, 1.4740368, 3.9774852, 1.4227057, 3.0365043, 2.1947289, 1.2668158, 6.6115746, 3.6496487, 4.601397, 1.1175554], [1.4188329, 4.356061, 3.9548216, 1.4496188, 3.9230072, 1.4003433, 2.98969, 2.1609898, 1.2465575, 6.5209427, 3.6004858, 4.5351977, 1.0964288], [1.3956916, 4.293631, 3.898875, 1.425712, 3.8694458, 1.3784447, 2.9437258, 2.1279154, 1.2267171, 6.4318337, 3.5521092, 4.4701076, 1.0757387], [1.3730233, 4.2323003, 3.843874, 1.4023023, 3.8167806, 1.357003, 2.8985953, 2.0954897, 1.2072872, 6.344219, 3.504507, 4.406107, 1.0554787], [1.3508192, 4.172044, 3.7898002, 1.3793766, 3.7649908, 1.3360081, 2.8542786, 2.063695, 1.1882573, 6.2580647, 3.4576616, 4.3431654, 1.035637], [1.329069, 4.112838, 3.736632, 1.3569232, 3.7140596, 1.3154498, 2.8107564, 2.032518, 1.1696177, 6.173335, 3.4115558, 4.28126, 1.0162019], [1.307762, 4.0546556, 3.68435, 1.3349301, 3.6639678, 1.2953155, 2.7680104, 2.0019445, 1.1513578, 6.0899987, 3.3661754, 4.220369, 0.9971637], [1.2868882, 3.9974754, 3.6329343, 1.313388, 3.614698, 1.2755933, 2.7260244, 1.9719584, 1.13347, 6.008023, 3.3215065, 4.1604705, 0.9785156], [1.266437, 3.9412744, 3.5823708, 1.2922894, 3.5662355, 1.2562733, 2.6847882, 1.9425447, 1.1159486, 5.9273844, 3.2775362, 4.1015463, 0.9602492], [1.2463986, 3.886035, 3.5326421, 1.2716244, 3.5185614, 1.2373458, 2.644282, 1.9136888, 1.098782, 5.848052, 3.2342498, 4.043575, 0.94235444], [1.2267631, 3.8317354, 3.48373, 1.2513802, 3.4716606, 1.2188014, 2.6044888, 1.8853781, 1.081961, 5.769999, 3.1916368, 3.986539, 0.92482334], [1.2075212, 3.7783527, 3.4356194, 1.2315462, 3.4255161, 1.200631, 2.5653934, 1.8576013, 1.0654784, 5.693199, 3.1496828, 3.9304202, 0.90764934], [1.188664, 3.7258682, 3.388293, 1.212113, 3.3801124, 1.1828285, 2.5269816, 1.8303497, 1.0493271, 5.617625, 3.1083767, 3.8751988, 0.8908245], [1.1701841, 3.6742642, 3.3417377, 1.1930715, 3.3354352, 1.1653849, 2.4892406, 1.803611, 1.0335023, 5.5432515, 3.067707, 3.8208575, 0.8743413], [1.1520733, 3.6235228, 3.2959406, 1.1744136, 3.291466, 1.1482904, 2.4521546, 1.7773715, 1.0179965, 5.470054, 3.0276606, 3.7673824, 0.8581911], [1.1343231, 3.5736225, 3.2508812, 1.1561298, 3.2481906, 1.1315346, 2.41571, 1.751617, 1.0028025, 5.3980083, 2.9882236, 3.7147546, 0.8423663], [1.116924, 3.524548, 3.206543, 1.1382097, 3.2055938, 1.1151106, 2.3798935, 1.7263356, 0.98791325, 5.327091, 2.9493873, 3.6629567, 0.8268602], [1.0998684, 3.4762847, 3.1629105, 1.1206459, 3.163662, 1.0990148, 2.3446975, 1.701517, 0.97332287, 5.257284, 2.911143, 3.611974, 0.8116658], [1.0831494, 3.428815, 3.1199691, 1.10343, 3.1223829, 1.0832367, 2.3101063, 1.6771504, 0.9590218, 5.1885605, 2.8734741, 3.56179, 0.7967763], [1.0667579, 3.3821182, 3.077708, 1.086553, 3.0817418, 1.0677671, 2.2761056, 1.6532265, 0.9450026, 5.120902, 2.836371, 3.5123904, 0.7821853], [1.0506858, 3.3361788, 3.0361161, 1.0700088, 3.0417287, 1.0526013, 2.242685, 1.6297371, 0.93126315, 5.0542884, 2.7998264, 3.4637668, 0.7678906], [1.0349281, 3.2909813, 2.9951868, 1.053791, 3.00233, 1.0377322, 2.2098343, 1.606672, 0.91779673, 4.9887013, 2.7638288, 3.4159055, 0.7538837], [1.0194801, 3.2465098, 2.9549077, 1.0378898, 2.9635355, 1.02315, 2.1775398, 1.5840209, 0.9045944, 4.924118, 2.7283633, 3.3687873, 0.74015707], [1.0043347, 3.2027516, 2.915264, 1.0222988, 2.9253302, 1.0088471, 2.1457899, 1.5617748, 0.89165, 4.860518, 2.6934204, 3.3223963, 0.72670406], [0.9894845, 3.1596923, 2.8762379, 1.0070084, 2.8877046, 0.99481684, 2.1145723, 1.5399253, 0.8789558, 4.7978845, 2.6589904, 3.2767239, 0.7135189], [0.9749221, 3.11732, 2.837819, 0.992011, 2.8506482, 0.9810515, 2.0838766, 1.5184625, 0.8665069, 4.7361984, 2.6250663, 3.2317622, 0.7005957], [0.9606423, 3.0756218, 2.7999983, 0.977302, 2.814148, 0.96754587, 2.0536947, 1.4973781, 0.8542983, 4.6754394, 2.591638, 3.1874955, 0.6879289], [0.94663864, 3.0345795, 2.762766, 0.9628749, 2.778192, 0.9542945, 2.024015, 1.4766637, 0.84232455, 4.615588, 2.5586958, 3.143905, 0.67551315], [0.932903, 2.994181, 2.7261076, 0.9487228, 2.7427692, 0.9412911, 1.9948267, 1.4563098, 0.83058226, 4.5566273, 2.5262322, 3.1009767, 0.66334295], [0.91942805, 2.954415, 2.690014, 0.9348373, 2.7078693, 0.9285308, 1.9661204, 1.4363072, 0.8190655, 4.4985385, 2.494239, 3.0587018, 0.6514132], [0.9062069, 2.9152696, 2.6544719, 0.92121196, 2.6734843, 0.91600907, 1.9378862, 1.4166476, 0.8077682, 4.441304, 2.462708, 3.0170665, 0.63971883], [0.89323235, 2.876731, 2.6194713, 0.90784085, 2.6396043, 0.9037186, 1.9101151, 1.3973248, 0.7966851, 4.384911, 2.4316292, 2.9760578, 0.6282576], [0.8804995, 2.8387842, 2.5850008, 0.8947173, 2.6062198, 0.8916542, 1.8827988, 1.3783321, 0.78581154, 4.3293424, 2.4009936, 2.9356658, 0.61702406], [0.868003, 2.801417, 2.5510523, 0.88183755, 2.5733202, 0.87981224, 1.8559314, 1.3596632, 0.7751434, 4.2745857, 2.3707945, 2.8958814, 0.6060143], [0.85573953, 2.7646165, 2.5176198, 0.86919755, 2.540896, 0.868188, 1.8295056, 1.3413116, 0.7646761, 4.220625, 2.3410254, 2.856691, 0.59522396], [0.84370255, 2.7283728, 2.4846883, 0.85679084, 2.5089378, 0.85677433, 1.8035084, 1.3232687, 0.75440514, 4.167445, 2.3116794, 2.8180835, 0.58464664], [0.8318869, 2.6926763, 2.452247, 0.844611, 2.4774377, 0.8455653, 1.777931, 1.3055278, 0.74432695, 4.1150312, 2.2827473, 2.7800481, 0.57427555], [0.82028717, 2.657516, 2.4202867, 0.8326522, 2.446387, 0.8345566, 1.752765, 1.288082, 0.7344357, 4.0633698, 2.2542212, 2.7425754, 0.5641077], [0.8088995, 2.622884, 2.3887987, 0.82090974, 2.4157794, 0.82374537, 1.7280041, 1.2709264, 0.72472805, 4.0124483, 2.2260954, 2.7056587, 0.5541402], [0.79772055, 2.5887728, 2.3577743, 0.8093808, 2.3856063, 0.8131285, 1.7036421, 1.2540549, 0.7152004, 3.9622536, 2.1983635, 2.669289, 0.54436827], [0.7867441, 2.5551693, 2.327207, 0.79805833, 2.355859, 0.80269974, 1.6796697, 1.2374606, 0.70584947, 3.9127712, 2.1710181, 2.633454, 0.5347902], [0.77596515, 2.5220635, 2.2970877, 0.78693634, 2.3265305, 0.79245347, 1.6560789, 1.2211381, 0.6966697, 3.8639894, 2.144055, 2.5981457, 0.5254003], [0.7653814, 2.4894454, 2.2674117, 0.7760099, 2.2976165, 0.7823887, 1.6328626, 1.2050827, 0.6876594, 3.815898, 2.1174676, 2.5633507, 0.51619685], [0.75498784, 2.4573083, 2.2381716, 0.7652762, 2.2691069, 0.7725036, 1.6100128, 1.1892854, 0.67881465, 3.7684844, 2.0912514, 2.5290582, 0.5071729], [0.7447808, 2.425641, 2.2093558, 0.75473076, 2.2409945, 0.7627905, 1.5875213, 1.1737425, 0.6701291, 3.721734, 2.0653975, 2.495261, 0.49832347], [0.7347561, 2.394435, 2.180957, 0.7443689, 2.2132719, 0.75324374, 1.5653838, 1.1584485, 0.6615992, 3.675635, 2.0398943, 2.4619544, 0.48964706], [0.7249088, 2.363681, 2.152967, 0.73418725, 2.1859317, 0.74386317, 1.5435907, 1.1434015, 0.6532235, 3.630171, 2.0147347, 2.4291306, 0.48114604], [0.71523684, 2.3333707, 2.1253788, 0.7241842, 2.1589665, 0.73464864, 1.5221395, 1.1285954, 0.64500076, 3.5853338, 1.9899198, 2.3967838, 0.47281328], [0.7057337, 2.3034964, 2.0981853, 0.71435386, 2.1323702, 0.72559434, 1.5010245, 1.114023, 0.63692695, 3.5411167, 1.9654422, 2.3649037, 0.46464175], [0.6963958, 2.274056, 2.071379, 0.70469105, 2.1061368, 0.716695, 1.4802378, 1.09968, 0.6289981, 3.4975123, 1.9412957, 2.3334801, 0.45662954], [0.6872212, 2.2450407, 2.0449524, 0.6951931, 2.0802617, 0.70794475, 1.4597747, 1.0855628, 0.62120867, 3.454507, 1.9174746, 2.302506, 0.44877306], [0.67820525, 2.2164407, 2.0189006, 0.68585515, 2.0547373, 0.69934165, 1.4396273, 1.0716635, 0.613555, 3.4120882, 1.8939741, 2.2719746, 0.44106698], [0.6693421, 2.188249, 1.9932188, 0.6766729, 2.0295599, 0.6908824, 1.4197887, 1.0579754, 0.60603553, 3.370247, 1.8707907, 2.2418776, 0.43350768], [0.6606317, 2.1604564, 1.9679008, 0.6676446, 2.0047228, 0.68256277, 1.4002548, 1.0444982, 0.59864616, 3.3289733, 1.8479179, 2.2122111, 0.4260926], [0.652071, 2.1330564, 1.9429362, 0.65876615, 1.98022, 0.6743809, 1.381021, 1.0312315, 0.5913848, 3.2882586, 1.8253509, 2.182968, 0.4188225], [0.643655, 2.1060412, 1.9183201, 0.65003395, 1.9560443, 0.66633254, 1.3620801, 1.0181673, 0.58424884, 3.2480931, 1.8030839, 2.1541386, 0.411692], [0.6353805, 2.079403, 1.8940467, 0.6414447, 1.9321901, 0.6584134, 1.3434284, 1.0053011, 0.57723385, 3.2084675, 1.781113, 2.1257186, 0.4046968], [0.6272436, 2.053137, 1.870114, 0.6329973, 1.9086516, 0.65062106, 1.3250602, 0.99262875, 0.57033825, 3.169375, 1.7594353, 2.0977008, 0.39783713], [0.6192415, 2.0272346, 1.8465148, 0.6246877, 1.8854233, 0.64295405, 1.3069689, 0.98014927, 0.56356144, 3.1308079, 1.7380444, 2.0700803, 0.39111218], [0.61137027, 2.001688, 1.8232433, 0.6165109, 1.8625001, 0.6354119, 1.2891486, 0.9678613, 0.55690163, 3.092759, 1.7169367, 2.042849, 0.38451573], [0.60362834, 1.9764937, 1.8002933, 0.60846496, 1.8398776, 0.6279884, 1.2715955, 0.9557616, 0.55035484, 3.055217, 1.6961095, 2.0160005, 0.37804267], [0.5960149, 1.9516487, 1.7776583, 0.60054743, 1.8175528, 0.62068033, 1.2543055, 0.9438453, 0.5439172, 3.0181735, 1.6755571, 1.989526, 0.37169084], [0.5885293, 1.9271473, 1.7553304, 0.5927592, 1.7955217, 0.61348534, 1.2372756, 0.93210703, 0.5375868, 2.9816186, 1.655272, 1.9634184, 0.36545795], [0.5811667, 1.902982, 1.7333058, 0.5850948, 1.7737784, 0.6064017, 1.2204995, 0.9205414, 0.5313609, 2.9455433, 1.6352507, 1.937673, 0.35934132], [0.5739255, 1.8791453, 1.711583, 0.57755077, 1.7523179, 0.59942794, 1.2039729, 0.9091405, 0.525239, 2.9099414, 1.6154908, 1.9122846, 0.35334143], [0.56680304, 1.8556356, 1.6901629, 0.5701241, 1.7311352, 0.59256506, 1.1876913, 0.8978993, 0.5192195, 2.8748085, 1.5959892, 1.8872474, 0.3474564], [0.5597965, 1.8324444, 1.6690354, 0.56281227, 1.7102268, 0.5858072, 1.1716497, 0.886819, 0.51329845, 2.8401353, 1.576739, 1.8625546, 0.34168464], [0.55290145, 1.8095657, 1.6481943, 0.55561364, 1.6895877, 0.57914907, 1.1558442, 0.8758986, 0.5074743, 2.8059137, 1.5577372, 1.8382009, 0.33602512], [0.5461179, 1.7869958, 1.6276336, 0.5485297, 1.6692137, 0.5725886, 1.1402727, 0.8651352, 0.50174516, 2.7721388, 1.5389805, 1.8141829, 0.33047166], [0.5394427, 1.7647277, 1.6073493, 0.54155654, 1.6491007, 0.56612575, 1.12493, 0.85452527, 0.49610886, 2.738803, 1.5204645, 1.7904946, 0.3250202], [0.5328724, 1.7427547, 1.5873364, 0.5346945, 1.6292455, 0.5597589, 1.10981, 0.84406614, 0.49056247, 2.7058961, 1.502184, 1.7671293, 0.31966832], [0.52640396, 1.7210727, 1.5675913, 0.5279381, 1.6096437, 0.55348605, 1.0949093, 0.8337542, 0.48510402, 2.6734114, 1.4841381, 1.7440842, 0.31441522], [0.52003556, 1.6996769, 1.5481089, 0.5212829, 1.5902936, 0.5473049, 1.080225, 0.823587, 0.47973242, 2.641341, 1.4663237, 1.7213539, 0.3092582], [0.51376665, 1.6785645, 1.5288863, 0.5147271, 1.5711911, 0.54121655, 1.0657527, 0.81356335, 0.47444898, 2.6096816, 1.4487354, 1.698933, 0.30419552], [0.50759935, 1.6577327, 1.5099192, 0.50827235, 1.5523317, 0.5352179, 1.051489, 0.8036821, 0.46924958, 2.5784273, 1.4313679, 1.6768153, 0.29922512], [0.50153166, 1.6371738, 1.4912071, 0.5019157, 1.5337129, 0.52930653, 1.0374328, 0.79394054, 0.46413267, 2.547569, 1.4142199, 1.6549977, 0.29434583], [0.4955608, 1.616883, 1.4727464, 0.49565506, 1.5153341, 0.5234784, 1.0235814, 0.7843335, 0.45909813, 2.5171013, 1.3972894, 1.6334777, 0.28955626], [0.48968428, 1.5968574, 1.4545339, 0.4894877, 1.497189, 0.5177334, 1.0099314, 0.7748592, 0.45414335, 2.4870207, 1.3805732, 1.6122484, 0.2848547], [0.48389933, 1.5770893, 1.4365624, 0.4834115, 1.4792758, 0.5120699, 0.9964776, 0.76551783, 0.44926476, 2.4573243, 1.3640664, 1.591304, 0.28023875], [0.47820172, 1.557577, 1.4188275, 0.47742215, 1.461589, 0.50648624, 0.98321694, 0.7563077, 0.44446006, 2.4280055, 1.3477666, 1.5706404, 0.2757117], [0.47258776, 1.5383173, 1.4013277, 0.4715178, 1.4441249, 0.5009804, 0.97014594, 0.74722534, 0.4397276, 2.3990576, 1.3316709, 1.5502542, 0.2712681], [0.46705878, 1.5193062, 1.3840585, 0.46569756, 1.4268821, 0.49555138, 0.9572599, 0.7382668, 0.43506658, 2.3704753, 1.3157774, 1.5301421, 0.26690835], [0.4616112, 1.500541, 1.3670166, 0.45995823, 1.4098557, 0.49019957, 0.9445567, 0.7294293, 0.4304799, 2.3422549, 1.3000883, 1.5102986, 0.26262987], [0.4562442, 1.482019, 1.3502011, 0.45429966, 1.3930418, 0.48492375, 0.93203413, 0.72071314, 0.4259629, 2.3143897, 1.2845961, 1.4907185, 0.25842938], [0.45095578, 1.4637346, 1.3336043, 0.44872266, 1.3764379, 0.47972053, 0.91968757, 0.7121169, 0.42151284, 2.2868724, 1.2692971, 1.4713982, 0.2543069], [0.44574627, 1.4456838, 1.3172218, 0.4432242, 1.360038, 0.4745904, 0.9075143, 0.7036357, 0.41712934, 2.2596977, 1.254189, 1.4523337, 0.25026274]], 'val': [[8.119839, 6.4994555, 7.7370186, 4.9806237, 6.6764727, 3.4713752, 6.250762, 9.236686, 6.4858503, 9.213117, 8.219168, 5.1341176, 8.940729], [6.2416987, 6.7758536, 4.3092384, 4.7247176, 7.977654, 8.881838, 8.022008, 6.925857, 4.644946, 7.7387156, 8.905577, 7.608957, 8.872054], [9.38496, 7.8539925, 4.2984047, 7.064474, 6.1467776, 5.452308, 9.118808, 7.300804, 8.671402, 6.1330233, 3.3879247, 3.3705852, 6.9685717], [3.323182, 6.269959, 3.479879, 8.703268, 7.5794334, 3.372943, 5.5080504, 5.355778, 7.425199, 6.096744, 8.256565, 9.106838, 5.787902], [6.041958, 6.2036405, 6.2867866, 7.6012383, 4.0133996, 6.1381106, 4.5087605, 5.5709643, 6.8589916, 10.551996, 8.474627, 7.7777176, 8.197991], [7.3260193, 5.9156885, 6.6363773, 10.37748, 5.688651, 5.973253, 7.2139997, 6.570174, 3.9609213, 4.454914, 4.684477, 4.250682, 4.2073393], [4.362866, 5.956788, 6.0712137, 7.326288, 5.9517474, 5.1740375, 6.0361857, 4.550296, 5.931761, 5.841447, 8.74723, 4.3968277, 5.419058], [6.6875796, 6.7010374, 6.6649895, 4.5137415, 9.192666, 3.0412774, 8.096149, 4.410282, 6.6213613, 5.5976, 6.5782466, 7.9940434, 7.0838494], [3.649735, 9.682831, 6.9543524, 6.8391414, 7.086441, 11.012235, 8.143796, 4.492911, 6.9848137, 3.6997857, 3.067079, 5.6214995, 4.2191358], [6.8523746, 5.8512154, 7.182226, 4.1831317, 5.8997054, 2.931995, 3.718215, 7.854601, 5.8053885, 9.800747, 5.813057, 6.8384504, 5.4499083], [6.773993, 6.592623, 4.3364244, 11.074308, 6.0648875, 4.5782375, 7.5854707, 6.221571, 5.4020376, 3.6579647, 2.8225117, 6.5184045, 4.6002913], [4.28826, 5.515314, 6.9511843, 5.5263567, 5.4239435, 8.026075, 4.135061, 2.8152294, 5.011051, 5.0689945, 7.820992, 4.8389225, 5.288796], [6.5522947, 4.2199054, 5.5929585, 5.3870783, 7.153913, 6.318914, 5.1365714, 4.123452, 7.880489, 3.5223079, 6.10202, 6.397574, 9.528346], [7.912498, 3.882539, 6.612521, 6.5184546, 4.103771, 7.3508997, 6.6661005, 3.883677, 5.156327, 4.584896, 6.5206275, 6.5110726, 5.4111366], [6.030034, 7.879784, 5.0761895, 3.8866045, 2.669836, 2.7872858, 6.5447726, 5.849013, 6.622574, 2.79109, 10.867943, 4.996767, 5.553514], [10.230921, 6.496788, 3.2972827, 4.606064, 5.3689065, 2.696024, 4.5747647, 5.999721, 2.7654037, 7.2752676, 8.410018, 5.9108443, 2.7741017], [10.167195, 5.23883, 6.734459, 4.888363, 4.9994106, 5.1412373, 7.4675174, 6.149746, 5.105021, 5.764493, 5.7414947, 5.814148, 5.622064], [3.9929209, 6.1699805, 2.6900048, 5.1933937, 7.3326974, 7.3303823, 7.563312, 3.8322558, 4.992318, 3.0578413, 4.9412084, 5.853045, 6.094187], [3.6345458, 7.277839, 5.7046547, 4.8227215, 4.88309, 2.670917, 7.170952, 8.582713, 4.7626033, 6.882371, 5.0777655, 7.29911, 4.740118], [4.8475375, 4.7067785, 7.2892547, 3.756422, 7.3727155, 5.3136854, 4.2795, 4.466124, 4.321417, 6.5909386, 7.058676, 4.9263296, 4.0087276], [5.8883843, 4.599235, 4.8586164, 3.613031, 4.9433384, 4.9897738, 2.5335598, 3.6599321, 5.299724, 6.9709597, 3.7902396, 4.9623737, 4.6952534], [5.298141, 2.579736, 3.6642473, 7.1424456, 7.245219, 5.1972094, 3.019991, 6.2021637, 3.5381687, 5.4178853, 2.397914, 5.73787, 5.301505], [2.4601362, 5.81188, 4.7771077, 2.9630866, 5.2068825, 5.7084327, 5.160745, 3.5055456, 4.714322, 6.575879, 2.4950125, 6.4997797, 2.523746], [7.8604655, 4.8846273, 2.4345899, 4.548128, 6.728546, 2.5043702, 7.880673, 6.527494, 4.3552265, 2.4277554, 4.3227496, 4.327864, 4.0156364], [6.215248, 5.5082455, 6.608685, 6.5734096, 5.0772347, 3.6963573, 3.3560767, 6.935295, 3.2141979, 7.240966, 7.433609, 4.9715595, 5.6558104], [6.258034, 3.30713, 3.4700513, 3.9010348, 4.703101, 4.0187917, 6.4055037, 4.479992, 4.2985415, 5.499282, 2.8971193, 4.244819, 3.4733288], [5.5721965, 3.4840236, 7.4663234, 5.91378, 3.3733945, 6.3582134, 4.4434123, 4.511248, 5.4627414, 2.4462256, 6.27583, 5.5528336, 5.6931353], [7.485315, 4.3400993, 6.0106916, 6.1877756, 4.40325, 2.3254254, 2.7968214, 3.4284987, 2.135283, 2.8148522, 2.6813657, 3.1649158, 5.8790936], [6.628664, 6.473187, 6.4056835, 4.784267, 2.2073975, 5.1567907, 3.8190415, 6.4128056, 4.29411, 3.0397944, 4.2357545, 4.090508, 4.809333], [2.7035542, 4.2042627, 5.2366247, 6.3330827, 4.307979, 5.2314143, 2.772041, 3.2028856, 3.1258073, 3.0926542, 5.8746934, 6.1593914, 6.2217135], [3.3476405, 7.2678113, 2.9569921, 6.7890806, 5.6297054, 5.287313, 4.182618, 8.241093, 2.723484, 4.308379, 4.92523, 8.98729, 2.1852562], [3.619964, 1.9005227, 3.57795, 4.9132724, 3.7405996, 3.0173423, 4.949913, 6.1574426, 3.7464995, 4.0468698, 2.1155283, 6.6465645, 2.081764], [8.211598, 2.9196448, 6.5512323, 3.1795692, 3.0008729, 6.09697, 5.183025, 3.417541, 4.4630065, 5.2613325, 2.1427705, 2.9493296, 4.1664195], [4.1678534, 4.055002, 4.180874, 2.0898273, 3.8555489, 5.9551063, 6.444994, 6.872993, 3.970882, 3.6055322, 5.6668363, 7.0650277, 4.532648], [3.4684894, 2.9227471, 2.1014996, 2.0906897, 3.8836498, 2.9580674, 3.880932, 3.0720983, 3.887815, 3.4480524, 4.914378, 4.7087383, 3.9093685], [3.824954, 3.3543355, 5.9068685, 3.897081, 4.766068, 3.0206788, 4.0753756, 3.914877, 2.71393, 3.707001, 6.133161, 3.9496322, 2.8577745], [2.0535443, 5.036501, 3.9808297, 2.4062407, 6.6163177, 3.4766362, 5.8262377, 3.529927, 3.0425134, 2.826753, 2.7646158, 2.9631112, 5.9076996], [3.2936583, 2.8317394, 1.9863075, 2.8152657, 3.2434013, 4.7160406, 4.175349, 1.9100051, 2.7106512, 5.502504, 5.1500564, 6.553876, 6.659602], [4.267888, 3.8815885, 7.221616, 4.66714, 4.6153803, 4.702695, 3.4023678, 2.6737475, 3.6086452, 2.8340912, 2.812583, 5.61909, 5.7956505], [5.0125675, 3.6428242, 2.2949305, 4.5122113, 5.05127, 3.4952664, 1.7858021, 2.668006, 3.6244953, 2.8190203, 4.3997936, 5.428019, 4.748271], [2.824894, 3.7399766, 4.434328, 4.248302, 3.5980508, 2.67156, 7.7888117, 3.7581334, 1.898155, 4.5234613, 2.7072692, 7.1436205, 4.4847894], [3.2037082, 3.8370845, 3.4952006, 1.8264855, 3.6366456, 5.0738254, 5.376877, 4.342446, 2.6477418, 5.308022, 3.5329168, 5.0480576, 5.2070236], [4.0027847, 4.4613314, 3.6607509, 3.3673074, 2.6642091, 3.9100244, 2.8284068, 3.0334249, 2.1802893, 2.6005833, 3.3024936, 6.5288687, 4.489656], [4.3757524, 4.805817, 5.2092514, 3.6231527, 5.838603, 3.0982985, 2.204228, 1.7670723, 3.2960517, 5.2274427, 4.173381, 3.7307022, 2.2144003], [2.6272867, 2.4518144, 1.6852236, 4.1145988, 2.5036626, 2.4377446, 2.4258707, 4.9577727, 5.7941823, 5.6005683, 3.124955, 1.6539336, 3.0419405], [4.1585407, 4.8920293, 3.3596537, 1.7626355, 1.6872783, 3.973793, 3.3448017, 3.473588, 3.4792507, 5.467283, 3.4132671, 1.6450276, 3.443108], [2.4218786, 3.4015038, 4.087077, 1.6566064, 1.9540795, 4.8386397, 4.2205815, 3.7018456, 4.991972, 4.1161284, 2.6033926, 1.6560738, 4.0953035], [4.863361, 1.6762893, 3.3082285, 2.090065, 2.5798173, 2.9800086, 6.781434, 6.000047, 2.5359573, 2.3605602, 4.0680733, 3.2107542, 2.2750993], [3.3608763, 3.7897344, 2.4779248, 3.3836584, 2.4350827, 1.5852417, 3.339796, 4.2031374, 2.4316554, 3.239476, 3.650955, 3.2029433, 4.0397987], [2.453562, 2.7835553, 2.8167806, 2.0603151, 5.0556164, 3.2558868, 2.6979172, 3.3621461, 1.5833857, 4.90409, 4.7501197, 2.277845, 3.2051768], [3.45073, 4.6711664, 3.9753344, 2.809826, 4.2017655, 2.4447896, 1.9894801, 3.170608, 4.5323486, 3.3137221, 2.3807833, 5.8882213, 1.9159123], [5.048579, 2.89178, 3.8516586, 3.5222654, 2.375467, 4.757863, 4.101087, 4.6554785, 4.0996327, 5.24925, 4.0112877, 2.1846066, 2.6792798], [4.498232, 3.8773522, 3.2246451, 2.6929276, 2.6384783, 3.4725547, 2.4114108, 1.9119005, 4.7407303, 3.079848, 4.759697, 2.2979937, 3.10139], [1.9098713, 6.3210387, 1.9108238, 3.658124, 5.105767, 3.7457142, 4.4930897, 2.957511, 2.372565, 2.4378302, 1.4163015, 6.259585, 5.275598], [2.797668, 3.4602396, 3.052708, 3.9541347, 3.1356509, 5.3326116, 2.1542423, 3.8455968, 3.784421, 5.1438346, 3.8704772, 2.9861472, 5.1271553], [2.3630638, 3.6695368, 1.4155273, 4.273447, 4.578461, 3.6422422, 1.439596, 2.5362668, 2.8670983, 1.5962412, 2.657174, 3.616861, 4.4399157], [3.350139, 3.7782183, 2.956901, 2.7356339, 3.0009997, 1.405976, 3.7411819, 3.1306002, 1.4466846, 2.9136057, 2.8573964, 5.789577, 3.0700908], [2.1197162, 2.155733, 3.6497219, 2.2661655, 3.9861038, 2.9798794, 5.2094364, 3.6085758, 2.184394, 2.8572571, 2.7467563, 2.6239903, 2.945806], [1.3110954, 4.577046, 3.5052714, 1.3556812, 2.906805, 3.6925023, 2.894828, 2.7248564, 5.6545067, 2.776343, 1.4139252, 4.522539, 3.430616], [3.9097269, 2.5815594, 3.501486, 2.8511834, 2.071735, 1.4302197, 4.3521934, 3.5194864, 4.367197, 2.4793994, 2.2185912, 3.49866, 3.3957834], [3.1390514, 4.561515, 3.6499348, 4.8701344, 3.5485244, 4.8596272, 2.0346248, 3.817764, 1.9592001, 2.8545551, 4.5875287, 1.3135477, 2.6573882], [4.823054, 2.8114302, 5.8747144, 2.01017, 3.2993288, 2.9691727, 3.3516672, 2.6185148, 2.7094512, 1.7186126, 4.038575, 2.7339113, 2.9064662], [3.0272985, 2.6530833, 4.1701903, 2.2427633, 3.4758458, 1.9890054, 2.429048, 3.1902125, 1.2258213, 3.4405472, 2.5024667, 3.4182298, 1.6741688], [4.296204, 3.2524254, 2.406508, 3.2433386, 3.803592, 2.5411015, 3.3141797, 2.5499613, 1.9657199, 1.85946, 1.9948454, 3.3854105, 2.5593507], [2.5751717, 1.8667696, 2.5655594, 3.3675761, 4.081793, 1.2311786, 2.611972, 1.9319392, 2.5786164, 2.304294, 2.6038249, 3.1725407, 3.3321712], [2.214854, 2.5418482, 4.3072305, 1.1813167, 3.431757, 1.8044456, 3.2745757, 1.9621938, 1.6346253, 1.8642997, 2.5600176, 2.3223355, 1.8697903], [3.9057908, 1.910435, 1.2299714, 3.2378147, 1.7814257, 2.2515013, 4.4393067, 3.8379521, 3.4157162, 4.5835567, 3.6925406, 2.3131201, 2.832379], [2.5206017, 3.7523346, 1.5310882, 4.491617, 4.414247, 2.108542, 2.955158, 3.4548144, 2.1220312, 2.9098217, 1.7155373, 2.671396, 3.0846713], [4.1815314, 3.134931, 2.2398765, 3.2067766, 3.0172505, 2.8797934, 3.0892181, 2.6774971, 1.7472098, 2.504754, 2.4269218, 1.5102539, 3.7122726], [1.7535563, 2.5552492, 4.814304, 3.4302382, 2.4204993, 1.7284244, 4.648043, 2.863388, 3.2378945, 1.7071728, 3.2275252, 1.904506, 3.1127014], [2.361642, 1.7783241, 2.9873722, 1.7906048, 3.0356429, 2.0642858, 1.7145447, 2.5837317, 3.0356345, 2.4058816, 5.09138, 2.6230173, 1.4267666], [3.802431, 2.1278868, 2.2170858, 3.9909532, 2.31996, 3.031104, 1.0998031, 5.572918, 1.6746912, 2.4836388, 2.990594, 3.0934486, 1.0698906], [2.6549664, 1.1001471, 4.100344, 2.9341366, 1.4640653, 1.9894981, 4.1511965, 1.2021635, 2.9274218, 2.3485956, 1.1871035, 3.6256354, 2.81398], [3.0092633, 2.3912358, 3.0779076, 2.684291, 2.8191774, 2.7587013, 2.6764607, 2.9266973, 2.345122, 2.8063235, 4.0629625, 1.911133, 2.3481212], [1.1887796, 2.4229903, 3.5829966, 2.4384255, 2.3495529, 2.9265437, 1.1106049, 1.5995836, 2.4059398, 4.038942, 1.6383256, 2.2875955, 3.6678836], [1.833588, 2.8368256, 3.368125, 2.158268, 2.9354281, 2.788126, 3.5257332, 1.71118, 3.549203, 2.473007, 2.612344, 4.0569944, 1.637711], [1.7601713, 2.754009, 1.5984691, 1.0459644, 2.858636, 2.254262, 2.6430538, 2.5696025, 2.2477458, 2.3146002, 1.1357723, 2.8788586, 1.8090955], [4.845126, 4.601461, 1.6063797, 1.3809661, 2.1243415, 2.275611, 1.1551037, 3.0322914, 2.840712, 1.1167881, 3.9082274, 1.6949365, 2.7645516], [2.4534483, 2.6315339, 2.8706884, 2.8768702, 3.1744132, 1.6143835, 2.6074104, 2.840936, 1.3223214, 2.1638227, 2.0605707, 1.9954653, 2.841956], [3.8065193, 1.2628359, 2.9215446, 3.9280899, 1.3524033, 2.208263, 2.0522163, 2.6893926, 1.6210953, 1.3268751, 1.8540031, 1.8089308, 2.1604648], [1.3450905, 1.8042728, 2.2103903, 2.63627, 1.47878, 3.42198, 2.574697, 1.0159203, 2.05423, 2.78098, 1.6448568, 2.346024, 3.1384804], [1.2337177, 1.7167703, 0.9964603, 2.2772048, 1.5462452, 2.1671112, 3.5114422, 1.4545542, 1.4685522, 2.1943438, 2.063034, 2.0776825, 3.4271991], [1.975374, 1.4830278, 2.7989876, 2.032973, 2.7686536, 2.3758352, 2.3339276, 1.5669692, 0.9870068, 3.5175824, 1.5777019, 1.5440984, 1.5280123], [2.0878506, 2.783702, 1.2729824, 2.0643744, 0.894442, 3.6801453, 2.7472656, 1.2854928, 2.1122286, 1.9243239, 1.4884398, 1.0526711, 3.1944728], [1.4298694, 2.657575, 4.1865206, 1.632033, 1.5397714, 1.4577391, 1.4656086, 0.979368, 1.1446322, 1.9815742, 1.0968686, 1.6767783, 2.4143188], [0.8898601, 1.4404191, 3.5810814, 1.4919972, 2.3548412, 0.98155075, 3.000263, 0.9705274, 2.0337598, 2.4942188, 1.7402842, 0.975688, 1.3868539], [3.0233815, 1.9660472, 1.7030077, 3.1377048, 0.9319519, 2.5040216, 2.6915984, 2.0799444, 1.7745764, 1.5786631, 2.9614184, 1.9979826, 2.3752682], [2.30927, 1.1491437, 2.3820038, 2.8508596, 2.9698138, 1.4556359, 1.9410124, 1.3622491, 0.92899156, 3.103486, 2.453052, 1.3995236, 1.8728582], [3.0034766, 1.5779856, 2.4351177, 1.6724502, 1.821546, 1.5986592, 2.8605483, 1.9974024, 1.3136262, 2.0656343, 2.9324424, 3.377638, 2.5665736], [2.9902103, 0.97145283, 2.6257439, 2.0110257, 1.3146349, 1.1041152, 1.2970747, 1.990162, 0.94487226, 1.8356736, 1.2212403, 1.3545281, 1.932377], [2.2846003, 2.3819187, 2.4694428, 2.385377, 1.85823, 2.3012516, 2.384387, 3.9069462, 2.4278998, 1.8824962, 2.8806436, 1.4904357, 1.689392], [1.7530887, 1.5059578, 1.8515831, 3.3000655, 1.5742584, 2.6242392, 1.8374492, 2.5893366, 1.3204496, 2.5254958, 2.3024669, 3.0713549, 1.424787], [2.3844109, 2.0489268, 1.8924598, 3.830666, 3.2726471, 2.2939565, 2.3534887, 0.87327594, 2.0525422, 1.4382957, 2.3502383, 2.8233907, 0.8732159], [2.0615168, 1.0611645, 1.0788316, 2.232822, 1.9606434, 1.1294312, 1.2875245, 1.3339097, 1.2237142, 2.2041337, 1.8643284, 1.853383, 1.3281571], [2.2020872, 1.0316304, 2.2905185, 1.3052657, 2.584798, 1.7921679, 2.6028943, 2.7937388, 1.659909, 3.255298, 1.2064242, 0.9261302, 2.2964277], [0.91883826, 2.2258458, 2.3327792, 2.1669931, 3.4457083, 1.3388954, 2.9825792, 1.8291166, 1.2479181, 1.7792481, 2.3016708, 1.7029995, 1.3904043], [2.2234728, 1.7701259, 1.7368761, 1.7696166, 1.3519516, 2.153086, 0.79241055, 1.9365178, 1.8922406, 1.7350892, 2.2488005, 1.4842212, 1.2301052], [1.2128832, 2.9492166, 1.9490004, 1.2162639, 1.8365653, 1.295065, 2.083723, 1.812826, 1.220536, 0.9551269, 1.3240256, 1.7502569, 0.7426545], [2.1003585, 2.5867035, 1.5394783, 3.1523275, 2.1617417, 2.1418824, 1.5944788, 1.7513467, 2.1726415, 2.0606866, 2.2104542, 1.6225541, 1.2076449], [1.5733646, 1.7200519, 1.1788327, 1.8477234, 1.834882, 1.2406375, 1.2349683, 1.2743708, 1.3058004, 1.6176226, 0.7519336, 1.6940454, 0.9653162]], 'val_mse': [[6.3431063, 4.725146, 5.965098, 3.2110689, 4.909275, 1.7065291, 4.4882665, 7.4765387, 4.728043, 7.457646, 6.4660287, 3.3833072, 7.1922374], [4.495523, 5.031992, 2.5676904, 2.9854808, 6.240724, 7.147211, 6.2896833, 5.1958323, 2.917217, 6.013281, 7.1824327, 5.888102, 7.153481], [7.6686635, 6.139971, 2.5866566, 5.3549967, 4.4395676, 3.7473607, 7.416123, 5.600378, 6.9732313, 4.437107, 1.6942595, 1.6791692, 5.2793975], [1.6362462, 4.5852604, 1.7974149, 7.0230355, 5.901428, 1.6971607, 3.834489, 3.6844344, 5.756069, 4.4298267, 6.5918574, 7.4443383, 4.1276026], [4.383856, 4.5477347, 4.6330767, 5.9497213, 2.3640711, 4.4909663, 2.8637986, 3.9281816, 5.2183824, 8.913559, 6.8383594, 6.1436195, 6.5660563], [5.6962414, 4.288067, 5.0109105, 8.754166, 4.0674853, 4.354231, 5.5971203, 4.9554343, 2.3483164, 2.8444426, 3.076136, 2.644471, 2.6032505], [2.7608967, 4.356938, 4.473482, 5.7306714, 4.358241, 3.5826375, 4.4468913, 2.963101, 4.346661, 4.2584424, 7.166318, 2.818008, 3.8423233], [5.1129255, 5.128463, 5.0944934, 2.9453223, 7.626317, 1.4769962, 6.533934, 2.850131, 5.0632696, 4.041566, 5.024267, 6.442117, 5.5339694], [2.1018987, 8.137037, 5.4106007, 5.2974286, 5.5467615, 9.474583, 6.6081705, 2.9593089, 5.453231, 2.170222, 1.5395325, 4.09597, 2.6956167], [5.3308616, 4.331708, 5.664724, 2.667631, 4.3862023, 1.4204859, 2.2086985, 6.3470736, 4.2998457, 8.297188, 4.3114786, 5.338852, 3.9522843], [5.278341, 5.0989423, 2.8447127, 9.584563, 4.577105, 3.092414, 6.1016045, 4.7396584, 3.9220762, 2.1799533, 1.3464478, 5.044287, 3.128114], [2.8180187, 4.047009, 5.4848156, 4.06192, 3.9614353, 6.5654907, 2.6764, 1.3584895, 3.5562267, 3.6160834, 6.3699927, 3.389835, 3.8416135], [5.1070156, 2.776528, 4.1514826, 3.9475021, 5.7162313, 4.8831234, 3.702671, 2.6914396, 6.45036, 2.0940611, 4.6756525, 4.9730854, 8.105731], [6.4917536, 2.4636633, 5.195514, 5.103314, 2.6904926, 5.9394803, 5.2565384, 2.475969, 3.750469, 3.1808875, 5.1184645, 5.110756, 4.01266], [4.6333947, 6.4849815, 3.6832232, 2.4954722, 1.2805331, 1.3998085, 5.159119, 4.46518, 5.240558, 1.4108896, 9.489555, 3.6201942, 4.178749], [8.857961, 5.1256313, 1.927928, 3.23851, 4.003148, 1.3320575, 3.2125888, 4.6393332, 1.4067999, 5.918449, 7.0549817, 4.557589, 1.4226223], [8.817489, 3.8908966, 5.388298, 3.543973, 3.6567864, 3.800374, 6.128414, 4.8123994, 3.7694263, 4.43065, 4.409401, 4.483804, 4.2934628], [2.6660597, 4.84486, 1.3666254, 3.8717535, 6.0127926, 6.0122085, 6.2468667, 2.5175383, 3.6793244, 1.7465693, 3.6316562, 4.5452127, 4.7880683], [2.330139, 5.9751444, 4.403671, 3.523446, 3.5855196, 1.3750479, 5.8767834, 7.2902417, 3.4718246, 5.593285, 3.790371, 6.0134053, 3.456097], [3.5651975, 3.4261181, 6.010275, 2.47912, 6.0970864, 4.0397263, 3.0072098, 3.1954994, 3.0524538, 5.323638, 5.7930346, 3.6623495, 2.7464023], [4.6277103, 3.3402114, 3.6012425, 2.3573053, 3.6892576, 3.7373343, 1.2827593, 2.410769, 4.052194, 5.725062, 2.5459719, 3.719736, 3.4542391], [4.058748, 1.3419642, 2.4280975, 5.9079156, 6.012304, 3.9659054, 1.7902992, 4.9740815, 2.311692, 4.193013, 1.1746441, 4.516202, 4.0814323], [1.2416564, 4.594993, 3.5618136, 1.7493829, 3.9947655, 4.4979, 3.9517956, 2.2981777, 3.508532, 5.3716664, 1.2923746, 5.298715, 1.3242496], [6.6625347, 3.6882617, 1.2397898, 3.3548908, 5.536869, 1.31425, 6.692109, 5.3404837, 3.1697664, 1.2438443, 3.140386, 3.1470475, 2.8363602], [5.037509, 4.332044, 5.4340224, 5.4002833, 3.9056406, 2.5262914, 2.1875386, 5.7682834, 2.0487082, 6.0769987, 6.2711625, 3.8106332, 4.4963994], [5.1001363, 2.1507432, 2.3151758, 2.7476687, 3.5512412, 2.8684356, 5.25665, 3.3326373, 3.1526828, 4.354919, 1.7542486, 3.103442, 2.3334398], [4.433792, 2.3471055, 6.330891, 4.7798305, 2.2409244, 5.227219, 3.3138945, 3.3832052, 4.3361692, 1.321124, 5.152197, 4.4306693, 4.572434], [6.3660746, 3.2223198, 4.8943725, 5.072915, 3.2898455, 1.213473, 1.6863202, 2.3194473, 1.0276765, 1.7086915, 1.576649, 2.0616422, 4.7772593], [5.5282674, 5.374227, 5.308161, 3.6881785, 1.1127406, 4.0635624, 2.727242, 5.322432, 3.2051585, 1.9522654, 3.1496458, 3.0058205, 3.7260602], [1.6216916, 3.1238117, 4.157586, 5.2554545, 3.231758, 4.1565976, 1.6986288, 2.1308753, 2.055195, 2.0234387, 4.8068724, 5.092965, 5.1566763], [2.283991, 6.2055492, 1.8961166, 5.729592, 4.5716, 4.2305875, 3.127272, 7.187124, 1.6708885, 3.257158, 3.8753822, 7.9388137, 1.1381453], [2.5742168, 0.85613877, 2.5349295, 3.871614, 2.7002997, 1.9783993, 3.9123259, 5.1212077, 2.711614, 3.0133333, 1.0833391, 5.6157227, 1.0522656], [7.183439, 1.892825, 5.525752, 2.1554272, 1.9780654, 5.075494, 4.16288, 2.3987262, 3.4455187, 4.2451706, 1.1279335, 1.9358166, 3.154224], [3.156974, 3.0454395, 3.1726267, 1.082894, 2.8499262, 4.950792, 5.441987, 5.871292, 2.9704835, 2.6064365, 4.6690407, 6.0685315, 3.5374463], [2.47458, 1.9301299, 1.110175, 1.1006571, 2.894905, 1.9706082, 2.8947582, 2.0872078, 2.9042041, 2.4657204, 3.9333222, 3.7289596, 2.9308617], [2.847716, 2.3783674, 4.9321694, 2.9236493, 3.793902, 2.049776, 3.105735, 2.9464982, 1.7468098, 2.7411385, 5.1685543, 2.98628, 1.8956724], [1.0926907, 4.076897, 3.0224736, 1.4491313, 5.6604514, 2.5220106, 4.872853, 2.5777829, 2.0916054, 1.8770806, 1.8161772, 2.0159051, 4.9617224], [2.3489072, 1.8882141, 1.0440093, 1.8741922, 2.3035495, 3.7774086, 3.2379375, 0.9738106, 1.7756722, 4.56874, 4.2175055, 5.622538, 5.7294717], [3.338963, 2.9538698, 6.2951035, 3.7418315, 3.6912744, 3.7797892, 2.4806604, 1.7532378, 2.689329, 1.9159677, 1.8956516, 4.7033505, 4.881097], [4.0991974, 2.7306373, 1.3839276, 3.602391, 4.14263, 2.587804, 0.8795184, 1.7628989, 2.7205617, 1.916259, 3.4982018, 4.527596, 3.8490136], [1.9267998, 2.8430457, 3.5385602, 3.3536966, 2.7046037, 1.7792699, 6.8976784, 2.8681548, 1.0093296, 3.6357892, 1.8207473, 6.2582483, 3.600561], [2.3206227, 2.955143, 2.6144023, 0.94682795, 2.7581275, 4.1964445, 4.500633, 3.467338, 1.7737659, 4.4351783, 2.661205, 4.1774774, 4.33757], [3.134456, 3.594126, 2.7946692, 2.5023491, 1.8003709, 3.0473037, 1.9668043, 2.17294, 1.320918, 1.7423264, 2.445348, 5.6728344, 3.6347296], [3.5219328, 3.953102, 4.357641, 2.7726462, 4.9891977, 2.2499933, 1.3570216, 0.92096376, 2.4510388, 4.383526, 3.330558, 2.8889728, 1.3737594], [1.7877312, 1.6133461, 0.84784204, 3.2783024, 1.6684492, 1.6036114, 1.5928166, 4.1257977, 4.9632816, 4.7707424, 2.2962036, 0.8262569, 2.2153327], [3.3330014, 4.0675583, 2.5362496, 0.94029844, 0.8660065, 3.1535838, 2.525655, 2.6555018, 2.6622217, 4.651311, 2.598352, 0.831167, 2.6302977], [1.6101173, 2.5907917, 3.2774146, 0.84799165, 1.1465094, 4.0321136, 3.415099, 2.897404, 4.18857, 3.3137653, 1.8020661, 0.8557839, 3.296046], [4.0651336, 0.8790926, 2.512063, 1.2949297, 1.7857109, 2.1869278, 5.9893785, 5.2090163, 1.7459478, 1.5715712, 3.2801027, 2.4238021, 1.4891615], [2.5759516, 3.0058234, 1.6950257, 2.6017718, 1.6542063, 0.8053736, 2.5609362, 3.4252844, 1.6548051, 2.4636297, 2.8761103, 2.4290996, 3.266952], [1.6817101, 2.0126991, 2.0469217, 1.2914509, 4.2877445, 2.4890068, 1.9320289, 2.597247, 0.81947297, 4.1411643, 3.988178, 1.5168893, 2.4452028], [2.6917353, 3.9131515, 3.2182987, 2.053769, 3.4466858, 1.6906847, 1.2363491, 2.4184504, 3.7811615, 2.5635061, 1.6315367, 5.139943, 1.1685977], [4.3022285, 2.1463926, 3.1072338, 2.778803, 1.6329654, 4.0163193, 3.3605022, 3.9158514, 3.360959, 4.5115304, 3.2745223, 1.4487942, 1.944416], [3.764315, 3.1443818, 2.4926221, 1.9618518, 1.908346, 2.743364, 1.6831621, 1.1845939, 4.014363, 2.3544195, 4.0352044, 1.5744377, 2.378767], [1.1881794, 5.600277, 1.1909926, 2.9392223, 4.3877935, 3.0286674, 3.7769692, 2.242316, 1.6582924, 1.7244803, 0.70387244, 5.548077, 4.5650063], [2.0879915, 2.7514796, 2.3448632, 3.2472057, 2.4296346, 4.627506, 1.4500468, 3.1423116, 3.082042, 4.4423623, 3.1699095, 2.2864842, 4.428393], [1.665201, 2.9725742, 0.71946365, 3.578283, 3.8841941, 2.9488714, 0.74712133, 1.8446867, 2.176411, 0.9064459, 1.9682686, 2.9288456, 3.7527854], [2.6638927, 3.0928576, 2.2724254, 2.0520415, 2.318289, 0.72414577, 3.060231, 2.450529, 0.7674898, 2.2352867, 2.1799524, 5.113009, 2.3943925], [1.444886, 1.4817717, 2.9766297, 1.5939428, 3.314748, 2.3093884, 4.53981, 2.9398139, 1.5164932, 2.1902177, 2.080576, 1.9586678, 2.2813387], [0.6474814, 3.9142857, 2.8433645, 0.6946277, 2.2466035, 3.0331516, 2.236328, 2.0672052, 4.9977036, 2.1203876, 0.7588153, 3.8682742, 2.7771904], [3.25714, 1.9298127, 2.8505793, 2.2011144, 1.4225018, 0.78182083, 3.704629, 2.8727565, 3.7212994, 1.8343327, 1.5743542, 2.8552532, 2.7532034], [2.4972963, 3.920584, 3.0098286, 4.2308526, 2.9100637, 4.221986, 1.397804, 3.1817627, 1.3240156, 2.2201889, 3.9539788, 0.68081295, 2.0254657], [4.1919436, 2.1811304, 5.245226, 1.3814918, 2.6714582, 2.3421094, 2.725411, 1.993065, 2.084805, 1.0947701, 3.4155354, 2.1116748, 2.285028], [2.4066565, 2.0332394, 3.5511441, 1.6245133, 2.8583903, 1.3723427, 1.8131765, 2.575133, 0.6115313, 2.8270473, 1.8897556, 2.8063068, 1.06303], [3.6858475, 2.6428518, 1.7977169, 2.6353307, 3.1963656, 1.9346539, 2.7085116, 1.9450716, 1.3616054, 1.2561216, 1.3922808, 2.7836199, 1.9583304], [1.9749199, 1.2672883, 1.9668479, 2.769634, 3.4846172, 0.6347681, 2.0163276, 1.3370608, 1.9845015, 1.7109427, 2.0112345, 2.5807114, 2.7410986], [1.6245373, 1.9522885, 3.718428, 0.59327054, 2.8444655, 1.2179064, 2.6887891, 1.3771594, 1.0503399, 1.2807636, 1.9772305, 1.7402966, 1.288495], [3.3252394, 1.3306284, 0.6509084, 2.6594944, 1.2038473, 1.6746637, 3.8632097, 3.2625954, 2.841096, 4.0096736, 3.1193933, 1.7407075, 2.2606988], [1.9496522, 3.182116, 0.96160233, 3.922863, 3.846222, 1.5412456, 2.38859, 2.888973, 1.5569149, 2.3454313, 1.15187, 2.1084523, 2.5224478], [3.6200264, 2.5741448, 1.6798081, 2.6474252, 2.4586165, 2.321875, 2.5320158, 2.1210105, 1.1914356, 1.9496932, 1.8725731, 0.95661825, 3.1593447], [1.201335, 2.0037363, 4.263499, 2.8801405, 1.8711069, 1.1797358, 4.100059, 2.3161082, 2.6913168, 1.1612973, 2.6823497, 1.3600312, 2.5689235], [1.8185592, 1.2359366, 2.4456804, 1.2496083, 2.4953394, 1.5246754, 1.1756262, 2.0455046, 2.4980977, 1.8690354, 4.555222, 2.0875478, 0.8919811], [3.2683287, 1.5944687, 1.6843513, 3.4589016, 1.7885898, 2.5004134, 0.5697929, 5.043588, 1.1460394, 1.9556644, 2.4632967, 2.5668263, 0.54394096], [2.1296892, 0.5755423, 3.576411, 2.4108758, 0.941475, 1.4675754, 3.6299427, 0.68157935, 2.4075048, 1.8293456, 0.6685183, 3.1077147, 2.296721], [2.492665, 1.8752984, 2.5626304, 2.169674, 2.305219, 2.2454007, 2.1638176, 2.4147105, 1.8337892, 2.2956452, 3.5529368, 1.4017599, 1.8393965], [0.6807022, 1.9155617, 3.076217, 1.9322945, 1.8440685, 2.4217048, 0.60641193, 1.096037, 1.9030367, 3.5366812, 1.136706, 1.7866169, 3.1675425], [1.3338842, 2.3377588, 2.8696945, 1.6604757, 2.4382725, 2.2916055, 3.0298476, 1.2159281, 3.0545824, 1.9790187, 2.1189866, 3.5642662, 1.1456091], [1.268696, 2.2631612, 1.1082486, 0.55637074, 2.3696666, 1.765917, 2.1553328, 2.0825055, 1.7612699, 1.828745, 0.6505367, 2.3942435, 1.3250967], [4.361742, 4.1186934, 1.1242288, 0.899431, 1.6434209, 1.795303, 0.6754079, 2.5532086, 2.3622396, 0.6389263, 3.4309745, 1.218293, 2.288514], [1.9780147, 2.1567054, 2.396465, 2.4032526, 2.7013993, 1.1419713, 2.1356008, 2.3697293, 0.851713, 1.693814, 1.5911611, 1.5266544, 2.3737402], [3.3388987, 0.795811, 2.455115, 3.462256, 0.88716304, 1.7436144, 1.5881604, 2.2259295, 1.1582233, 0.864593, 1.3923093, 1.3478261, 1.6999454], [0.88515526, 1.3449224, 1.7516252, 2.1780894, 1.0211825, 2.9649634, 2.1182616, 0.5600665, 1.5989554, 2.326285, 1.1907393, 1.8924838, 2.6855142], [0.78132606, 1.264953, 0.5452172, 1.8265365, 1.0961499, 1.7175868, 3.0624897, 1.006173, 1.0207398, 1.747101, 1.6163598, 1.6315758, 2.981657], [1.5303941, 1.0386107, 2.3551352, 1.5896848, 2.3259275, 1.9336708, 1.8923246, 1.125928, 0.54652524, 3.0776608, 1.1383376, 1.1052914, 1.0897596], [1.6501511, 2.346557, 0.8363905, 1.6283355, 0.45895502, 3.2452095, 2.3128812, 0.85166097, 1.6789466, 1.4915913, 1.0562553, 0.62103426, 2.76338], [0.9993205, 2.2275712, 3.757061, 1.2031181, 1.1113985, 1.0299067, 1.0383184, 0.55262, 0.7184238, 1.5559059, 0.6717386, 1.2521865, 1.9902629], [0.46633846, 1.017432, 3.158629, 1.0700802, 1.933458, 0.5607002, 2.5799456, 0.55074245, 1.6145066, 2.0754967, 1.3220922, 0.5580254, 0.9697166], [2.6067698, 1.5499619, 1.2874482, 2.7226703, 0.5174418, 2.0900345, 2.2781358, 1.6670065, 1.36216, 1.166769, 2.5500462, 1.5871317, 1.9649343], [1.899452, 0.7398423, 1.9732196, 2.442593, 2.5620635, 1.0483997, 1.5342914, 0.95604473, 0.52330047, 2.6983097, 2.0483875, 0.9953715, 1.4692163], [2.600342, 1.1753591, 2.0330007, 1.2708421, 1.4204447, 1.1980648, 2.4604604, 1.5978208, 0.9145495, 1.6670634, 2.5343742, 2.9800735, 2.1695092], [2.5936449, 0.5753879, 2.2301793, 1.6159608, 0.92006874, 0.7100461, 0.9035037, 1.5970902, 0.5522956, 1.4435923, 0.8296537, 0.96343625, 1.5417758], [1.8944896, 1.9922996, 2.0803142, 1.9967399, 1.4700826, 1.9135928, 1.9972169, 3.5202649, 2.0417051, 1.4967898, 2.495423, 1.1056998, 1.3051382], [1.369317, 1.1226685, 1.4687761, 2.9177403, 1.1924126, 2.242874, 1.4565647, 2.208932, 0.94052273, 2.1460469, 1.9234957, 2.6928616, 1.0467683], [2.006865, 1.6718553, 1.5158625, 3.4545426, 2.896996, 1.918777, 1.9787806, 0.4990403, 1.678776, 1.0649993, 1.977411, 2.4510317, 0.5013234], [1.6900898, 0.69020313, 0.7083353, 1.8627913, 1.5910772, 0.760328, 0.91888535, 0.96573347, 0.85599834, 1.8368802, 1.4975348, 1.4870491, 0.962279], [1.8366648, 0.6666649, 1.9260113, 0.94121504, 2.2212026, 1.4290272, 2.2402081, 2.4315076, 1.2981315, 2.8939743, 0.8455531, 0.5657123, 1.936458], [0.559316, 1.8667732, 1.9741558, 1.8088185, 3.0879815, 0.98161465, 2.6257448, 1.4727292, 0.8919757, 1.4237512, 1.9466184, 1.3483907, 1.036235], [1.8697441, 1.4168375, 1.3840286, 1.4172105, 0.99998415, 1.8015575, 0.4413221, 1.5858691, 1.5420285, 1.385315, 1.899463, 1.1353198, 0.88163763], [0.8648484, 2.6016154, 1.601833, 0.8695299, 1.4902623, 0.9491937, 1.7382843, 1.4678199, 0.87596077, 0.61098176, 0.9803091, 1.4069697, 0.39979488], [1.7579241, 2.2446954, 1.197896, 2.8111713, 1.8210113, 1.8015759, 1.2545965, 1.4118887, 1.8336067, 1.7220749, 1.8722649, 1.2847868, 0.8702968], [1.2364346, 1.3835402, 0.8427393, 1.5120493, 1.4996254, 0.90579665, 0.900545, 0.9403639, 0.9722082, 1.2844466, 0.41917148, 1.3616976, 0.63337994]]}\n"
     ]
    }
   ],
   "source": [
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W', 'alpha', 'eps', 'y_hat'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screenshot.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out Pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 14:34:29.044140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 14:34:36.527694: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-12 14:34:58.904985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-07-12 14:34:58.906728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cm/shared/apps/lsf10/10.1/linux3.10-glibc2.17-x86_64/lib:/data/weirauchlab/opt/lib:/data/weirauchlab/opt/lib64:/data/weirauchlab/local/lib\n",
      "2023-07-12 14:34:58.906759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/ngun7t/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "================================================================================\n",
      "   _____     _ _ ____              \n",
      "  / ____|   | | |  _ \\             \n",
      " | |     ___| | | |_) | _____  __  \n",
      " | |    / _ \\ | |  _ < / _ \\ \\/ /  \n",
      " | |___|  __/ | | |_) | (_) >  <   \n",
      "  \\_____\\___|_|_|____/ \\___/_/\\_\\  \n",
      "Running CellBox scripts developed in Sander lab\n",
      "Maintained by Bo Yuan, Judy Shen, and Augustin Luna; contributions by Daniel Ritter\n",
      "\n",
      "        version 0.3.2\n",
      "        -- Feb 10, 2023 --\n",
      "        * Modify CellBox to support TF2     \n",
      "        \n",
      "Tutorials and documentations are available at https://github.com/sanderlab/CellBox\n",
      "If you want to discuss the usage or to report a bug, please use the 'Issues' function at GitHub.\n",
      "If you find CellBox useful for your research, please consider citing the corresponding publication.\n",
      "For more information, please email us at boyuan@g.harvard.edu and c_shen@g.harvard.edu, augustin_luna@hms.harvard.edu\n",
      " --------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cellbox\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import argparse\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "from cellbox.utils import TimeLogger\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.errors import OutOfRangeError\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'experiment_id': 'Example_RP', 'model_prefix': 'seed', 'ckpt_name': 'model11.ckpt', 'export_verbose': 3, 'experiment_type': 'random partition', 'sparse_data': False, 'batchsize': 4, 'trainset_ratio': 0.7, 'validset_ratio': 0.8, 'n_batches_eval': None, 'add_noise_level': 0, 'dT': 0.1, 'ode_solver': 'heun', 'envelope_form': 'tanh', 'envelope': 0, 'pert_form': 'by u', 'ode_degree': 1, 'ode_last_steps': 2, 'n_iter_buffer': 50, 'n_iter_patience': 100, 'weight_loss': 'None', 'l1lambda': 0.0001, 'l2lambda': 0.0001, 'model': 'CellBox', 'pert_file': '/users/ngun7t/Documents/cellbox-jun-6/data/pert.csv', 'expr_file': '/users/ngun7t/Documents/cellbox-jun-6/data/expr.csv', 'node_index_file': '/users/ngun7t/Documents/cellbox-jun-6/data/node_Index.csv', 'n_protein_nodes': 82, 'n_activity_nodes': 87, 'n_x': 99, 'envelop_form': 'tanh', 'envelop': 0, 'n_epoch': 100, 'n_iter': 100, 'stages': [{'nT': 100, 'sub_stages': [{'lr_val': 0.1, 'l1lambda': 0.01, 'n_iter_patience': 1000}, {'lr_val': 0.01, 'l1lambda': 0.01}, {'lr_val': 0.01, 'l1lambda': 0.0001}, {'lr_val': 0.001, 'l1lambda': 1e-05}]}], 'ckpt_path_full': './model11.ckpt', 'drug_index': 5, 'seed': 1000}\n",
      "Working directory is ready at results/Example_RP_370705d3fa02832e2d75733a602382b0.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(in_seed):\n",
    "    int_seed = int(in_seed)\n",
    "    tf.compat.v1.set_random_seed(int_seed)\n",
    "    np.random.seed(int_seed)\n",
    "\n",
    "\n",
    "def prepare_workdir(in_cfg):\n",
    "    # Read Data\n",
    "    in_cfg.root_dir = os.getcwd()\n",
    "    in_cfg.node_index = pd.read_csv(in_cfg.node_index_file, header=None, names=None) \\\n",
    "        if hasattr(in_cfg, 'node_index_file') else pd.DataFrame(np.arange(in_cfg.n_x))\n",
    "\n",
    "    # Create Output Folder\n",
    "    experiment_path = 'results/{}_{}'.format(in_cfg.experiment_id, md5)\n",
    "    try:\n",
    "        os.makedirs(experiment_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    out_cfg = vars(in_cfg)\n",
    "    out_cfg = {key: out_cfg[key] for key in out_cfg if type(out_cfg[key]) is not pd.DataFrame}\n",
    "    os.chdir(experiment_path)\n",
    "    json.dump(out_cfg, open('config.json', 'w'), indent=4)\n",
    "\n",
    "    if \"leave one out\" in in_cfg.experiment_type:\n",
    "        try:\n",
    "            in_cfg.model_prefix = '{}_{}'.format(in_cfg.model_prefix, in_cfg.drug_index)\n",
    "        except Exception('Drug index not specified') as e:\n",
    "            raise e\n",
    "\n",
    "    in_cfg.working_index = in_cfg.model_prefix + \"_\" + str(working_index).zfill(3)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(in_cfg.working_index)\n",
    "    except Exception:\n",
    "        pass\n",
    "    os.makedirs(in_cfg.working_index)\n",
    "    os.chdir(in_cfg.working_index)\n",
    "\n",
    "    with open(\"record_eval.csv\", 'w') as f:\n",
    "        f.write(\"epoch,iter,train_loss,valid_loss,train_mse,valid_mse,test_mse,time_elapsed\\n\")\n",
    "\n",
    "    print('Working directory is ready at {}.'.format(experiment_path))\n",
    "    return 0\n",
    "\n",
    "experiment_config_path = \"/users/ngun7t/Documents/cellbox-jun-6/configs_dev/Example.random_partition.CellBox.json\"\n",
    "working_index = 0\n",
    "stage = {\n",
    "    \"nT\": 100,\n",
    "    \"sub_stages\":[\n",
    "        {\"lr_val\": 0.1,\"l1lambda\": 0.01, \"n_iter_patience\":1000},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.01},\n",
    "        {\"lr_val\": 0.01,\"l1lambda\": 0.0001},\n",
    "        {\"lr_val\": 0.001,\"l1lambda\": 0.00001}\n",
    "    ]}\n",
    "\n",
    "cfg = cellbox.config.Config(experiment_config_path)\n",
    "cfg.ckpt_path_full = os.path.join('./', cfg.ckpt_name)\n",
    "md5 = cellbox.utils.md5(cfg)\n",
    "cfg.drug_index = 5         # Change this for testing purposes\n",
    "cfg.seed = working_index + cfg.seed if hasattr(cfg, \"seed\") else working_index + 1000\n",
    "set_seed(cfg.seed)\n",
    "print(vars(cfg))\n",
    "\n",
    "prepare_workdir(cfg)\n",
    "logger = cellbox.utils.TimeLogger(time_logger_step=1, hierachy=3)\n",
    "args = cfg\n",
    "for i, stage in enumerate(cfg.stages):\n",
    "    set_seed(cfg.seed)\n",
    "    cfg = cellbox.dataset_torch.factory(cfg)\n",
    "    args.sub_stages = stage['sub_stages']\n",
    "    args.n_T = stage['nT']\n",
    "    model = cellbox.model_torch.factory(args)\n",
    "    if i == 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_substage(model, lr_val, l1_lambda, l2_lambda, n_epoch, n_iter, n_iter_buffer, n_iter_patience, args):\n",
    "\n",
    "    stages = glob.glob(\"*best*.csv\")\n",
    "    try:\n",
    "        substage_i = 1 + max([int(stage[0]) for stage in stages])\n",
    "    except Exception:\n",
    "        substage_i = 1\n",
    "\n",
    "    best_params = Screenshot(args, n_iter_buffer)\n",
    "\n",
    "    n_unchanged = 0\n",
    "    idx_iter = 0\n",
    "    #for key in args.feed_dicts:\n",
    "    #    args.feed_dicts[key].update({\n",
    "    #        model.lr: lr_val,\n",
    "    #        model.l1_lambda: l1_lambda,\n",
    "    #        model.l2_lambda: l2_lambda\n",
    "    #    })\n",
    "    args.logger.log(\"--------- lr: {}\\tl1: {}\\tl2: {}\\t\".format(lr_val, l1_lambda, l2_lambda))\n",
    "    args.l1_lambda = l1_lambda\n",
    "    args.l2_lambda = l2_lambda\n",
    "    args.lr = lr_val\n",
    "    args.optimizer = cellbox.utils_torch.optimize(\n",
    "        model.parameters(),\n",
    "        lr=args.lr\n",
    "    )\n",
    "\n",
    "    #sess.run(model.iter_monitor.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    for idx_epoch in range(n_epoch):\n",
    "\n",
    "        if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "            break\n",
    "\n",
    "        for i, train_minibatch in enumerate(args.iter_train):\n",
    "            # Each train_minibatch has shape of (batch_size, num_features)\n",
    "            x_train, y_train = train_minibatch\n",
    "\n",
    "            if idx_iter > n_iter or n_unchanged > n_iter_patience:\n",
    "                break\n",
    "\n",
    "            # Do one forward pass\n",
    "            t0 = time.perf_counter()\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            args.optimizer.zero_grad()\n",
    "            if args.pert_form == \"by u\":\n",
    "                prediction = model(torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), x_train.to(args.device))\n",
    "            elif args.pert_form == \"fix x\":\n",
    "                prediction = model(x_train.T.to(args.device), x_train.to(args.device))\n",
    "            convergence_metric, yhat = prediction\n",
    "\n",
    "            for param in model.named_parameters():\n",
    "                if param[0] == \"params.W\":\n",
    "                    param_mat = param[1]\n",
    "                    break\n",
    "            #loss_train_i, loss_train_mse_i = args.loss_fn(y_train.to(args.device), yhat, model.state_dict()[\"params.W\"], l1=l1_lambda, l2=l2_lambda)\n",
    "            loss_train_i, loss_train_mse_i = args.loss_fn(y_train.to(args.device), yhat, param_mat, l1=l1_lambda, l2=l2_lambda, weight=y_train.to(args.device))\n",
    "            # Check the gradients of the model\n",
    "            #params = model.parameters()\n",
    "            #for item in params:\n",
    "            #    item.register_hook(lambda grad: print(grad))\n",
    "                #print(f\"Grad of before backward: {item.grad}\")\n",
    "\n",
    "            loss_train_i.backward()\n",
    "            #params = model.state_dict()\n",
    "            #for item in params:\n",
    "            #    print(f\"Grad of {item} after backward: {params[item].grad}\")\n",
    "            args.optimizer.step()\n",
    "\n",
    "            # Record training\n",
    "            with torch.no_grad():\n",
    "                # Very questionable for validation\n",
    "                model.eval()\n",
    "                valid_minibatch = iter(args.iter_monitor)\n",
    "                x_valid, y_valid = next(valid_minibatch)\n",
    "                if args.pert_form == \"by u\":\n",
    "                    prediction = model(\n",
    "                        torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), \n",
    "                        x_valid.to(args.device)\n",
    "                    )\n",
    "                elif args.pert_form == \"fix x\":\n",
    "                    prediction = model(\n",
    "                        x_valid.T.to(args.device),\n",
    "                        x_valid.to(args.device)\n",
    "                    )\n",
    "                    \n",
    "                convergence_metric, yhat = prediction\n",
    "\n",
    "                for param in model.named_parameters():\n",
    "                    if param[0] == \"params.W\":\n",
    "                        param_mat = param[1]\n",
    "                        break\n",
    "                #loss_valid_i, loss_valid_mse_i = args.loss_fn(y_valid.to(args.device), yhat, model.state_dict()[\"params.W\"], l1=l1_lambda, l2=l2_lambda)\n",
    "                loss_valid_i, loss_valid_mse_i = args.loss_fn(y_valid.to(args.device), yhat, param_mat, l1=l1_lambda, l2=l2_lambda, weight=y_valid.to(args.device))\n",
    "\n",
    "            # Record results to screenshot\n",
    "            new_loss = best_params.avg_n_iters_loss(loss_valid_i)\n",
    "            if args.export_verbose > 0:\n",
    "                print((\"Substage:{}\\tEpoch:{}/{}\\tIteration: {}/{}\" + \"\\tloss (train):{:1.6f}\\tloss (buffer on valid):\"\n",
    "                       \"{:1.6f}\" + \"\\tbest:{:1.6f}\\tTolerance: {}/{}\").format(substage_i, idx_epoch, n_epoch, idx_iter,\n",
    "                                                                              n_iter, loss_train_i, new_loss,\n",
    "                                                                              best_params.loss_min, n_unchanged,\n",
    "                                                                              n_iter_patience))\n",
    "            \n",
    "            append_record(\"record_eval.csv\",\n",
    "                          [idx_epoch, idx_iter, loss_train_i.item(), loss_valid_i.item(), loss_train_mse_i.item(),\n",
    "                           loss_valid_mse_i.item(), None, time.perf_counter() - t0])\n",
    "\n",
    "            # Early stopping\n",
    "            idx_iter += 1\n",
    "            if new_loss < best_params.loss_min:\n",
    "                n_unchanged = 0\n",
    "                best_params.screenshot(model, substage_i, args=args,\n",
    "                                       node_index=args.dataset['node_index'], loss_min=new_loss)\n",
    "            else:\n",
    "                n_unchanged += 1\n",
    "\n",
    "        for k, v in best_params.items():\n",
    "            assert type(v) == pd.DataFrame, print(k)\n",
    "\n",
    "    # Evaluation on valid set\n",
    "    t0 = time.perf_counter()\n",
    "    loss_valid_i = eval_model(\n",
    "        args, args.iter_monitor, model, return_value=\"loss_full\", n_batches_eval=args.n_batches_eval\n",
    "    )\n",
    "    loss_valid_mse_i = eval_model(\n",
    "        args, args.iter_monitor, model, return_value=\"loss_mse\", n_batches_eval=args.n_batches_eval\n",
    "    )\n",
    "    append_record(\"record_eval.csv\", [-1, None, None, loss_valid_i, None, loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "\n",
    "    # Evaluation on test set\n",
    "    t0 = time.perf_counter()\n",
    "    loss_test_mse = eval_model(\n",
    "        args, args.iter_eval, model, return_value=\"loss_mse\", n_batches_eval=args.n_batches_eval\n",
    "    )\n",
    "    append_record(\"record_eval.csv\", [-1, None, None, None, None, None, loss_test_mse, time.perf_counter() - t0])\n",
    "\n",
    "    # Save results\n",
    "    best_params.save()\n",
    "    args.logger.log(\"------------------ Substage {} finished!-------------------\".format(substage_i))\n",
    "    save_model(model, f\"./model11.pth\")\n",
    "\n",
    "\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['valid_set'])\n",
    "    #loss_valid_i, loss_valid_mse_i = eval_model(sess, model.iter_eval, (model.eval_loss, model.eval_mse_loss),\n",
    "    #                                            args.feed_dicts['valid_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, loss_valid_i, None, loss_valid_mse_i, None, time.perf_counter() - t0])\n",
    "#\n",
    "    ## Evaluation on test set\n",
    "    #t0 = time.perf_counter()\n",
    "    #sess.run(model.iter_eval.initializer, feed_dict=args.feed_dicts['test_set'])\n",
    "    #loss_test_mse = eval_model(sess, model.iter_eval, model.eval_mse_loss,\n",
    "    #                           args.feed_dicts['test_set'], n_batches_eval=args.n_batches_eval)\n",
    "    #append_record(\"record_eval.csv\", [-1, None, None, None, None, None, loss_test_mse, time.perf_counter() - t0])\n",
    "#\n",
    "    #best_params.save()\n",
    "    #args.logger.log(\"------------------ Substage {} finished!-------------------\".format(substage_i))\n",
    "    #save_model(args.saver, sess, './' + args.ckpt_name)\n",
    "\n",
    "    \n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def append_record(filename, contents):\n",
    "    \"\"\"define function for appending training record\"\"\"\n",
    "    with open(filename, 'a') as f:\n",
    "        for content in contents:\n",
    "            f.write('{},'.format(content))\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "def eval_model(args, eval_iter, model, return_value, return_avg=True, n_batches_eval=None):\n",
    "    \"\"\" Simulate the model for prediction \"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        counter = 0\n",
    "        eval_results = []\n",
    "        for item in eval_iter:\n",
    "            pert, expr = item\n",
    "            if args.pert_form == \"by u\":\n",
    "                prediction = model(\n",
    "                    torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), \n",
    "                    pert.to(args.device)\n",
    "                )\n",
    "            elif args.pert_form == \"fix x\":\n",
    "                prediction = model(\n",
    "                    pert.T.to(args.device), \n",
    "                    pert.to(args.device)\n",
    "                )\n",
    "            _, yhat = prediction\n",
    "            if return_value == \"prediction\":\n",
    "                eval_results.append(yhat.detach().cpu().numpy())\n",
    "            elif return_value == \"loss_full\":\n",
    "                loss_full, _ = args.loss_fn(expr.to(args.device), yhat, model.state_dict()[\"params.W\"], l1=args.l1_lambda, l2=args.l2_lambda)\n",
    "                eval_results.append(loss_full.detach().cpu().numpy())\n",
    "            elif return_value == \"loss_mse\":\n",
    "                _, loss_mse = args.loss_fn(expr.to(args.device), yhat, model.state_dict()[\"params.W\"], l1=args.l1_lambda, l2=args.l2_lambda)\n",
    "                eval_results.append(loss_mse.detach().cpu().numpy())\n",
    "            counter += 1\n",
    "            if n_batches_eval is not None and counter > n_batches_eval:\n",
    "                break\n",
    "\n",
    "        if return_avg:\n",
    "            return np.mean(np.array(eval_results), axis=0)\n",
    "        return np.vstack(eval_results)\n",
    "    \n",
    "\n",
    "def save_model(model, save_dir):\n",
    "    \"\"\" Save the model \"\"\"\n",
    "    torch.save(model.state_dict(), save_dir)\n",
    "\n",
    "\n",
    "def train_model(model, args):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    args.logger = TimeLogger(time_logger_step=1, hierachy=2)\n",
    "    model = model[0].to(args.device)\n",
    "\n",
    "    # Check if all variables in scope\n",
    "    # TODO: put variables under appropriate scopes\n",
    "    #try:\n",
    "    #    args.saver.restore(sess, './' + args.ckpt_name)\n",
    "    #    print('Load existing model at {}...'.format(args.ckpt_name))\n",
    "    #except Exception:\n",
    "    #    print('Create new model at {}...'.format(args.ckpt_name))\n",
    "\n",
    "    # Training\n",
    "    for substage in args.sub_stages:\n",
    "        n_iter_buffer = substage['n_iter_buffer'] if 'n_iter_buffer' in substage else args.n_iter_buffer\n",
    "        n_iter = substage['n_iter'] if 'n_iter' in substage else args.n_iter\n",
    "        n_iter_patience = substage['n_iter_patience'] if 'n_iter_patience' in substage else args.n_iter_patience\n",
    "        n_epoch = substage['n_epoch'] if 'n_epoch' in substage else args.n_epoch\n",
    "        l1 = substage['l1lambda'] if 'l1lambda' in substage else args.l1lambda if hasattr(args, 'l1lambda') else 0\n",
    "        l2 = substage['l2lambda'] if 'l2lambda' in substage else args.l2lambda if hasattr(args, 'l2lambda') else 0\n",
    "        screenshot = train_substage(model, substage['lr_val'], l1_lambda=l1, l2_lambda=l2, n_epoch=n_epoch,\n",
    "                       n_iter=n_iter, n_iter_buffer=n_iter_buffer, n_iter_patience=n_iter_patience, args=args)\n",
    "\n",
    "    return screenshot\n",
    "        \n",
    "\n",
    "class Screenshot(dict):\n",
    "    \"\"\"summarize the model\"\"\"\n",
    "    def __init__(self, args, n_iter_buffer):\n",
    "        # initialize loss_min\n",
    "        super().__init__()\n",
    "        self.loss_min = 1000\n",
    "        # initialize tuning_metric\n",
    "        self.saved_losses = [self.loss_min]\n",
    "        self.n_iter_buffer = n_iter_buffer\n",
    "        # initialize verbose\n",
    "        self.summary = {}\n",
    "        self.summary = {}\n",
    "        self.substage_i = []\n",
    "        self.export_verbose = args.export_verbose\n",
    "        self.args = args\n",
    "\n",
    "    def avg_n_iters_loss(self, new_loss):\n",
    "        \"\"\"average the last few losses\"\"\"\n",
    "        self.saved_losses = self.saved_losses + [new_loss]\n",
    "        self.saved_losses = self.saved_losses[-self.n_iter_buffer:]\n",
    "        return sum(self.saved_losses) / len(self.saved_losses)\n",
    "\n",
    "    def screenshot(self, model, substage_i, node_index, loss_min, args):\n",
    "        \"\"\"evaluate models\"\"\"\n",
    "        self.substage_i = substage_i\n",
    "        self.loss_min = loss_min\n",
    "\n",
    "        # Save the variable weights associated with each of the conditions in a csv file\n",
    "        if self.export_verbose > 0:\n",
    "            #layer = model.W\n",
    "            params = model.state_dict()\n",
    "            new_params = {}\n",
    "            for item in params:\n",
    "                try:\n",
    "                    new_params[item] = pd.DataFrame(params[item].detach().numpy(), index=node_index[0])\n",
    "                except Exception:\n",
    "                    new_params[item] = pd.DataFrame(params[item].detach().numpy())\n",
    "            self.update(new_params)\n",
    "\n",
    "        if self.export_verbose > 1 or self.export_verbose == -1:  # no params but y_hat\n",
    "            y_hat = eval_model(args, args.iter_eval, model, return_value=\"prediction\", return_avg=False)\n",
    "            y_hat = pd.DataFrame(y_hat, columns=node_index[0])\n",
    "            self.update({'y_hat': y_hat})\n",
    "\n",
    "        if self.export_verbose > 2:\n",
    "            try:\n",
    "                # Run summary on train set\n",
    "                converge_train_mat, converge_eval_mat, converge_test_mat = [], [], []\n",
    "                for item in args.iter_train:\n",
    "                    pert, _ = item\n",
    "                    if args.pert_form == \"by u\":\n",
    "                        prediction = model(\n",
    "                            torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), \n",
    "                            pert.to(args.device)\n",
    "                        )\n",
    "                    elif args.pert_form == \"fix x\":\n",
    "                        prediction = model(\n",
    "                            pert.T.to(args.device), \n",
    "                            pert.to(args.device)\n",
    "                        )\n",
    "                    # Shape (length, batch_size)\n",
    "                    convergence_metric_train, _ = prediction\n",
    "                    converge_train_mat.append(convergence_metric_train.detach().numpy())\n",
    "\n",
    "                # Run summary on eval set\n",
    "                for item in args.iter_monitor:\n",
    "                    pert, _ = item\n",
    "                    if args.pert_form == \"by u\":\n",
    "                        prediction = model(\n",
    "                            torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), \n",
    "                            pert.to(args.device)\n",
    "                        )\n",
    "                    elif args.pert_form == \"fix x\":\n",
    "                        prediction = model(\n",
    "                            pert.T.to(args.device), \n",
    "                            pert.to(args.device)\n",
    "                        )\n",
    "                    # Shape (length, batch_size)\n",
    "                    convergence_metric_eval, _ = prediction\n",
    "                    converge_eval_mat.append(convergence_metric_eval.detach().numpy())\n",
    "                \n",
    "                # Run summary on test set\n",
    "                for item in args.iter_eval:\n",
    "                    pert, _ = item\n",
    "                    if args.pert_form == \"by u\":\n",
    "                        prediction = model(\n",
    "                            torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), \n",
    "                            pert.to(args.device)\n",
    "                        )\n",
    "                    elif args.pert_form == \"fix x\":\n",
    "                        prediction = model(\n",
    "                            pert.T.to(args.device), \n",
    "                            pert.to(args.device)\n",
    "                        )\n",
    "                    # Shape (length, batch_size)\n",
    "                    convergence_metric_test, _ = prediction\n",
    "                    converge_test_mat.append(convergence_metric_test.detach().numpy())\n",
    "\n",
    "                # Concatenate the results:\n",
    "                converge_train_mat = np.concatenate(converge_train_mat, axis=1)\n",
    "                converge_test_mat = np.concatenate(converge_test_mat, axis=1)\n",
    "                converge_eval_mat = np.concatenate(converge_eval_mat, axis=1)\n",
    "                \n",
    "                # Summarize performance\n",
    "                cols = [node_index.values + '_mean', node_index.values + '_sd', node_index.values + '_dxdt']\n",
    "                cols = np.squeeze(np.concatenate(cols)).tolist()\n",
    "                summary_train = pd.DataFrame(converge_train_mat.T, columns=cols)\n",
    "                summary_test = pd.DataFrame(converge_test_mat.T, columns=cols)\n",
    "                summary_valid = pd.DataFrame(converge_eval_mat.T, columns=cols)\n",
    "                self.update(\n",
    "                    {'summary_train': summary_train, 'summary_test': summary_test, 'summary_valid': summary_valid}\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"save model parameters\"\"\"\n",
    "        for file in glob.glob(str(self.substage_i) + \"_best.*.csv\"):\n",
    "            os.remove(file)\n",
    "        for key in self:\n",
    "            self[key].to_csv(\"{}_best.{}.loss.{}.csv\".format(self.substage_i, key, self.loss_min))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########   --------- lr: 0.1\tl1: 0.01\tl2: 0.0001\t   --time elapsed: 0.00\n",
      "Substage:1\tEpoch:0/100\tIteration: 0/100\tloss (train):65.206741\tloss (buffer on valid):533.057373\tbest:1000.000000\tTolerance: 0/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substage:1\tEpoch:0/100\tIteration: 1/100\tloss (train):65.931412\tloss (buffer on valid):377.345947\tbest:533.057373\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 2/100\tloss (train):66.116508\tloss (buffer on valid):299.499664\tbest:377.345947\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 3/100\tloss (train):65.780098\tloss (buffer on valid):252.872162\tbest:299.499664\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 4/100\tloss (train):65.498451\tloss (buffer on valid):221.606552\tbest:252.872162\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 5/100\tloss (train):65.409477\tloss (buffer on valid):199.208847\tbest:221.606552\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 6/100\tloss (train):65.636185\tloss (buffer on valid):182.340973\tbest:199.208847\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 7/100\tloss (train):65.217102\tloss (buffer on valid):169.192276\tbest:182.340973\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 8/100\tloss (train):63.767570\tloss (buffer on valid):158.602997\tbest:169.192276\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 9/100\tloss (train):62.668976\tloss (buffer on valid):149.811783\tbest:158.602997\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 10/100\tloss (train):62.440796\tloss (buffer on valid):142.471024\tbest:149.811783\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 11/100\tloss (train):60.833488\tloss (buffer on valid):136.180344\tbest:142.471024\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:0/100\tIteration: 12/100\tloss (train):60.738674\tloss (buffer on valid):130.696259\tbest:136.180344\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 13/100\tloss (train):59.515285\tloss (buffer on valid):125.902016\tbest:130.696259\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 14/100\tloss (train):58.176674\tloss (buffer on valid):121.646988\tbest:125.902016\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 15/100\tloss (train):57.318390\tloss (buffer on valid):117.801292\tbest:121.646988\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 16/100\tloss (train):56.435265\tloss (buffer on valid):114.354843\tbest:117.801292\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 17/100\tloss (train):56.409073\tloss (buffer on valid):111.217247\tbest:114.354843\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 18/100\tloss (train):54.748405\tloss (buffer on valid):108.331253\tbest:111.217247\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 19/100\tloss (train):53.897057\tloss (buffer on valid):105.673767\tbest:108.331253\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 20/100\tloss (train):52.351292\tloss (buffer on valid):103.218773\tbest:105.673767\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 21/100\tloss (train):51.570793\tloss (buffer on valid):100.941246\tbest:103.218773\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 22/100\tloss (train):50.891403\tloss (buffer on valid):98.809441\tbest:100.941246\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 23/100\tloss (train):50.310337\tloss (buffer on valid):96.828659\tbest:98.809441\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 24/100\tloss (train):48.997242\tloss (buffer on valid):94.969650\tbest:96.828659\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:1/100\tIteration: 25/100\tloss (train):49.252708\tloss (buffer on valid):93.225883\tbest:94.969650\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 26/100\tloss (train):48.366158\tloss (buffer on valid):91.576477\tbest:93.225883\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 27/100\tloss (train):47.547314\tloss (buffer on valid):90.020027\tbest:91.576477\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 28/100\tloss (train):46.291885\tloss (buffer on valid):88.556610\tbest:90.020027\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 29/100\tloss (train):46.099472\tloss (buffer on valid):87.146652\tbest:88.556610\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 30/100\tloss (train):45.185467\tloss (buffer on valid):85.806335\tbest:87.146652\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 31/100\tloss (train):44.500301\tloss (buffer on valid):84.524231\tbest:85.806335\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 32/100\tloss (train):43.693562\tloss (buffer on valid):83.296692\tbest:84.524231\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 33/100\tloss (train):42.843124\tloss (buffer on valid):82.111938\tbest:83.296692\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 34/100\tloss (train):41.809845\tloss (buffer on valid):80.967163\tbest:82.111938\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 35/100\tloss (train):41.028786\tloss (buffer on valid):79.865852\tbest:80.967163\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 36/100\tloss (train):40.073082\tloss (buffer on valid):78.799286\tbest:79.865852\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 37/100\tloss (train):39.157402\tloss (buffer on valid):77.763550\tbest:78.799286\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:2/100\tIteration: 38/100\tloss (train):38.044407\tloss (buffer on valid):76.759407\tbest:77.763550\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 39/100\tloss (train):37.643913\tloss (buffer on valid):75.777931\tbest:76.759407\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 40/100\tloss (train):36.810432\tloss (buffer on valid):74.831078\tbest:75.777931\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 41/100\tloss (train):35.788795\tloss (buffer on valid):73.914711\tbest:74.831078\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 42/100\tloss (train):35.365036\tloss (buffer on valid):73.011177\tbest:73.914711\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 43/100\tloss (train):34.952164\tloss (buffer on valid):72.135849\tbest:73.011177\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 44/100\tloss (train):33.517094\tloss (buffer on valid):71.278648\tbest:72.135849\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 45/100\tloss (train):32.725815\tloss (buffer on valid):70.453819\tbest:71.278648\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 46/100\tloss (train):32.128078\tloss (buffer on valid):69.643669\tbest:70.453819\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 47/100\tloss (train):31.284159\tloss (buffer on valid):68.848579\tbest:69.643669\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 48/100\tloss (train):30.814192\tloss (buffer on valid):68.078880\tbest:68.848579\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 49/100\tloss (train):30.228577\tloss (buffer on valid):48.668125\tbest:68.078880\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 50/100\tloss (train):29.427078\tloss (buffer on valid):47.916248\tbest:48.668125\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:3/100\tIteration: 51/100\tloss (train):28.860739\tloss (buffer on valid):47.162476\tbest:47.916248\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 52/100\tloss (train):28.170919\tloss (buffer on valid):46.396690\tbest:47.162476\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 53/100\tloss (train):27.659159\tloss (buffer on valid):45.604359\tbest:46.396690\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 54/100\tloss (train):27.153835\tloss (buffer on valid):44.828907\tbest:45.604359\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 55/100\tloss (train):26.086964\tloss (buffer on valid):44.053837\tbest:44.828907\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 56/100\tloss (train):25.512712\tloss (buffer on valid):43.274273\tbest:44.053837\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 57/100\tloss (train):25.178860\tloss (buffer on valid):42.482101\tbest:43.274273\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 58/100\tloss (train):24.807730\tloss (buffer on valid):41.705040\tbest:42.482101\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 59/100\tloss (train):24.138876\tloss (buffer on valid):40.934608\tbest:41.705040\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 60/100\tloss (train):23.659170\tloss (buffer on valid):40.166431\tbest:40.934608\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 61/100\tloss (train):22.797501\tloss (buffer on valid):39.398537\tbest:40.166431\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 62/100\tloss (train):22.696369\tloss (buffer on valid):38.659340\tbest:39.398537\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 63/100\tloss (train):21.866314\tloss (buffer on valid):37.909252\tbest:38.659340\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:4/100\tIteration: 64/100\tloss (train):22.118509\tloss (buffer on valid):37.181423\tbest:37.909252\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 65/100\tloss (train):21.036459\tloss (buffer on valid):36.470673\tbest:37.181423\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 66/100\tloss (train):20.494108\tloss (buffer on valid):35.755035\tbest:36.470673\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 67/100\tloss (train):20.325937\tloss (buffer on valid):35.055145\tbest:35.755035\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 68/100\tloss (train):20.039141\tloss (buffer on valid):34.384918\tbest:35.055145\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 69/100\tloss (train):19.631266\tloss (buffer on valid):33.738533\tbest:34.384918\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 70/100\tloss (train):20.027313\tloss (buffer on valid):33.104706\tbest:33.738533\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 71/100\tloss (train):19.719048\tloss (buffer on valid):32.476929\tbest:33.104706\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 72/100\tloss (train):19.441368\tloss (buffer on valid):31.875986\tbest:32.476929\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 73/100\tloss (train):19.822571\tloss (buffer on valid):31.268492\tbest:31.875986\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 74/100\tloss (train):18.814793\tloss (buffer on valid):30.678682\tbest:31.268492\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 75/100\tloss (train):18.481731\tloss (buffer on valid):30.095312\tbest:30.678682\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 76/100\tloss (train):18.866613\tloss (buffer on valid):29.523020\tbest:30.095312\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:5/100\tIteration: 77/100\tloss (train):17.899393\tloss (buffer on valid):28.944487\tbest:29.523020\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 78/100\tloss (train):17.488972\tloss (buffer on valid):28.365749\tbest:28.944487\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 79/100\tloss (train):17.334209\tloss (buffer on valid):27.808935\tbest:28.365749\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 80/100\tloss (train):16.708200\tloss (buffer on valid):27.257889\tbest:27.808935\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 81/100\tloss (train):17.190565\tloss (buffer on valid):26.715132\tbest:27.257889\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 82/100\tloss (train):16.314610\tloss (buffer on valid):26.190298\tbest:26.715132\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 83/100\tloss (train):15.579174\tloss (buffer on valid):25.662243\tbest:26.190298\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 84/100\tloss (train):15.584150\tloss (buffer on valid):25.153921\tbest:25.662243\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 85/100\tloss (train):14.852561\tloss (buffer on valid):24.653276\tbest:25.153921\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 86/100\tloss (train):14.561872\tloss (buffer on valid):24.149412\tbest:24.653276\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 87/100\tloss (train):14.586655\tloss (buffer on valid):23.670359\tbest:24.149412\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 88/100\tloss (train):14.053043\tloss (buffer on valid):23.199898\tbest:23.670359\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 89/100\tloss (train):14.090841\tloss (buffer on valid):22.732840\tbest:23.199898\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:6/100\tIteration: 90/100\tloss (train):13.084188\tloss (buffer on valid):22.273766\tbest:22.732840\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 91/100\tloss (train):13.294170\tloss (buffer on valid):21.832653\tbest:22.273766\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 92/100\tloss (train):12.768211\tloss (buffer on valid):21.404448\tbest:21.832653\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 93/100\tloss (train):12.443151\tloss (buffer on valid):20.987463\tbest:21.404448\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 94/100\tloss (train):12.267352\tloss (buffer on valid):20.578789\tbest:20.987463\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 95/100\tloss (train):11.926123\tloss (buffer on valid):20.176765\tbest:20.578789\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 96/100\tloss (train):11.821085\tloss (buffer on valid):19.785608\tbest:20.176765\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 97/100\tloss (train):11.420201\tloss (buffer on valid):19.399887\tbest:19.785608\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 98/100\tloss (train):11.334803\tloss (buffer on valid):19.011984\tbest:19.399887\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 99/100\tloss (train):11.220766\tloss (buffer on valid):18.649275\tbest:19.011984\tTolerance: 0/1000\n",
      "Substage:1\tEpoch:7/100\tIteration: 100/100\tloss (train):10.980704\tloss (buffer on valid):18.304310\tbest:18.649275\tTolerance: 0/1000\n",
      "########   ------------------ Substage 1 finished!-------------------   --time elapsed: 62.04\n",
      "########   --------- lr: 0.01\tl1: 0.01\tl2: 0.0001\t   --time elapsed: 0.01\n",
      "Substage:2\tEpoch:0/100\tIteration: 0/100\tloss (train):11.119411\tloss (buffer on valid):505.678497\tbest:1000.000000\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 1/100\tloss (train):11.218662\tloss (buffer on valid):340.656219\tbest:505.678497\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 2/100\tloss (train):10.126088\tloss (buffer on valid):257.927094\tbest:340.656219\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 3/100\tloss (train):9.634875\tloss (buffer on valid):208.279938\tbest:257.927094\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 4/100\tloss (train):9.308085\tloss (buffer on valid):175.192551\tbest:208.279938\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 5/100\tloss (train):9.046805\tloss (buffer on valid):151.526932\tbest:175.192551\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 6/100\tloss (train):9.081310\tloss (buffer on valid):133.714478\tbest:151.526932\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 7/100\tloss (train):8.889901\tloss (buffer on valid):119.837959\tbest:133.714478\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 8/100\tloss (train):8.452847\tloss (buffer on valid):108.695175\tbest:119.837959\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 9/100\tloss (train):8.413900\tloss (buffer on valid):99.563797\tbest:108.695175\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 10/100\tloss (train):7.924730\tloss (buffer on valid):91.930382\tbest:99.563797\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 11/100\tloss (train):8.130825\tloss (buffer on valid):85.438705\tbest:91.930382\tTolerance: 0/100\n",
      "Substage:2\tEpoch:0/100\tIteration: 12/100\tloss (train):8.387982\tloss (buffer on valid):79.889503\tbest:85.438705\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 13/100\tloss (train):7.419672\tloss (buffer on valid):75.090858\tbest:79.889503\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 14/100\tloss (train):7.331226\tloss (buffer on valid):70.862053\tbest:75.090858\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 15/100\tloss (train):7.372481\tloss (buffer on valid):67.154877\tbest:70.862053\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 16/100\tloss (train):7.031516\tloss (buffer on valid):63.820210\tbest:67.154877\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 17/100\tloss (train):7.126435\tloss (buffer on valid):60.822155\tbest:63.820210\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 18/100\tloss (train):6.591462\tloss (buffer on valid):58.133221\tbest:60.822155\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 19/100\tloss (train):6.418469\tloss (buffer on valid):55.711769\tbest:58.133221\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 20/100\tloss (train):7.196343\tloss (buffer on valid):53.489620\tbest:55.711769\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 21/100\tloss (train):6.390242\tloss (buffer on valid):51.449867\tbest:53.489620\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 22/100\tloss (train):6.188556\tloss (buffer on valid):49.570103\tbest:51.449867\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 23/100\tloss (train):6.350767\tloss (buffer on valid):47.828579\tbest:49.570103\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 24/100\tloss (train):6.119387\tloss (buffer on valid):46.225510\tbest:47.828579\tTolerance: 0/100\n",
      "Substage:2\tEpoch:1/100\tIteration: 25/100\tloss (train):6.596700\tloss (buffer on valid):44.726372\tbest:46.225510\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 26/100\tloss (train):5.559015\tloss (buffer on valid):43.348885\tbest:44.726372\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 27/100\tloss (train):5.501407\tloss (buffer on valid):42.064407\tbest:43.348885\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 28/100\tloss (train):6.029226\tloss (buffer on valid):40.851200\tbest:42.064407\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 29/100\tloss (train):5.379846\tloss (buffer on valid):39.720905\tbest:40.851200\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 30/100\tloss (train):5.478429\tloss (buffer on valid):38.650982\tbest:39.720905\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 31/100\tloss (train):5.412858\tloss (buffer on valid):37.633244\tbest:38.650982\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 32/100\tloss (train):5.252735\tloss (buffer on valid):36.673466\tbest:37.633244\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 33/100\tloss (train):5.048130\tloss (buffer on valid):35.773891\tbest:36.673466\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 34/100\tloss (train):5.109214\tloss (buffer on valid):34.925014\tbest:35.773891\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 35/100\tloss (train):5.569165\tloss (buffer on valid):34.115868\tbest:34.925014\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 36/100\tloss (train):4.877522\tloss (buffer on valid):33.351894\tbest:34.115868\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 37/100\tloss (train):4.591274\tloss (buffer on valid):32.624161\tbest:33.351894\tTolerance: 0/100\n",
      "Substage:2\tEpoch:2/100\tIteration: 38/100\tloss (train):5.369138\tloss (buffer on valid):31.929230\tbest:32.624161\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 39/100\tloss (train):4.956186\tloss (buffer on valid):31.269575\tbest:31.929230\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 40/100\tloss (train):4.480119\tloss (buffer on valid):30.626270\tbest:31.269575\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 41/100\tloss (train):4.408361\tloss (buffer on valid):30.014524\tbest:30.626270\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 42/100\tloss (train):4.583667\tloss (buffer on valid):29.429138\tbest:30.014524\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 43/100\tloss (train):4.223010\tloss (buffer on valid):28.871233\tbest:29.429138\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 44/100\tloss (train):4.102582\tloss (buffer on valid):28.341358\tbest:28.871233\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 45/100\tloss (train):4.143525\tloss (buffer on valid):27.827496\tbest:28.341358\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 46/100\tloss (train):4.585643\tloss (buffer on valid):27.331789\tbest:27.827496\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 47/100\tloss (train):4.041295\tloss (buffer on valid):26.855162\tbest:27.331789\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 48/100\tloss (train):3.987864\tloss (buffer on valid):26.392166\tbest:26.855162\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 49/100\tloss (train):3.648283\tloss (buffer on valid):6.480562\tbest:26.392166\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 50/100\tloss (train):3.879011\tloss (buffer on valid):6.335563\tbest:6.480562\tTolerance: 0/100\n",
      "Substage:2\tEpoch:3/100\tIteration: 51/100\tloss (train):4.144679\tloss (buffer on valid):6.204851\tbest:6.335563\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 52/100\tloss (train):3.785695\tloss (buffer on valid):6.091895\tbest:6.204851\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 53/100\tloss (train):3.827627\tloss (buffer on valid):5.975369\tbest:6.091895\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 54/100\tloss (train):3.502687\tloss (buffer on valid):5.852068\tbest:5.975369\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 55/100\tloss (train):3.525761\tloss (buffer on valid):5.742693\tbest:5.852068\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 56/100\tloss (train):3.389313\tloss (buffer on valid):5.634774\tbest:5.742693\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 57/100\tloss (train):3.708993\tloss (buffer on valid):5.522278\tbest:5.634774\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 58/100\tloss (train):3.308428\tloss (buffer on valid):5.424018\tbest:5.522278\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 59/100\tloss (train):3.512200\tloss (buffer on valid):5.327863\tbest:5.424018\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 60/100\tloss (train):3.308152\tloss (buffer on valid):5.232529\tbest:5.327863\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 61/100\tloss (train):3.056070\tloss (buffer on valid):5.144217\tbest:5.232529\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 62/100\tloss (train):3.220701\tloss (buffer on valid):5.052257\tbest:5.144217\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 63/100\tloss (train):3.173346\tloss (buffer on valid):4.964561\tbest:5.052257\tTolerance: 0/100\n",
      "Substage:2\tEpoch:4/100\tIteration: 64/100\tloss (train):2.774964\tloss (buffer on valid):4.874784\tbest:4.964561\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 65/100\tloss (train):2.938447\tloss (buffer on valid):4.777779\tbest:4.874784\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 66/100\tloss (train):3.020432\tloss (buffer on valid):4.695293\tbest:4.777779\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 67/100\tloss (train):2.748045\tloss (buffer on valid):4.615123\tbest:4.695293\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 68/100\tloss (train):3.185037\tloss (buffer on valid):4.528536\tbest:4.615123\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 69/100\tloss (train):2.741029\tloss (buffer on valid):4.442756\tbest:4.528536\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 70/100\tloss (train):3.015682\tloss (buffer on valid):4.368589\tbest:4.442756\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 71/100\tloss (train):3.146391\tloss (buffer on valid):4.312436\tbest:4.368589\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 72/100\tloss (train):2.511155\tloss (buffer on valid):4.239006\tbest:4.312436\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 73/100\tloss (train):2.688626\tloss (buffer on valid):4.178360\tbest:4.239006\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 74/100\tloss (train):2.624035\tloss (buffer on valid):4.104417\tbest:4.178360\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 75/100\tloss (train):2.666506\tloss (buffer on valid):4.040089\tbest:4.104417\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 76/100\tloss (train):2.591908\tloss (buffer on valid):3.973672\tbest:4.040089\tTolerance: 0/100\n",
      "Substage:2\tEpoch:5/100\tIteration: 77/100\tloss (train):2.576884\tloss (buffer on valid):3.901250\tbest:3.973672\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 78/100\tloss (train):2.360094\tloss (buffer on valid):3.846921\tbest:3.901250\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 79/100\tloss (train):2.695157\tloss (buffer on valid):3.789105\tbest:3.846921\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 80/100\tloss (train):2.324687\tloss (buffer on valid):3.733139\tbest:3.789105\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 81/100\tloss (train):2.854053\tloss (buffer on valid):3.678711\tbest:3.733139\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 82/100\tloss (train):2.458388\tloss (buffer on valid):3.624401\tbest:3.678711\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 83/100\tloss (train):2.312760\tloss (buffer on valid):3.577044\tbest:3.624401\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 84/100\tloss (train):2.136891\tloss (buffer on valid):3.526011\tbest:3.577044\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 85/100\tloss (train):2.197619\tloss (buffer on valid):3.469857\tbest:3.526011\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 86/100\tloss (train):1.983854\tloss (buffer on valid):3.415031\tbest:3.469857\tTolerance: 0/100\n",
      "Substage:2\tEpoch:6/100\tIteration: 87/100\tloss (train):2.621172\tloss (buffer on valid):3.365119\tbest:3.415031\tTolerance: 0/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m screenshot \u001b[39m=\u001b[39m train_model(model, args)\n",
      "Cell \u001b[0;32mIn[31], line 235\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    233\u001b[0m     l1 \u001b[39m=\u001b[39m substage[\u001b[39m'\u001b[39m\u001b[39ml1lambda\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39ml1lambda\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m substage \u001b[39melse\u001b[39;00m args\u001b[39m.\u001b[39ml1lambda \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(args, \u001b[39m'\u001b[39m\u001b[39ml1lambda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    234\u001b[0m     l2 \u001b[39m=\u001b[39m substage[\u001b[39m'\u001b[39m\u001b[39ml2lambda\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39ml2lambda\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m substage \u001b[39melse\u001b[39;00m args\u001b[39m.\u001b[39ml2lambda \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(args, \u001b[39m'\u001b[39m\u001b[39ml2lambda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 235\u001b[0m     screenshot \u001b[39m=\u001b[39m train_substage(model, substage[\u001b[39m'\u001b[39;49m\u001b[39mlr_val\u001b[39;49m\u001b[39m'\u001b[39;49m], l1_lambda\u001b[39m=\u001b[39;49ml1, l2_lambda\u001b[39m=\u001b[39;49ml2, n_epoch\u001b[39m=\u001b[39;49mn_epoch,\n\u001b[1;32m    236\u001b[0m                    n_iter\u001b[39m=\u001b[39;49mn_iter, n_iter_buffer\u001b[39m=\u001b[39;49mn_iter_buffer, n_iter_patience\u001b[39m=\u001b[39;49mn_iter_patience, args\u001b[39m=\u001b[39;49margs)\n\u001b[1;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m screenshot\n",
      "Cell \u001b[0;32mIn[31], line 113\u001b[0m, in \u001b[0;36mtrain_substage\u001b[0;34m(model, lr_val, l1_lambda, l2_lambda, n_epoch, n_iter, n_iter_buffer, n_iter_patience, args)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m new_loss \u001b[39m<\u001b[39m best_params\u001b[39m.\u001b[39mloss_min:\n\u001b[1;32m    112\u001b[0m     n_unchanged \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 113\u001b[0m     best_params\u001b[39m.\u001b[39;49mscreenshot(model, substage_i, args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    114\u001b[0m                            node_index\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39mnode_index\u001b[39;49m\u001b[39m'\u001b[39;49m], loss_min\u001b[39m=\u001b[39;49mnew_loss)\n\u001b[1;32m    115\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     n_unchanged \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[31], line 292\u001b[0m, in \u001b[0;36mScreenshot.screenshot\u001b[0;34m(self, model, substage_i, node_index, loss_min, args)\u001b[0m\n\u001b[1;32m    290\u001b[0m pert, _ \u001b[39m=\u001b[39m item\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mpert_form \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mby u\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 292\u001b[0m     prediction \u001b[39m=\u001b[39m model(\n\u001b[1;32m    293\u001b[0m         torch\u001b[39m.\u001b[39;49mzeros((args\u001b[39m.\u001b[39;49mn_x, \u001b[39m1\u001b[39;49m), dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mto(args\u001b[39m.\u001b[39;49mdevice), \n\u001b[1;32m    294\u001b[0m         pert\u001b[39m.\u001b[39;49mto(args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mpert_form \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfix x\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    297\u001b[0m     prediction \u001b[39m=\u001b[39m model(\n\u001b[1;32m    298\u001b[0m         pert\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice), \n\u001b[1;32m    299\u001b[0m         pert\u001b[39m.\u001b[39mto(args\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/cellbox-jun-6/cellbox/cellbox/model_torch.py:161\u001b[0m, in \u001b[0;36mCellBox.forward\u001b[0;34m(self, y0, mu)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y0, mu):\n\u001b[1;32m    160\u001b[0m     mu_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtranspose(mu, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     ys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mode_solver(y0, mu_t, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mn_T, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dxdt, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_zero_from)\n\u001b[1;32m    162\u001b[0m     \u001b[39m# [n_T, n_x, batch_size]\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     ys \u001b[39m=\u001b[39m ys[\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mode_last_steps:]\n",
      "File \u001b[0;32m~/Documents/cellbox-jun-6/cellbox/cellbox/kernel_torch.py:81\u001b[0m, in \u001b[0;36mheun_solver\u001b[0;34m(x, t_mu, dT, n_T, _dXdt, n_activity_nodes)\u001b[0m\n\u001b[1;32m     76\u001b[0m dxdt_mask \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mpad(\n\u001b[1;32m     77\u001b[0m     torch\u001b[39m.\u001b[39mones((n_activity_nodes, \u001b[39m1\u001b[39m)), \n\u001b[1;32m     78\u001b[0m     (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, n_x \u001b[39m-\u001b[39m n_activity_nodes)\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_T):\n\u001b[0;32m---> 81\u001b[0m     dxdt_current \u001b[39m=\u001b[39m _dXdt(x, t_mu)\n\u001b[1;32m     82\u001b[0m     dxdt_next \u001b[39m=\u001b[39m _dXdt(x \u001b[39m+\u001b[39m dT \u001b[39m*\u001b[39m dxdt_current, t_mu)\n\u001b[1;32m     83\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m dT \u001b[39m*\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (dxdt_current \u001b[39m+\u001b[39m dxdt_next) \u001b[39m*\u001b[39m dxdt_mask\n",
      "File \u001b[0;32m~/Documents/cellbox-jun-6/cellbox/cellbox/kernel_torch.py:46\u001b[0m, in \u001b[0;36mget_dxdt.<locals>.<lambda>\u001b[0;34m(x, t_mu)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIllegal ODE degree. Choose from [1,2].\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39menvelope \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     45\u001b[0m     \u001b[39m# epsilon*phi(Sigma+u)-alpha*x\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m x, t_mu: params[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m*\u001b[39m args\u001b[39m.\u001b[39menvelope_fn(weighted_sum(x) \u001b[39m+\u001b[39m t_mu) \u001b[39m-\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m x\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39menvelope \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[39m# epsilon*[phi(Sigma)+u]-alpha*x\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m x, t_mu: params[\u001b[39m'\u001b[39m\u001b[39meps\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m (args\u001b[39m.\u001b[39menvelope_fn(weighted_sum(x)) \u001b[39m+\u001b[39m t_mu) \u001b[39m-\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39malpha\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/torch/nn/modules/container.py:738\u001b[0m, in \u001b[0;36mParameterDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    737\u001b[0m     attr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_key_to_attr(key)\n\u001b[0;32m--> 738\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, attr)\n",
      "File \u001b[0;32m~/anaconda3/envs/cellbox-3.6-2/lib/python3.8/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "screenshot = train_model(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CellBox(\n",
       "  (params): ParameterDict(\n",
       "      (W): Parameter containing: [torch.FloatTensor of size 99x99]\n",
       "      (alpha): Parameter containing: [torch.FloatTensor of size 99x1]\n",
       "      (eps): Parameter containing: [torch.FloatTensor of size 99x1]\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_model = model[0]\n",
    "actual_model.load_state_dict(torch.load(\"/users/ngun7t/Documents/cellbox-jun-6/results/Example_RP_369eb781d89dcf91404cf96a8fefb773/seed_000/model11.pth\"))\n",
    "actual_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in actual_model.parameters():\n",
    "    print(param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for thing in actual_model.named_parameters():\n",
    "    print(thing[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "False\n",
      "params.W\n",
      "params.alpha\n",
      "params.eps\n"
     ]
    }
   ],
   "source": [
    "params = actual_model.state_dict()\n",
    "print(type(params[\"params.W\"]))\n",
    "print(params[\"params.W\"].requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.nn.Parameter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_train_i: 81.7782974243164\n",
      "loss_train_i: 82.3116455078125\n",
      "loss_train_i: 82.89201354980469\n",
      "loss_train_i: 83.25794982910156\n",
      "loss_train_i: 83.6027603149414\n",
      "loss_train_i: 83.91826629638672\n",
      "loss_train_i: 84.33826446533203\n",
      "loss_train_i: 84.68413543701172\n",
      "loss_train_i: 84.81233215332031\n",
      "loss_train_i: 85.213623046875\n",
      "loss_train_i: 85.30145263671875\n",
      "loss_train_i: 85.51406860351562\n",
      "loss_train_i: 85.80474853515625\n",
      "loss_train_i: 86.03447723388672\n",
      "loss_train_i: 85.9633560180664\n",
      "loss_train_i: 86.2061538696289\n",
      "loss_train_i: 86.38774108886719\n",
      "loss_train_i: 86.35466766357422\n",
      "loss_train_i: 86.42328643798828\n",
      "loss_train_i: 86.5203857421875\n",
      "loss_train_i: 86.67794036865234\n",
      "loss_train_i: 86.6491470336914\n",
      "loss_train_i: 86.88619232177734\n",
      "loss_train_i: 86.7999038696289\n",
      "loss_train_i: 86.7457504272461\n",
      "loss_train_i: 86.94552612304688\n",
      "loss_train_i: 86.96121215820312\n",
      "loss_train_i: 86.96076202392578\n",
      "loss_train_i: 86.84939575195312\n",
      "loss_train_i: 87.06599426269531\n",
      "loss_train_i: 87.01042175292969\n",
      "loss_train_i: 87.040771484375\n",
      "loss_train_i: 86.98552703857422\n",
      "loss_train_i: 86.94056701660156\n",
      "loss_train_i: 87.00919342041016\n",
      "loss_train_i: 87.05601501464844\n",
      "loss_train_i: 87.23880767822266\n",
      "loss_train_i: 87.04667663574219\n",
      "loss_train_i: 87.20069885253906\n",
      "loss_train_i: 87.04849243164062\n",
      "loss_train_i: 87.03677368164062\n",
      "loss_train_i: 87.08328247070312\n",
      "loss_train_i: 87.05314636230469\n",
      "loss_train_i: 87.00015258789062\n",
      "loss_train_i: 87.28899383544922\n",
      "loss_train_i: 86.97570037841797\n",
      "loss_train_i: 86.9691162109375\n",
      "loss_train_i: 87.14582824707031\n",
      "loss_train_i: 86.95321655273438\n",
      "loss_train_i: 86.92768859863281\n",
      "loss_train_i: 86.95207214355469\n",
      "loss_train_i: 86.87342834472656\n",
      "loss_train_i: 86.94328308105469\n",
      "loss_train_i: 86.91221618652344\n",
      "loss_train_i: 87.01486206054688\n",
      "loss_train_i: 86.85089111328125\n",
      "loss_train_i: 86.89535522460938\n",
      "loss_train_i: 86.95874786376953\n",
      "loss_train_i: 87.11519622802734\n",
      "loss_train_i: 86.8935775756836\n",
      "loss_train_i: 86.89212799072266\n",
      "loss_train_i: 86.85440063476562\n",
      "loss_train_i: 86.91356658935547\n",
      "loss_train_i: 86.85467529296875\n",
      "loss_train_i: 87.18737030029297\n",
      "loss_train_i: 86.84771728515625\n",
      "loss_train_i: 86.88750457763672\n",
      "loss_train_i: 86.94293975830078\n",
      "loss_train_i: 86.85968017578125\n",
      "loss_train_i: 86.7986831665039\n",
      "loss_train_i: 86.76776123046875\n",
      "loss_train_i: 86.78688049316406\n",
      "loss_train_i: 86.75959777832031\n",
      "loss_train_i: 86.81793975830078\n",
      "loss_train_i: 86.76240539550781\n",
      "loss_train_i: 86.76917266845703\n",
      "loss_train_i: 86.76040649414062\n",
      "loss_train_i: 86.7602767944336\n",
      "loss_train_i: 86.7221450805664\n",
      "loss_train_i: 86.88419342041016\n",
      "loss_train_i: 86.67717742919922\n",
      "loss_train_i: 86.63971710205078\n",
      "loss_train_i: 86.85670471191406\n",
      "loss_train_i: 86.61676025390625\n",
      "loss_train_i: 86.6404800415039\n",
      "loss_train_i: 86.60001373291016\n",
      "loss_train_i: 86.56271362304688\n",
      "loss_train_i: 86.7404556274414\n",
      "loss_train_i: 86.68378448486328\n",
      "loss_train_i: 86.57091522216797\n",
      "loss_train_i: 86.4342041015625\n",
      "loss_train_i: 86.52989959716797\n",
      "loss_train_i: 86.65701293945312\n",
      "loss_train_i: 86.50991821289062\n",
      "loss_train_i: 86.55298614501953\n",
      "loss_train_i: 86.6712646484375\n",
      "loss_train_i: 86.66212463378906\n",
      "loss_train_i: 86.66845703125\n",
      "loss_train_i: 86.51014709472656\n",
      "loss_train_i: 86.46866607666016\n",
      "loss_train_i: 86.52900695800781\n",
      "loss_train_i: 86.5273208618164\n",
      "loss_train_i: 86.53768157958984\n",
      "loss_train_i: 86.51780700683594\n",
      "loss_train_i: 86.48970031738281\n",
      "loss_train_i: 86.4644546508789\n",
      "loss_train_i: 86.49394226074219\n",
      "loss_train_i: 86.42658233642578\n",
      "loss_train_i: 86.47087097167969\n",
      "loss_train_i: 86.51296997070312\n",
      "loss_train_i: 86.49761962890625\n",
      "loss_train_i: 86.41669464111328\n",
      "loss_train_i: 86.52658081054688\n",
      "loss_train_i: 86.55758666992188\n",
      "loss_train_i: 86.39856719970703\n",
      "loss_train_i: 86.4493408203125\n",
      "loss_train_i: 86.69158172607422\n",
      "loss_train_i: 86.5883560180664\n",
      "loss_train_i: 86.52762603759766\n",
      "loss_train_i: 86.3584213256836\n",
      "loss_train_i: 86.38944244384766\n",
      "loss_train_i: 86.41387176513672\n",
      "loss_train_i: 86.40754699707031\n",
      "loss_train_i: 86.35940551757812\n",
      "loss_train_i: 86.33712005615234\n",
      "loss_train_i: 86.40933990478516\n",
      "loss_train_i: 86.33524322509766\n",
      "loss_train_i: 86.29329681396484\n",
      "loss_train_i: 86.51997375488281\n",
      "loss_train_i: 86.37229919433594\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "for idx_epoch in range(n_epoch):\n",
    "\n",
    "    for i, train_minibatch in enumerate(args.iter_train):\n",
    "        # Each train_minibatch has shape of (batch_size, num_features)\n",
    "        x_train, y_train = train_minibatch\n",
    "\n",
    "        # Do one forward pass\n",
    "        t0 = time.perf_counter()\n",
    "        actual_model.train()\n",
    "        args.optimizer.zero_grad()\n",
    "        if args.pert_form == \"by u\":\n",
    "            prediction = actual_model(torch.zeros((args.n_x, 1), dtype=torch.float32).to(args.device), x_train.to(args.device))\n",
    "        elif args.pert_form == \"fix x\":\n",
    "            prediction = actual_model(x_train.T.to(args.device), x_train.to(args.device))\n",
    "        convergence_metric, yhat = prediction\n",
    "        loss_train_i, loss_train_mse_i = args.loss_fn(y_train.to(args.device), yhat, actual_model.state_dict()[\"params.W\"], l1=args.l1_lambda, l2=args.l2_lambda)\n",
    "        loss_train_i.backward()\n",
    "        args.optimizer.step()\n",
    "\n",
    "        print(f\"loss_train_i: {loss_train_i.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params.W', 'params.alpha', 'params.eps', 'y_hat', 'summary_train', 'summary_test', 'summary_valid'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "screenshot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(screenshot[\"summary_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellbox-3.6-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
